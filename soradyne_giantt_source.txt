=======================================================================
= SORADYNE & GIANTT MULTI-PLATFORM SUITE - COMPLETE SOURCE CODE       =
= Generated on: 2025-10-11 15:21:30                                     =
= Total files: 217                                                      =
=======================================================================

SORADYNE & GIANTT MULTI-PLATFORM SUITE DIRECTORY STRUCTURE (included files only):
.
├── .source_manager
│   └── config.json
├── README.md
├── analysis_options.yaml
├── android
│   ├── app
│   │   └── src
│   │       ├── debug
│   │       │   └── AndroidManifest.xml
│   │       ├── main
│   │       │   ├── AndroidManifest.xml
│   │       │   ├── java
│   │       │   │   └── io
│   │       │   │       └── flutter
│   │       │   │           └── plugins
│   │       │   │               └── GeneratedPluginRegistrant.java
│   │       │   └── res
│   │       │       ├── drawable-v21
│   │       │       │   └── launch_background.xml
│   │       │       ├── drawable
│   │       │       │   └── launch_background.xml
│   │       │       ├── values-night
│   │       │       │   └── styles.xml
│   │       │       └── values
│   │       │           └── styles.xml
│   │       └── profile
│   │           └── AndroidManifest.xml
│   └── gradlew.bat
├── apps
│   ├── giantt
│   │   ├── README.md
│   │   ├── analysis_options.yaml
│   │   ├── android
│   │   │   ├── app
│   │   │   │   └── src
│   │   │   │       ├── debug
│   │   │   │       │   └── AndroidManifest.xml
│   │   │   │       ├── main
│   │   │   │       │   ├── AndroidManifest.xml
│   │   │   │       │   ├── java
│   │   │   │       │   │   └── io
│   │   │   │       │   │       └── flutter
│   │   │   │       │   │           └── plugins
│   │   │   │       │   │               └── GeneratedPluginRegistrant.java
│   │   │   │       │   └── res
│   │   │   │       │       ├── drawable-v21
│   │   │   │       │       │   └── launch_background.xml
│   │   │   │       │       ├── drawable
│   │   │   │       │       │   └── launch_background.xml
│   │   │   │       │       ├── values-night
│   │   │   │       │       │   └── styles.xml
│   │   │   │       │       └── values
│   │   │   │       │           └── styles.xml
│   │   │   │       └── profile
│   │   │   │           └── AndroidManifest.xml
│   │   │   └── gradlew.bat
│   │   ├── ios
│   │   │   ├── Flutter
│   │   │   │   └── ephemeral
│   │   │   │       └── flutter_lldb_helper.py
│   │   │   └── Runner
│   │   │       ├── Assets.xcassets
│   │   │       │   ├── AppIcon.appiconset
│   │   │       │   │   └── Contents.json
│   │   │       │   └── LaunchImage.imageset
│   │   │       │       ├── Contents.json
│   │   │       │       └── README.md
│   │   │       ├── GeneratedPluginRegistrant.h
│   │   │       └── Runner-Bridging-Header.h
│   │   ├── lib
│   │   │   ├── main.dart
│   │   │   ├── screens
│   │   │   │   ├── add_item_screen.dart
│   │   │   │   ├── chart_view_screen.dart
│   │   │   │   ├── home_screen.dart
│   │   │   │   └── item_detail_screen.dart
│   │   │   ├── services
│   │   │   │   └── giantt_service.dart
│   │   │   ├── theme
│   │   │   │   └── app_theme.dart
│   │   │   └── widgets
│   │   │       ├── item_card.dart
│   │   │       └── stats_card.dart
│   │   ├── linux
│   │   │   ├── flutter
│   │   │   │   └── generated_plugin_registrant.h
│   │   │   └── runner
│   │   │       └── my_application.h
│   │   ├── macos
│   │   │   └── Runner
│   │   │       └── Assets.xcassets
│   │   │           └── AppIcon.appiconset
│   │   │               └── Contents.json
│   │   ├── pubspec.yaml
│   │   ├── pubspec_overrides.yaml
│   │   ├── test
│   │   │   └── widget_test.dart
│   │   ├── web
│   │   │   ├── index.html
│   │   │   └── manifest.json
│   │   └── windows
│   │       ├── flutter
│   │       │   └── generated_plugin_registrant.h
│   │       └── runner
│   │           ├── flutter_window.cpp
│   │           ├── flutter_window.h
│   │           ├── main.cpp
│   │           ├── resource.h
│   │           ├── utils.cpp
│   │           ├── utils.h
│   │           ├── win32_window.cpp
│   │           └── win32_window.h
│   └── soradyne_demo
│       ├── flutter_app
│       │   ├── README.md
│       │   ├── analysis_options.yaml
│       │   ├── devtools_options.yaml
│       │   ├── lib
│       │   │   ├── ffi
│       │   │   │   └── soradyne_bindings.dart
│       │   │   ├── main.dart
│       │   │   ├── models
│       │   │   │   ├── album.dart
│       │   │   │   └── media_item.dart
│       │   │   ├── screens
│       │   │   │   ├── activity_selector_screen.dart
│       │   │   │   ├── album_detail_screen.dart
│       │   │   │   └── album_list_screen.dart
│       │   │   ├── services
│       │   │   │   └── album_service.dart
│       │   │   ├── theme
│       │   │   │   └── app_theme.dart
│       │   │   └── widgets
│       │   │       ├── progressive_image.dart
│       │   │       └── storage_status_widget.dart
│       │   ├── pubspec.yaml
│       │   └── test
│       │       └── widget_test.dart
│       ├── pubspec.yaml
│       └── pubspec_overrides.yaml
├── docs
│   └── port_reference
│       ├── giantt_cli.py
│       └── giantt_core.py
├── lib
│   └── main.dart
├── melos.yaml
├── packages
│   ├── ai_chat_flutter
│   │   ├── lib
│   │   │   ├── ai_chat_flutter.dart
│   │   │   └── src
│   │   │       ├── action_registry.dart
│   │   │       └── chat_widget.dart
│   │   └── pubspec.yaml
│   ├── giantt_core
│   │   ├── lib
│   │   │   ├── giantt_core.dart
│   │   │   └── src
│   │   │       ├── commands
│   │   │       │   ├── add_command.dart
│   │   │       │   ├── clean_command.dart
│   │   │       │   ├── command_interface.dart
│   │   │       │   ├── doctor_command.dart
│   │   │       │   ├── includes_command.dart
│   │   │       │   ├── init_command.dart
│   │   │       │   ├── insert_command.dart
│   │   │       │   ├── modify_command.dart
│   │   │       │   ├── occlude_command.dart
│   │   │       │   ├── remove_command.dart
│   │   │       │   ├── set_status_command.dart
│   │   │       │   ├── show_command.dart
│   │   │       │   ├── sort_command.dart
│   │   │       │   └── touch_command.dart
│   │   │       ├── graph
│   │   │       │   ├── cycle_detector.dart
│   │   │       │   └── giantt_graph.dart
│   │   │       ├── logging
│   │   │       │   ├── log_collection.dart
│   │   │       │   ├── log_occluder.dart
│   │   │       │   ├── log_repository.dart
│   │   │       │   ├── log_serializer.dart
│   │   │       │   └── session_manager.dart
│   │   │       ├── models
│   │   │       │   ├── chart.dart
│   │   │       │   ├── duration.dart
│   │   │       │   ├── giantt_item.dart
│   │   │       │   ├── graph_exceptions.dart
│   │   │       │   ├── log_entry.dart
│   │   │       │   ├── priority.dart
│   │   │       │   ├── relation.dart
│   │   │       │   ├── status.dart
│   │   │       │   └── time_constraint.dart
│   │   │       ├── parser
│   │   │       │   ├── giantt_parser.dart
│   │   │       │   └── parse_exceptions.dart
│   │   │       ├── storage
│   │   │       │   ├── atomic_file_writer.dart
│   │   │       │   ├── backup_manager.dart
│   │   │       │   ├── dual_file_manager.dart
│   │   │       │   ├── file_header_generator.dart
│   │   │       │   ├── file_repository.dart
│   │   │       │   ├── include_resolver.dart
│   │   │       │   └── path_resolver.dart
│   │   │       └── validation
│   │   │           ├── graph_doctor.dart
│   │   │           └── issue_types.dart
│   │   ├── pubspec.yaml
│   │   └── test
│   │       ├── compatibility
│   │       │   ├── python_dart_comparison_test.dart
│   │       │   ├── python_dart_execution_test.dart
│   │       │   └── python_dart_validation_test.dart
│   │       ├── graph
│   │       │   ├── relation_system_test.dart
│   │       │   └── topological_sort_test.dart
│   │       ├── models
│   │       │   └── symbol_conversion_test.dart
│   │       ├── parser
│   │       │   └── giantt_parser_test.dart
│   │       ├── storage
│   │       │   ├── include_system_test.dart
│   │       │   └── robust_file_operations_test.dart
│   │       └── validation
│   │           └── graph_doctor_test.dart
│   ├── soradyne_core
│   │   ├── Cargo.toml
│   │   ├── data
│   │   │   ├── 552489f5-0065-4ab7-a0cb-c56363ff1690.json
│   │   │   └── be36205d-7489-4f27-95d0-7dddfe35bab7.json
│   │   ├── examples
│   │   │   ├── album_sync_test.rs
│   │   │   ├── block_storage_demo.rs
│   │   │   ├── block_storage_example.rs
│   │   │   ├── data
│   │   │   │   ├── 34e6e366-562d-4189-9a03-3dad0434cdd6.json
│   │   │   │   └── 6112f9f8-aa50-4852-a105-f7a42914739b.json
│   │   │   ├── heartrate_demo.rs
│   │   │   ├── init_sd_card.rs
│   │   │   ├── large_video_test.rs
│   │   │   ├── renderer_test.rs
│   │   │   ├── robot_joints.rs
│   │   │   ├── visual_block_test.rs
│   │   │   └── web_album_server.rs
│   │   ├── heartrate_data
│   │   │   ├── 004fbc20-d8db-4314-a63e-d71715452474.json
│   │   │   ├── 04aa44fd-d642-4b01-bb77-19f36578c48e.json
│   │   │   ├── 83db1212-3443-43d9-b8d1-f3dbc4fb4736.json
│   │   │   ├── 950080ad-a6ca-4d3b-8baf-9efb09ba1063.json
│   │   │   ├── 9c5240f8-221f-47f3-847e-966c0db13006.json
│   │   │   ├── b76ddcdc-e2e2-4343-9ed2-da8aaf77a078.json
│   │   │   ├── cd685576-4829-4b5f-81c3-f7f26ec40704.json
│   │   │   ├── ce8ea0c1-4e1b-43f9-8e30-23a9df3d3e58.json
│   │   │   ├── df263cc7-f3f1-4ca1-b42a-23e900e14752.json
│   │   │   ├── e3b0e6bf-301c-42f9-9bfa-2e38fe5c72b5.json
│   │   │   └── e98b83da-fc33-4f3a-bf22-f6fc3dade5c8.json
│   │   ├── large_video_test
│   │   │   └── metadata.json
│   │   ├── src
│   │   │   ├── album
│   │   │   │   ├── album.rs
│   │   │   │   ├── crdt.rs
│   │   │   │   ├── mod.rs
│   │   │   │   ├── operations.rs
│   │   │   │   ├── renderer.rs
│   │   │   │   └── sync.rs
│   │   │   ├── ffi
│   │   │   │   └── mod.rs
│   │   │   ├── flow
│   │   │   │   ├── conflict.rs
│   │   │   │   ├── error.rs
│   │   │   │   ├── examples
│   │   │   │   │   ├── mod.rs
│   │   │   │   │   └── robot_joints.rs
│   │   │   │   ├── mod.rs
│   │   │   │   ├── persistence.rs
│   │   │   │   ├── routing.rs
│   │   │   │   ├── subscription.rs
│   │   │   │   └── traits.rs
│   │   │   ├── identity
│   │   │   │   ├── auth.rs
│   │   │   │   ├── keys.rs
│   │   │   │   └── mod.rs
│   │   │   ├── lib.rs
│   │   │   ├── network
│   │   │   │   ├── connection.rs
│   │   │   │   ├── discovery.rs
│   │   │   │   ├── mod.rs
│   │   │   │   └── protocol.rs
│   │   │   ├── storage
│   │   │   │   ├── backends
│   │   │   │   │   ├── bcachefs.rs
│   │   │   │   │   ├── mod.rs
│   │   │   │   │   └── sdyn_erasure.rs
│   │   │   │   ├── block.rs
│   │   │   │   ├── block_file.rs
│   │   │   │   ├── block_manager.rs
│   │   │   │   ├── device_identity.rs
│   │   │   │   ├── dissolution.rs
│   │   │   │   ├── erasure.rs
│   │   │   │   ├── examples.rs
│   │   │   │   ├── galois.rs
│   │   │   │   ├── local_file.rs
│   │   │   │   └── mod.rs
│   │   │   ├── types
│   │   │   │   ├── chat.rs
│   │   │   │   ├── files.rs
│   │   │   │   ├── heartrate.rs
│   │   │   │   ├── media.rs
│   │   │   │   ├── mod.rs
│   │   │   │   ├── photos.rs
│   │   │   │   └── robot_state.rs
│   │   │   ├── utils
│   │   │   │   ├── crypto.rs
│   │   │   │   └── mod.rs
│   │   │   └── video
│   │   │       └── mod.rs
│   │   └── visual_block_test
│   │       ├── metadata.json
│   │       └── results
│   │           └── comparison.html
│   └── soradyne_flutter
│       ├── lib
│       │   ├── soradyne_flutter.dart
│       │   └── src
│       │       ├── crdt
│       │       │   └── crdt_document.dart
│       │       └── soradyne_client.dart
│       └── pubspec.yaml
├── pubspec.yaml
├── source_manager.py
└── test
    └── widget_test.dart

=====================================================================
EXCLUDED FILES (14 total):

Other:
  - *.log
  - *.sh
  - *.tmp
  - */Pods/*
  - */RunnerTests/*
  - *GeneratedPluginRegistrant.swift
  - *Test.swift
  - .idea/*
  - apps/soradyne_demo/flutter_app/macos/*
  - data/*
  - heartrate_data/*
  - large_video_test/*
  - packages/giantt_core/.giantt/*
  - visual_block_test/*

=====================================================================



======================================================================
= SOURCE CODE FILES                                                  =
======================================================================



# =====================================================================
# FILE: source_manager.py
# =====================================================================

#!/usr/bin/env python3
"""
Universal Source Manager

A configurable tool for collecting and concatenating source code files from any repository.
Designed to help prepare comprehensive source code documentation for LLM analysis.

Features:
- Configurable file type collection via JSON config
- Automatic discovery with customizable patterns
- Manual file addition and exclusion management
- Interactive and batch modes
- Flexible output formatting
- Project-agnostic design

Usage:
    source_manager.py init                    # Create default config
    source_manager.py generate               # Generate concatenated file
    source_manager.py add <file>             # Add specific file
    source_manager.py exclude <file>         # Exclude file from collection
    source_manager.py list                   # Show included files
    source_manager.py interactive            # Interactive mode
"""

import os
import sys
import glob
import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Optional, Any

class SourceManager:
    def __init__(self, source_dir='.', output_file=None, config_file=None):
        self.source_dir = Path(source_dir).resolve()
        self.config_file = config_file or self.source_dir / '.source_manager' / 'config.json'
        self.config = self._load_config()
        self.output_file = output_file or self.config.get('output_file', 'source_complete.txt')
        
        # File collections by category
        self.file_collections = {category: [] for category in self.config['file_types'].keys()}
        
        # Exclusion and priority file management
        self.exclusions_dir = self.source_dir / '.source_manager'
        self.exclusions_file = self.exclusions_dir / 'exclusions.txt'
        self.priority_file = self.exclusions_dir / 'priority.txt'
        self._ensure_exclusions_setup()
        self._ensure_priority_setup()
        self.individual_exclusions = self._load_exclusions()
        self.priority_files = self._load_priority_files()
    
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from JSON file or create default."""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r') as f:
                    return json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                print(f"Warning: Could not load config from {self.config_file}, using defaults")
        
        return self._default_config()
    
    def _default_config(self) -> Dict[str, Any]:
        """Return default configuration."""
        return {
            "project_name": "Source Code",
            "output_file": "source_complete.txt",
            "file_types": {
                "source": {
                    "extensions": [".py", ".js", ".ts", ".rs", ".dart", ".java", ".cpp", ".c", ".h", ".go", ".cs"],
                    "description": "Source code files"
                },
                "config": {
                    "extensions": [".json", ".yaml", ".yml", ".toml", ".ini", ".cfg"],
                    "description": "Configuration files"
                },
                "markup": {
                    "extensions": [".html", ".xml", ".md", ".rst"],
                    "description": "Markup and documentation files"
                },
                "scripts": {
                    "extensions": [".sh", ".bat", ".ps1"],
                    "description": "Shell scripts and batch files"
                }
            },
            "excluded_dirs": [
                ".git", "node_modules", "__pycache__", ".pytest_cache",
                "target", "build", "dist", ".dart_tool", "coverage",
                "venv", "env", ".env", "vendor", ".aider", ".vscode",
                ".idea", "bin", "obj", ".webpack", "out", "Generated"
            ],
            "auto_include_patterns": [],
            "banner_config": {
                "padding_h": 5,
                "padding_v": 1,
                "char": "="
            }
        }
    
    def _save_config(self):
        """Save current configuration to file."""
        self.exclusions_dir.mkdir(parents=True, exist_ok=True)
        with open(self.config_file, 'w') as f:
            json.dump(self.config, f, indent=2)
        
    def collect_files(self, debug=False):
        """Collect files based on current configuration and filtering rules only."""
        # Clear existing collections
        for category in self.file_collections:
            self.file_collections[category] = []
        
        # Add priority files first
        for priority_file in self.priority_files:
            self._add_priority_file(priority_file)
        
        # Auto-discover files by walking the directory tree
        for root, dirs, files in os.walk(self.source_dir):
            # Skip excluded directories (only exact directory name matches)
            original_dirs = dirs[:]
            dirs[:] = [d for d in dirs if d not in self.config['excluded_dirs']]
            
            if debug and len(dirs) != len(original_dirs):
                excluded = [d for d in original_dirs if d not in dirs]
                rel_root = Path(root).relative_to(self.source_dir)
                print(f"DEBUG: Excluded directories in {rel_root}: {excluded}")
            
            for file in files:
                full_path = Path(root) / file
                rel_path = full_path.relative_to(self.source_dir)
                
                # Skip excluded files
                if self._is_excluded(str(rel_path)):
                    if debug:
                        print(f"DEBUG: Excluded file: {rel_path}")
                    continue
                
                # Skip aider files that might be in root directory
                if file.startswith('.aider'):
                    continue
                
                # Skip test data and metadata files
                if self._should_skip_file(str(rel_path), file):
                    if debug:
                        print(f"DEBUG: Skipped by pattern: {rel_path}")
                    continue
                
                # Categorize file by extension
                self._categorize_and_add_file(str(rel_path), file)
                if debug and str(rel_path).endswith('.py'):
                    print(f"DEBUG: Added Python file: {rel_path}")
        
        # Print summary
        total_files = sum(len(collection) for collection in self.file_collections.values())
        category_counts = {cat: len(files) for cat, files in self.file_collections.items() if files}
        
        print(f"Found {total_files} files:")
        for category, count in category_counts.items():
            description = self.config['file_types'][category]['description']
            print(f"  {count} {description.lower()}")
        print(f"Excluded {len(self.individual_exclusions)} individual files.")
        
        return self.file_collections
    
    def _add_priority_file(self, file_path: str):
        """Add a priority file to the appropriate collection."""
        if self._add_if_exists(file_path):
            file_name = Path(file_path).name
            self._categorize_and_add_file(file_path, file_name, force=True)
    
    def _should_skip_file(self, rel_path: str, file_name: str) -> bool:
        """Check if a file should be skipped based on patterns."""
        import re
        
        # Convert to string and normalize path separators
        rel_path_str = str(rel_path).replace('\\', '/')
        
        # Debug output
        debug_skip = False
        
        # Skip common build and temporary directories
        skip_patterns = [
            r'^\.git/',
            r'^node_modules/',
            r'^__pycache__/',
            r'^\.pytest_cache/',
            r'^target/',
            r'^build/',
            r'^dist/',
        ]
        
        for pattern in skip_patterns:
            if re.search(pattern, rel_path_str):
                if debug_skip:
                    print(f"DEBUG: Skipping {rel_path_str} due to pattern: {pattern}")
                return True
        
        
        
        
        return False
    
    def _categorize_and_add_file(self, rel_path: str, file_name: str, force: bool = False):
        """Categorize a file by extension and add to appropriate collection."""
        file_ext = Path(file_name).suffix.lower()
        
        for category, config in self.config['file_types'].items():
            if file_ext in config['extensions']:
                if rel_path not in self.file_collections[category]:
                    self.file_collections[category].append(rel_path)
                return
        
        # If no category matches and force is True, add to 'other' category
        if force:
            if 'other' not in self.file_collections:
                self.file_collections['other'] = []
            if rel_path not in self.file_collections['other']:
                self.file_collections['other'].append(rel_path)
            
            # Also ensure 'other' exists in config for summary generation
            if 'other' not in self.config['file_types']:
                self.config['file_types']['other'] = {
                    'extensions': [],
                    'description': 'Other files'
                }
        
    def _ensure_exclusions_setup(self):
        """Ensure the .source_manager directory and exclusions file exist"""
        self.exclusions_dir.mkdir(parents=True, exist_ok=True)
        
        if not self.exclusions_file.exists():
            # Create default exclusions file
            with open(self.exclusions_file, 'w') as f:
                f.write("# Individual file exclusions for source_manager\n")
                f.write("# One file path per line, relative to project root\n")
                f.write("# Lines starting with # are comments and will be ignored\n\n")
                f.write("# Common output files\n")
                f.write("source_complete.txt\n")
                f.write("*.log\n")
                f.write("*.tmp\n")
    
    def _ensure_priority_setup(self):
        """Ensure the priority files file exists"""
        self.exclusions_dir.mkdir(parents=True, exist_ok=True)
        
        if not self.priority_file.exists():
            # Create default priority file
            with open(self.priority_file, 'w') as f:
                f.write("# Priority files for source_manager\n")
                f.write("# These files will always be included in the output\n")
                f.write("# One file path per line, relative to project root\n")
                f.write("# Lines starting with # are comments and will be ignored\n\n")
    
    def _load_exclusions(self):
        """Load the list of excluded files"""
        if not self.exclusions_file.exists():
            return []
        
        with open(self.exclusions_file, 'r') as f:
            exclusions = []
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    exclusions.append(line)
            return exclusions
    
    def _load_priority_files(self):
        """Load the list of priority files"""
        if not self.priority_file.exists():
            return []
        
        with open(self.priority_file, 'r') as f:
            priority_files = []
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    priority_files.append(line)
            return priority_files
    
    def _is_excluded(self, file_path: str) -> bool:
        """Check if a file path matches any exclusion pattern"""
        import fnmatch
        
        # Always exclude the configured output file
        output_filename = Path(self.output_file).name
        if Path(file_path).name == output_filename:
            return True
        
        for exclusion in self.individual_exclusions:
            # Check exact match
            if file_path == exclusion:
                return True
            
            # Check if it's a glob pattern
            if '*' in exclusion or '?' in exclusion:
                if fnmatch.fnmatch(file_path, exclusion):
                    return True
            else:
                # For non-glob patterns, check if the filename matches
                # This handles cases like "package-lock.json" matching "desktop/electron-app/package-lock.json"
                if Path(file_path).name == exclusion:
                    return True
        
        return False
    
    def _save_exclusions(self):
        """Save the current exclusions list to file"""
        with open(self.exclusions_file, 'w') as f:
            f.write("# Individual file exclusions for source_manager.py\n")
            f.write("# One file path per line, relative to project root\n")
            f.write("# Lines starting with # are comments and will be ignored\n\n")
            for exclusion in sorted(self.individual_exclusions):
                f.write(f"{exclusion}\n")
    
    def _save_priority_files(self):
        """Save the current priority files list to file"""
        with open(self.priority_file, 'w') as f:
            f.write("# Priority files for source_manager\n")
            f.write("# These files will always be included in the output\n")
            f.write("# One file path per line, relative to project root\n")
            f.write("# Lines starting with # are comments and will be ignored\n\n")
            for priority_file in sorted(self.priority_files):
                f.write(f"{priority_file}\n")
    
    def add_exclusion(self, file_path):
        """Add a file to the exclusions list"""
        if file_path not in self.individual_exclusions:
            self.individual_exclusions.append(file_path)
            self._save_exclusions()
            print(f"Added '{file_path}' to exclusions list")
            return True
        else:
            print(f"'{file_path}' is already in exclusions list")
            return False
    
    def remove_exclusion(self, file_path):
        """Remove a file from the exclusions list"""
        if file_path in self.individual_exclusions:
            self.individual_exclusions.remove(file_path)
            self._save_exclusions()
            print(f"Removed '{file_path}' from exclusions list")
            return True
        else:
            print(f"'{file_path}' is not in exclusions list")
            return False
    
    def list_exclusions(self):
        """List all currently excluded files"""
        if not self.individual_exclusions:
            print("No files are currently excluded.")
            return
        
        print("Currently excluded files:")
        for i, exclusion in enumerate(sorted(self.individual_exclusions), 1):
            print(f"{i}. {exclusion}")
        print(f"\nTotal: {len(self.individual_exclusions)} excluded files")
    
    def _add_if_exists(self, file_path):
        """Check if a file exists and is not excluded"""
        if file_path in self.individual_exclusions:
            return False
        
        full_path = self.source_dir / file_path
        return full_path.exists()
    
    def _generate_directory_tree(self):
        """Generate a directory tree of included files"""
        all_files = []
        for collection in self.file_collections.values():
            all_files.extend(collection)
        
        # Build directory structure
        tree = {}
        for file_path in sorted(all_files):
            parts = file_path.split('/')
            current = tree
            for part in parts[:-1]:  # All but the last part (filename)
                if part not in current:
                    current[part] = {}
                current = current[part]
            # Add the file
            current[parts[-1]] = None
        
        # Convert to string representation
        def format_tree(node, prefix="", is_last=True):
            lines = []
            items = list(node.items())
            for i, (name, subtree) in enumerate(items):
                is_last_item = i == len(items) - 1
                current_prefix = "└── " if is_last_item else "├── "
                lines.append(f"{prefix}{current_prefix}{name}")
                
                if subtree is not None:  # It's a directory
                    extension = "    " if is_last_item else "│   "
                    lines.extend(format_tree(subtree, prefix + extension, is_last_item))
            return lines
        
        tree_lines = [f"{self.config['project_name'].upper()} DIRECTORY STRUCTURE (included files only):"]
        tree_lines.append(".")
        tree_lines.extend(format_tree(tree))
        return "\n".join(tree_lines)
    
    def _generate_exclusions_summary(self):
        """Generate a summary of excluded files"""
        if not self.individual_exclusions:
            return "No files are currently excluded."
        
        lines = [f"EXCLUDED FILES ({len(self.individual_exclusions)} total):"]
        lines.append("")
        
        # Group by category based on generic patterns
        categories = {
            "Configuration": [],
            "Documentation": [],
            "Build/Output": [],
            "Tests": [],
            "Other": []
        }
        
        for exclusion in sorted(self.individual_exclusions):
            if any(exclusion.startswith(p) for p in ["test/", "tests/", "spec/"]):
                categories["Tests"].append(exclusion)
            elif any(exclusion.endswith(e) for e in [".md", ".txt", ".rst"]):
                categories["Documentation"].append(exclusion)
            elif any(exclusion.startswith(p) for p in ["build/", "dist/", "target/"]):
                categories["Build/Output"].append(exclusion)
            elif any(exclusion.endswith(e) for e in [".json", ".yaml", ".yml", ".toml"]):
                categories["Configuration"].append(exclusion)
            else:
                categories["Other"].append(exclusion)
        
        for category, files in categories.items():
            if files:
                lines.append(f"{category}:")
                for file in files:
                    lines.append(f"  - {file}")
                lines.append("")
        
        return "\n".join(lines)
        
    def generate_concatenated_file(self, debug=False):
        """Generate a new concatenated file from all source files"""
        self.collect_files(debug=debug)
        
        with open(self.output_file, 'w') as output:
            total_files = sum(len(collection) for collection in self.file_collections.values())
            
            # Generate header
            project_name = self.config['project_name'].upper()
            header_text = f"{project_name} - COMPLETE SOURCE CODE"
            border_char = self.config['banner_config']['char']
            border_len = max(70, len(header_text) + 10)
            border = border_char * border_len
            
            output.write(f"{border}\n")
            output.write(f"{border_char} {header_text:<{border_len-4}} {border_char}\n")
            output.write(f"{border_char} Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S'):<{border_len-16}} {border_char}\n")
            output.write(f"{border_char} Total files: {total_files:<{border_len-15}} {border_char}\n")
            output.write(f"{border}\n\n")
            
            # Add directory tree
            output.write(self._generate_directory_tree())
            output.write("\n\n")
            
            # Add exclusions summary
            output.write("=====================================================================\n")
            output.write(self._generate_exclusions_summary())
            output.write("\n=====================================================================\n\n")
            
            # Write files by category
            for category, files in self.file_collections.items():
                if not files:
                    continue
                
                category_config = self.config['file_types'][category]
                section_title = category_config['description'].upper()
                
                border = "=" * 70
                output.write(f"\n\n{border}\n")
                output.write(f"= {section_title:<66} =\n")
                output.write(f"{border}\n\n")
                
                for file_path in files:
                    full_path = self.source_dir / file_path
                    
                    # Choose comment style based on file extension
                    ext = Path(file_path).suffix.lower()
                    if ext in ['.html', '.xml', '.md']:
                        comment_start = "<!-- "
                        comment_end = " -->"
                    elif ext in ['.py', '.sh', '.yaml', '.yml', '.toml']:
                        comment_start = "# "
                        comment_end = ""
                    else:
                        comment_start = "// "
                        comment_end = ""
                    
                    output.write(f"\n\n{comment_start}{'=' * 69}{comment_end}\n")
                    output.write(f"{comment_start}FILE: {file_path}{comment_end}\n")
                    output.write(f"{comment_start}{'=' * 69}{comment_end}\n\n")
                    
                    try:
                        with open(full_path, 'r', encoding='utf-8') as input_file:
                            output.write(input_file.read())
                    except UnicodeDecodeError:
                        output.write(f"{comment_start}[Error reading file: {file_path} - possible binary content]{comment_end}\n")
            
            # Footer
            border = "=" * 70
            output.write(f"\n\n{border}\n")
            output.write(f"= {'END OF FILES':<66} =\n")
            output.write(f"{border}\n\n")

        print(f"Concatenation complete! Output file: {self.output_file}")
        
    def add_file(self, file_path):
        """Add a new file to the existing concatenated file and save to priority list"""
        full_path = os.path.join(self.source_dir, file_path)
        
        if not os.path.exists(full_path):
            print(f"Error: File '{file_path}' does not exist")
            return False
        
        # Add to priority files list to make it persistent
        if file_path not in self.priority_files:
            self.priority_files.append(file_path)
            self._save_priority_files()
            print(f"Added '{file_path}' to priority files list (will be included in future generations)")
        
        print(f"Added '{file_path}' to priority files. Run 'generate' to include it in the output file.")
        
        return True
        
    def _currently_included_files(self):
        """Get a list of currently included files in the concatenated file"""
        if not os.path.exists(self.output_file):
            print(f"(Output file '{self.output_file}' does not exist.)")
            return []
        
        included_files = []
        with open(self.output_file, 'r', encoding='utf-8') as f:
            for line in f:
                # Handle different comment styles
                if line.startswith('// FILE: '):
                    included_files.append(line[9:].strip())
                elif line.startswith('# FILE: '):
                    included_files.append(line[8:].strip())
                elif line.startswith('<!-- FILE: ') and line.endswith(' -->\n'):
                    included_files.append(line[11:-5].strip())
        
        return included_files
    
    def list_files(self):
        """List all files currently included in the concatenation"""
        if not Path(self.output_file).exists():
            print(f"Error: Output file '{self.output_file}' does not exist. Generate it first.")
            return
            
        included_files = self._currently_included_files()
        if not included_files:
            print("No files currently included in the concatenation.")
            return
        
        print(f"Files included in {self.output_file}:")
        
        # Group files by category
        categorized_files = {category: [] for category in self.config['file_types'].keys()}
        uncategorized_files = []
        
        for file_path in included_files:
            ext = Path(file_path).suffix.lower()
            categorized = False
            
            for category, config in self.config['file_types'].items():
                if ext in config['extensions']:
                    categorized_files[category].append(file_path)
                    categorized = True
                    break
            
            if not categorized:
                uncategorized_files.append(file_path)
        
        # Print categorized files
        total_count = 0
        for category, files in categorized_files.items():
            if files:
                description = self.config['file_types'][category]['description']
                print(f"\n{description}:")
                for i, file_path in enumerate(files, 1):
                    print(f"{i:3}. {file_path}")
                total_count += len(files)
        
        # Print uncategorized files
        if uncategorized_files:
            print(f"\nOther files:")
            for i, file_path in enumerate(uncategorized_files, 1):
                print(f"{i:3}. {file_path}")
            total_count += len(uncategorized_files)
        
        # Print summary
        category_summary = []
        for category, files in categorized_files.items():
            if files:
                category_summary.append(f"{len(files)} {category}")
        
        if uncategorized_files:
            category_summary.append(f"{len(uncategorized_files)} other")
        
        print(f"\nTotal: {total_count} files ({', '.join(category_summary)})")
    
    def init_config(self):
        """Initialize configuration file with defaults."""
        if self.config_file.exists():
            print(f"Configuration already exists at {self.config_file}")
            return False
        
        self.config = self._default_config()
        self._save_config()
        print(f"Created default configuration at {self.config_file}")
        print("Edit this file to customize file types, exclusions, and other settings.")
        return True
    
    def show_config(self):
        """Display current configuration."""
        print("Current configuration:")
        print(json.dumps(self.config, indent=2))
    
    def remove_priority_file(self, file_path):
        """Remove a file from the priority files list"""
        if file_path in self.priority_files:
            self.priority_files.remove(file_path)
            self._save_priority_files()
            print(f"Removed '{file_path}' from priority files list")
            return True
        else:
            print(f"'{file_path}' is not in priority files list")
            return False
    
    def list_priority_files(self):
        """List all priority files"""
        if not self.priority_files:
            print("No priority files configured.")
            return
        
        print("Priority files (always included):")
        for i, file_path in enumerate(self.priority_files, 1):
            exists = (self.source_dir / file_path).exists()
            status = "✓" if exists else "✗ (missing)"
            print(f"{i:3}. {file_path} {status}")
        print(f"\nTotal: {len(self.priority_files)} priority files")
    
    def add_file_type(self, category: str, extensions: List[str], description: str):
        """Add a new file type category."""
        self.config['file_types'][category] = {
            'extensions': extensions,
            'description': description
        }
        self._save_config()
        print(f"Added file type category '{category}' with extensions: {', '.join(extensions)}")

def main():
    parser = argparse.ArgumentParser(
        description='Universal Source Manager - Collect and concatenate source code files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s init                     # Create default configuration
  %(prog)s generate                 # Generate concatenated source file
  %(prog)s add src/main.py          # Add specific file
  %(prog)s exclude tests/           # Exclude directory
  %(prog)s list                     # Show included files
  %(prog)s config                   # Show current configuration
  %(prog)s interactive              # Interactive mode

Configuration:
  The tool uses .source_manager/config.json for settings.
  Run 'init' to create a default configuration file.

Note:
  The 'list' command shows files from the existing output file.
  The 'generate' command always uses current filtering rules and
  configuration - it does not preserve previously included files
  that would now be filtered out. Run 'generate' to apply any
  changes to filtering rules or exclusions.
        """
    )
    parser.add_argument('--source', default='.', help='Source directory (default: current directory)')
    parser.add_argument('--output', help='Output file path (overrides config)')
    parser.add_argument('--config', help='Configuration file path')
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Init command
    init_parser = subparsers.add_parser('init', help='Initialize configuration file')
    
    # Generate command
    generate_parser = subparsers.add_parser('generate', help='Generate concatenated source file')
    generate_parser.add_argument('--debug', action='store_true', help='Enable debug output')
    
    # Add command
    add_parser = subparsers.add_parser('add', help='Add specific file to collection')
    add_parser.add_argument('file', help='Path to the file to add')
    
    # List command
    list_parser = subparsers.add_parser('list', help='List all included files')
    
    # Config commands
    config_parser = subparsers.add_parser('config', help='Show current configuration')
    
    # Exclusion management
    exclude_parser = subparsers.add_parser('exclude', help='Add file/pattern to exclusions')
    exclude_parser.add_argument('file', help='Path to the file to exclude')
    
    include_parser = subparsers.add_parser('include', help='Remove file from exclusions')
    include_parser.add_argument('file', help='Path to the file to include')
    
    list_exclusions_parser = subparsers.add_parser('list-exclusions', help='List excluded files')
    
    # Priority file management
    remove_parser = subparsers.add_parser('remove', help='Remove file from priority list')
    remove_parser.add_argument('file', help='Path to the file to remove from priority list')
    
    list_priority_parser = subparsers.add_parser('list-priority', help='List priority files')
    
    # File type management
    add_type_parser = subparsers.add_parser('add-type', help='Add new file type category')
    add_type_parser.add_argument('category', help='Category name')
    add_type_parser.add_argument('extensions', nargs='+', help='File extensions (e.g., .py .pyx)')
    add_type_parser.add_argument('--description', required=True, help='Category description')
    
    # Interactive mode
    interactive_parser = subparsers.add_parser('interactive', help='Run in interactive mode')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = SourceManager(args.source, args.output, args.config)
    
    if args.command == 'init':
        manager.init_config()
    elif args.command == 'generate':
        debug = getattr(args, 'debug', False)
        manager.generate_concatenated_file(debug=debug)
    elif args.command == 'add':
        manager.add_file(args.file)
    elif args.command == 'list':
        manager.list_files()
    elif args.command == 'config':
        manager.show_config()
    elif args.command == 'exclude':
        manager.add_exclusion(args.file)
    elif args.command == 'include':
        manager.remove_exclusion(args.file)
    elif args.command == 'list-exclusions':
        manager.list_exclusions()
    elif args.command == 'remove':
        manager.remove_priority_file(args.file)
    elif args.command == 'list-priority':
        manager.list_priority_files()
    elif args.command == 'add-type':
        manager.add_file_type(args.category, args.extensions, args.description)
    elif args.command == 'interactive':
        run_interactive_mode(manager)

def run_interactive_mode(manager):
    """Run an interactive command loop"""
    print("Universal Source Manager - Interactive Mode")
    print("Type 'help' for a list of commands")
    
    while True:
        cmd = input("\nsource> ").strip()
        
        if cmd.lower() in ['exit', 'quit']:
            break
        elif cmd.lower() == 'help':
            print("Available commands:")
            print("  generate              - Generate a new concatenated file")
            print("  add <file>            - Add a new file to the collection")
            print("  list                  - List all files in the concatenation")
            print("  config                - Show current configuration")
            print("  exclude <file>        - Add a file to the exclusions list")
            print("  include <file>        - Remove a file from the exclusions list")
            print("  list-exclusions       - List all excluded files")
            print("  add-type <cat> <exts> - Add new file type category")
            print("  exit/quit             - Exit the program")
        elif cmd.lower() == 'generate':
            manager.generate_concatenated_file()
        elif cmd.lower().startswith('add '):
            file_path = cmd[4:].strip()
            if file_path:
                manager.add_file(file_path)
            else:
                print("Error: Please specify a file path")
        elif cmd.lower() == 'list':
            manager.list_files()
        elif cmd.lower() == 'config':
            manager.show_config()
        elif cmd.lower().startswith('exclude '):
            file_path = cmd[8:].strip()
            if file_path:
                manager.add_exclusion(file_path)
            else:
                print("Error: Please specify a file path")
        elif cmd.lower().startswith('include '):
            file_path = cmd[8:].strip()
            if file_path:
                manager.remove_exclusion(file_path)
            else:
                print("Error: Please specify a file path")
        elif cmd.lower() == 'list-exclusions':
            manager.list_exclusions()
        elif cmd.lower().startswith('add-type '):
            parts = cmd[9:].strip().split()
            if len(parts) >= 2:
                category = parts[0]
                extensions = parts[1:]
                description = input(f"Description for '{category}': ").strip()
                if description:
                    manager.add_file_type(category, extensions, description)
                else:
                    print("Error: Description is required")
            else:
                print("Error: Usage: add-type <category> <extension1> [extension2] ...")
        elif cmd.strip():
            print(f"Unknown command: '{cmd}'. Type 'help' for a list of commands.")

if __name__ == '__main__':
    main()


// =====================================================================
// FILE: test/widget_test.dart
// =====================================================================

// This is a basic Flutter widget test.
//
// To perform an interaction with a widget in your test, use the WidgetTester
// utility in the flutter_test package. For example, you can send tap and scroll
// gestures. You can also use WidgetTester to find child widgets in the widget
// tree, read text, and verify that the values of widget properties are correct.

import 'package:flutter/material.dart';
import 'package:flutter_test/flutter_test.dart';

import 'package:soradyne_workspace/main.dart';

void main() {
  testWidgets('Counter increments smoke test', (WidgetTester tester) async {
    // Build our app and trigger a frame.
    await tester.pumpWidget(const MyApp());

    // Verify that our counter starts at 0.
    expect(find.text('0'), findsOneWidget);
    expect(find.text('1'), findsNothing);

    // Tap the '+' icon and trigger a frame.
    await tester.tap(find.byIcon(Icons.add));
    await tester.pump();

    // Verify that our counter has incremented.
    expect(find.text('0'), findsNothing);
    expect(find.text('1'), findsOneWidget);
  });
}


# =====================================================================
# FILE: docs/port_reference/giantt_cli.py
# =====================================================================

from typing import List, Dict, Tuple, Optional, Set
import click
import shutil
import tempfile
from pathlib import Path
import re
import json
import os

from giantt_core import (
    GianttGraph, GianttItem, RelationType,
    Status, Priority, Duration,
    LogEntry, LogCollection,
    Issue, IssueType, GianttDoctor,
    CycleDetectedException
)

def get_default_giantt_path(filename: str = 'items.txt', occlude: bool = False) -> str:
    """Get the default path for Giantt files."""
    # whether it's the occlude or include directory
    filepath = Path('occlude' if occlude else 'include') / filename
    # First check for local .giantt directory
    local_path = Path.cwd() / '.giantt' / filepath
    if local_path.exists():
        return str(local_path)

    # Fall back to home directory
    home_path = Path.home() / '.giantt' / filepath
    if home_path.exists():
        return str(home_path)

    # If neither exists, raise an error
    raise click.ClickException(f"No Giantt {filepath} found. Please run 'giantt init' or 'giantt init --dev' first.")

def increment_backup_name(filepath: str) -> str:
    """Increment the backup name for a file."""
    backup_num = 1
    while True:
        backup_path = f"{filepath}.{backup_num}.backup"
        if not os.path.exists(backup_path):
            return backup_path
        backup_num += 1

def most_recent_backup_name(filepath: str) -> str:
    """Get the most recent backup name for a file."""
    # get list of backups by listing the directory
    backups = os.listdir(os.path.dirname(filepath))
    # sort key that gets the number in the backup name as an integer
    key = lambda x: int(x.split('.')[-2]) if x.endswith('.backup') else 0
    for backup in reversed(sorted(backups, key=key)):
        if backup.startswith(f"{os.path.basename(filepath)}.") and backup.endswith(".backup"):
            return os.path.join(os.path.dirname(filepath), backup)

def parse_include_directives(filepath: str) -> List[str]:
    """Parse include directives from a file.
    
    Include directives should be at the top of the file in the format:
    #include path/to/file.txt
    
    Returns:
        List of file paths to include
    """
    includes = []
    try:
        with open(filepath, 'r') as f:
            for line in f:
                line = line.strip()
                if not line or not line.startswith('#include '):
                    break  # Only process directives at the top
                include_path = line[9:].strip()  # Remove '#include ' prefix
                includes.append(include_path)
    except FileNotFoundError:
        click.echo(f"Warning: Include file not found: {filepath}", err=True)
    return includes

def load_graph_from_file(filepath: str, loaded_files: Optional[Set[str]] = None) -> GianttGraph:
    """Load a graph from a file, processing include directives.
    
    Args:
        filepath: Path to the file to load
        loaded_files: Set of files already loaded (to prevent circular includes)
        
    Returns:
        GianttGraph object
    """
    if loaded_files is None:
        loaded_files = set()
    
    # Prevent circular includes
    if filepath in loaded_files:
        click.echo(f"Warning: Circular include detected for {filepath}, skipping", err=True)
        return GianttGraph()
    
    loaded_files.add(filepath)
    
    # Create a backup of the file first if it exists
    if os.path.exists(filepath):
        shutil.copyfile(filepath, increment_backup_name(filepath))
    else:
        click.echo(f"Warning: File not found: {filepath}, skipping", err=True)
        return GianttGraph()
    
    # Process include directives
    includes = parse_include_directives(filepath)
    
    # Create the graph
    graph = GianttGraph()
    
    # Load included files first
    for include_path in includes:
        # Handle relative paths
        if not os.path.isabs(include_path):
            base_dir = os.path.dirname(filepath)
            include_path = os.path.join(base_dir, include_path)
        
        try:
            include_graph = load_graph_from_file(include_path, loaded_files)
            graph = graph + include_graph
        except Exception as e:
            click.echo(f"Warning: Error loading include {include_path}: {e}", err=True)
    
    # Now load the main file
    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                try:
                    item = GianttItem.from_string(line, occlude='occlude' in filepath)
                    graph.add_item(item)
                except ValueError as e:
                    click.echo(f"Warning: Skipping invalid line: {e}", err=True)
    
    return graph

def load_logs_from_file(filepath: str) -> LogCollection:
    # create a backup of the file first
    shutil.copyfile(filepath, increment_backup_name(filepath))
    logs = LogCollection()
    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                try:
                    log = LogEntry.from_line(line, occlude='occlude' in filepath)
                    logs.add_entry(log)
                except json.JSONDecodeError as e:
                    click.echo(f"Warning: Skipping invalid log line: {e}", err=True)
    return logs

def load_graph(filepath: str, occlude_filepath: str) -> GianttGraph:
    """Load a graph from main and occluded files, processing includes."""
    loaded_files = set()
    return load_graph_from_file(filepath, loaded_files) + load_graph_from_file(occlude_filepath, loaded_files)

def load_logs(filepath: str, occlude_filepath: str) -> LogCollection:
    logs = load_logs_from_file(filepath)
    logs.add_entries(load_logs_from_file(occlude_filepath)) # occlude status is picked up from the file path
    return logs

def load_graph_and_logs(items_file: str, occlude_items_file: str, logs_file: str, occlude_logs_file: str):
    """Load GianttGraph and LogCollection objects from files."""
    return load_graph(items_file, occlude_items_file), load_logs(logs_file, occlude_logs_file)

def create_banner(text: str, padding_h: int = 5, padding_v: int = 1):
    """Create a banner of hash characters in a box with text centered inside."""
    lines = text.split('\n')
    max_length = max(len(line) for line in lines)
    banner_len = max_length + 2 * padding_h  # Account for horizontal padding
    top_bottom_border = "#" * (banner_len + 2)  # Add border around text

    banner = top_bottom_border + "\n"

    # Add vertical padding lines
    empty_line = "#" + " " * banner_len + "#"
    for _ in range(padding_v):
        banner += empty_line + "\n"

    # Add text lines, centered
    for line in lines:
        padding = max_length - len(line)
        left_padding = padding // 2
        right_padding = padding - left_padding
        banner += f"#{' ' * padding_h}{' ' * left_padding}{line}{' ' * right_padding}{' ' * padding_h}#\n"

    # Add vertical padding lines
    for _ in range(padding_v):
        banner += empty_line + "\n"

    banner += top_bottom_border + "\n"

    return banner

ITEMS_FILE_BANNER = (
    create_banner(
        'Giantt Items\n'
        'This file contains all include Giantt items in topological\n'
        f'order according to the REQUIRES ({RelationType.REQUIRES.value}) relation.\n'
        'You can use #include directives at the top of this file\n'
        'to include other Giantt item files.\n'
        'Edit this file manually at your own risk.'
    )
)

ITEMS_ARCHIVE_FILE_BANNER = (
    create_banner(
        'Giantt Occluded Items\n'
        'This file contains all occluded Giantt items in topological\n'
        f'order according to the REQUIRES ({RelationType.REQUIRES.value}) relation.\n'
        'Edit this file manually at your own risk.'
    )
)

def save_graph_files(filepath: str, occlude_filepath: str, graph: GianttGraph):
    """
    Safely saves all graph items, split appropriately between
    include and occlude files, by first performing a sort in memory
    and using a transaction-like write to temporary files to
    ensure consistency.

    Args:
        filepath: Path to the items file to save
        occlude_filepath: Path to the occluded items file to save
        graph: GianttGraph object containing items to save

    Raises:
        CycleDetectedException: If dependencies contain a cycle
        ValueError: If dependencies reference non-existent items
    """
    try:
        # First perform the sort in memory to check for issues
        sorted_items = graph.topological_sort()

        # Create temporary files
        temp_include = filepath + '.temp'
        temp_occlude = occlude_filepath + '.temp'

        # Write to temporary files
        with open(temp_include, "w") as f:
            f.write(ITEMS_FILE_BANNER + "\n")
            for item in sorted_items:
                if not item.occlude:
                    f.write(item.to_string() + "\n")

        with open(temp_occlude, "w") as f:
            f.write(ITEMS_ARCHIVE_FILE_BANNER + "\n")
            for item in sorted_items:
                if item.occlude:
                    f.write(item.to_string() + "\n")

        # If we get here, both writes succeeded, so rename temp files
        os.replace(temp_include, filepath)
        os.replace(temp_occlude, occlude_filepath)

        # If most recent backup is identical to the new file, remove it
        for backed_up_file in [filepath, occlude_filepath]:
            most_recent_backup = most_recent_backup_name(backed_up_file)
            if most_recent_backup:
                with open(backed_up_file, 'r') as f:
                    new_contents = f.read()
                with open(most_recent_backup, 'r') as f:
                    old_contents = f.read()
                if new_contents == old_contents:
                    os.remove(most_recent_backup)

        # Run a quick health check
        run_quick_check(graph)

    except (CycleDetectedException, ValueError) as e:
        # Clean up temp files if they exist
        for temp_file in [temp_include, temp_occlude]:
            try:
                os.remove(temp_file)
            except OSError:
                pass
        raise click.ClickException(str(e))

def save_log_files(filepath: str, occlude_filepath: str, logs: LogCollection):
    """
    Safely saves all log entries, split appropriately between
    include and occlude files, with transaction-like behavior.

    Args:
        filepath: Path to the logs file to save
        occlude_filepath: Path to the occlude logs file to save
        logs: LogCollection object containing entries to save
    """
    try:
        # Create temporary files
        temp_include = filepath + '.temp'
        temp_occlude = occlude_filepath + '.temp'

        # Write include logs to temporary file
        with open(temp_include, 'w') as f:
            for log in logs:
                if not log.occlude:
                    f.write(log.to_line() + '\n')

        # Write occluded logs to temporary file
        with open(temp_occlude, 'w') as f:
            for log in logs:
                if log.occlude:
                    f.write(log.to_line() + '\n')

        # If we get here, both writes succeeded, so rename temp files
        os.replace(temp_include, filepath)
        os.replace(temp_occlude, occlude_filepath)

        # If most recent backup is identical to the new file, remove it
        for backed_up_file in [filepath, occlude_filepath]:
            most_recent_backup = most_recent_backup_name(backed_up_file)
            if most_recent_backup:
                with open(backed_up_file, 'r') as f:
                    new_contents = f.read()
                with open(most_recent_backup, 'r') as f:
                    old_contents = f.read()
                if new_contents == old_contents:
                    os.remove(most_recent_backup)

    except Exception as e:
        # Clean up temp files if they exist
        for temp_file in [temp_include, temp_occlude]:
            try:
                os.remove(temp_file)
            except OSError:
                pass
        raise click.ClickException(f"Error saving log files: {str(e)}")

def run_quick_check(graph: GianttGraph) -> None:
    """Run a quick health check after operations."""
    doctor = GianttDoctor(graph)
    issues = doctor.quick_check()
    if issues > 0:
        click.echo(
            click.style(
                f"\n{issues} or more warnings. Run 'giantt doctor' for details.",
                fg='yellow'
            )
        )

@click.group()
def cli():
    """Giantt command line utility for managing task dependencies."""
    pass

@cli.command()
@click.option('--dev', is_flag=True, help='Initialize for development')
@click.option('--data-dir', type=click.Path(), help='Custom data directory location')
def init(dev: bool, data_dir: str):
    """Initialize Giantt directory structure and files."""

    # Determine base directory
    if dev:
        # Use local directory in dev mode
        base_dir = Path.cwd() / '.giantt'
    else:
        # Use ~/.giantt in normal mode
        base_dir = Path.home() / '.giantt'

    # Override with custom location if specified
    if data_dir:
        base_dir = Path(data_dir)

    # Create directory structure
    dirs = [
        base_dir / 'include',
        base_dir / 'occlude'
    ]

    for dir_path in dirs:
        dir_path.mkdir(parents=True, exist_ok=True)

    # Create initial files if they don't exist
    files = {
        base_dir / 'include' / 'items.txt': ITEMS_FILE_BANNER,
        base_dir / 'include' / 'metadata.json': "{}",
        base_dir / 'include' / 'logs.jsonl': "",
        base_dir / 'occlude' / 'items.txt': ITEMS_ARCHIVE_FILE_BANNER,
        base_dir / 'occlude' / 'metadata.json': "{}",
        base_dir / 'occlude' / 'logs.jsonl': ""
    }

    already_exists = set()

    for file_path, initial_content in files.items():
        if file_path.exists():
            already_exists.add(file_path)
        else:
            with open(file_path, 'w') as f:
                f.write(initial_content)

    if already_exists == set(files.keys()):
        click.echo(f"Giantt is already initialized at {base_dir}. Enjoy!")
    else:
        click.echo(f"Initialized Giantt at {base_dir}")

def show_one_item(graph, substring):
    # If there is an exact match to an ID, select that item. Otherwise, find by title substring
    error = None
    item = None
    if substring in graph.items:
        item = graph.items[substring]
    else:
        try:
            item = graph.find_by_substring(substring)
        except ValueError as e:
            error = str(e)
            click.echo(f"{error}")
    if item:
        click.echo(f"Title: {item.title}")
        click.echo(f"ID: {item.id}")
        click.echo(f"Status: {item.status.name}")
        click.echo(f"Priority: {item.priority.name}")
        click.echo(f"Duration: {item.duration}")
        click.echo(f"Charts: {', '.join(item.charts)}")
        click.echo(f"Tags: {', '.join(item.tags) if item.tags else 'None'}")
        click.echo(f"Time Constraint: {item.time_constraint}")
        click.echo("Relations:")
        for rel_type, targets in item.relations.items():
            click.echo(f"    - {rel_type}: {', '.join(targets)}")
        click.echo(f"Comment: {item.user_comment}")
        click.echo(f"Auto Comment: {item.auto_comment}")

def show_chart(graph, substring):
    chart_items = {}
    for item in graph.items.values():
        for chart in item.charts:
            if substring.lower() in chart.lower():
                if chart not in chart_items:
                    chart_items[chart] = []
                chart_items[chart].append(item)
    for chart, items in chart_items.items():
        click.echo(f"Chart '{chart}':")
        for item in items:
            click.echo(f"  - {item.id} {item.title}")
    if not chart_items:
        click.echo(f"No items found in chart '{substring}'")


def show_logs(logs, substring):
    # search in logs and log sessions
    log_entries = logs.get_by_session(substring)
    if log_entries:
        click.echo(f"Logs for session '{substring}':")
        for entry in log_entries:
            click.echo(f"  - {entry}")
    else:
        click.echo(f"No logs found for session '{substring}'")
    log_entries = logs.get_by_substring(substring)
    if log_entries:
        click.echo(f"Logs matching '{substring}':")
        for entry in log_entries:
            click.echo(f"  - {entry}")

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.option('--log-file', '-l', default=None, help='Giantt log file to use')
@click.option('--occlude-log-file', '-al', default=None, help='Giantt occlude log file to use')
@click.option('--chart', is_flag=True, default=False, help='Search in chart names')
@click.option('--log', is_flag=True, default=False, help='Search in logs and log sessions')
@click.argument('substring')
def show(file: str, occlude_file: str, log_file: str, occlude_log_file: str, chart: bool, log: bool, substring: str):
    """Show details of an item matching the substring."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    log_file = log_file or get_default_giantt_path('logs.jsonl')
    occlude_log_file = occlude_log_file or get_default_giantt_path('logs.jsonl', occlude=True)

    graph, log_collection = load_graph_and_logs(file, occlude_file, log_file, occlude_log_file)
    if not chart and not log:
        show_one_item(graph, substring)
    if chart:
        show_chart(graph, substring)
    if log:
        show_logs(log_collection, substring)

@cli.command()
@click.option('--file', '-f', default=None, help='Logs file to use')
@click.option('--occlude-file', '-a', default=None, help='Occluded logs file to use')
@click.argument('session')
@click.option('--tags', help='Additional comma-separated tags')
@click.argument('message')
def log(file: str, occlude_file: str, session: str, tags: str, message: str):
    """Create a log entry with session tag and message.

    session: The session tag for the log entry.
    tags: Optional additional tags for the log entry.
    message: The message to log.

    The log entry will be appended to logs.jsonl in the include directory.
    Each entry includes:
    - Timestamp
    - Session tag
    - Additional tags (optional)
    - Message

    Example usage:
    $ giantt log rim0 --tags planning,ideas "Initial brainstorming session"
    """
    file = file or get_default_giantt_path('logs.jsonl')
    occlude_file = occlude_file or get_default_giantt_path('logs.jsonl', occlude=True)
    logs = load_logs(file, occlude_file)
    logs.create_entry(session, message, tags.split(',') if tags else None)
    save_log_files(file, occlude_file, logs)
    click.echo(f"Log entry created with session tag '{session}'")

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.argument('substring')
@click.argument('new_status', type=click.Choice([s.name for s in Status]))
def set_status(file: str, occlude_file: str, substring: str, new_status: str):
    """Set the status of an item."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    graph = load_graph(file, occlude_file)
    try:
        item = graph.find_by_substring(substring)
    except ValueError as e:
        raise click.ClickException(str(e))
    item.status = Status[new_status]
    save_graph_files(file, occlude_file, graph)
    click.echo(f"Set status of item '{item.id}' to {new_status}")

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.argument('id')
@click.argument('title')
@click.option('--duration', default='1d', help='Duration (e.g., 1d, 2w, 3mo)')
@click.option('--priority', type=click.Choice([p.name for p in Priority]), default='NEUTRAL')
@click.option('--charts', help='Comma-separated list of chart names')
@click.option('--tags', help='Comma-separated list of tags')
@click.option('--status', type=click.Choice([s.name for s in Status]), default='NOT_STARTED')
@click.option('--requires', help='Comma-separated list of item IDs that this item requires')
@click.option('--any-of', help='Comma-separated list of item IDs that are individually sufficient for this item')
def add(file: str, occlude_file: str, id: str, title: str, duration: str, priority: str, 
        charts: str, tags: str, status: str, requires: str, any_of: str):
    """Add a new item to the Giantt chart."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    graph = load_graph(file, occlude_file)

    # Validate ID is unique and string search for this ID or title won't conflict with other titles
    for item in graph.items.values():
        if item.id == id:
            raise click.ClickException(f"Item ID '{id}' already exists\n"
                                       f"Existing item: {item.id} - {item.title}")
        if id.lower() in item.title.lower():
            raise click.ClickException(f"Item ID '{id}' conflicts with title of another item\n"
                                       f"Conflicting item: {item.id} - {item.title}")
        if title.lower() in item.title.lower():
            raise click.ClickException(f"Title '{title}' conflicts with title of another item\n"
                                       f"Conflicting item: {item.id} - {item.title}")

    # Create relations dict
    relations = {}
    if requires:
        relations['REQUIRES'] = requires.split(',')

    if any_of:
        relations['ANYOF'] = any_of.split(',')

    # Create new item
    try:
        item = GianttItem(
            id=id,
            title=title,
            description="",  # Not currently supported
            status=Status[status],
            priority=Priority[priority],
            duration=Duration.parse(duration),
            charts=charts.split(',') if charts else [],
            tags=tags.split(',') if tags else [],
            relations=relations,
            time_constraint=None,
            user_comment=None,
            auto_comment=None
        )
    except ValueError as e:
        raise click.ClickException(f"Error: {str(e)}")

    # Add item
    graph.add_item(item)

    # Try to save, catching potential cycle issues
    try:
        save_graph_files(file, occlude_file, graph)
        click.echo(f"Added item '{id}'")
    except CycleDetectedException as e:
        click.echo(f"Error: {str(e)}", err=True)
        click.echo("\nThe new item would create a dependency cycle. Please revise the relations.", err=True)
    except ValueError as e:
        click.echo(f"Error: {str(e)}", err=True)
        click.echo("\nPlease fix invalid dependencies before adding the item.", err=True)

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.option('--force', '-f', is_flag=True, help='Force removal without confirmation')
@click.argument('item_id')
@click.option('--keep-relations', is_flag=True, default=False, help='Keep relations to other items')
def remove(file: str, occlude_file: str, force: bool, item_id: str, keep_relations: bool):
    """Remove an item from the Giantt chart and clean up relations."""

    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)

    graph = load_graph(file, occlude_file)

    # Find the item
    if item_id not in graph.items:
        click.echo(click.style(f"Error: Item '{item_id}' not found.", fg='red'), err=True)
        return

    item = graph.items[item_id]

    if not force:
        # Display item details
        click.echo(click.style("\nItem to be removed:", fg='yellow', bold=True))
        click.echo(f"  ID: {item.id}")
        click.echo(f"  Title: {item.title}")
        click.echo(f"  Status: {item.status.name}")
        click.echo(f"  Priority: {item.priority.name}")
        click.echo(f"  Duration: {item.duration}")
        click.echo(f"  Charts: {', '.join(item.charts) if item.charts else 'None'}")
        click.echo(f"  Tags: {', '.join(item.tags) if item.tags else 'None'}")

        # Count affected relations
        relation_counts = {rel: 0 for rel in RelationType._member_names_}
        for other_item in graph.items.values():
            for rel_type, targets in other_item.relations.items():
                if item_id in targets:
                    relation_counts[rel_type] += 1

        # Display relation impact
        if any(relation_counts.values()):
            click.echo(click.style("\nRelations that will be affected" + (" (but not removed):" if keep_relations else ":"), fg='yellow', bold=True))
            for rel_type, count in relation_counts.items():
                if count > 0:
                    click.echo(f"  {rel_type}: {count} references removed")
        else:
            click.echo(click.style("\nNo relations will be affected.", fg='cyan'))

        # Confirm deletion
        confirm = click.prompt("\nConfirm removal? (y/N)", default="N").strip().lower()
        if confirm != 'y':
            click.echo(click.style("Aborted. No changes made.", fg='cyan'))
            return

    # Remove the item from the graph
    del graph.items[item_id]

    if not keep_relations:
        # Remove references in other items
        for other_item in graph.items.values():
            for rel_type in other_item.relations:
                other_item.relations[rel_type] = [t for t in other_item.relations[rel_type] if t != item_id]

    # Save changes
    save_graph_files(file, occlude_file, graph)

    click.echo(click.style(f"\nSuccessfully removed '{item_id}' and cleaned up relations.", fg='green'))

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.option('--add', is_flag=True, help='Add a relation')
@click.option('--remove', is_flag=True, help='Remove a relation')
@click.argument('substring')
@click.argument('property')
@click.argument('value')
def modify(file: str, occlude_file: str, add: bool, remove: bool, substring: str, property: str, value: str):
    """Modify any property of a Giantt item."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    graph = load_graph(file, occlude_file)
    try:
        item = graph.find_by_substring(substring)
    except ValueError as e:
        raise click.ClickException(str(e))

    if add and remove:
        raise click.ClickException("Cannot add and remove a relation at the same time")

    # Handle relations
    relation_types = {r.name.lower() for r in RelationType}
    if (add or remove) and property.lower() not in relation_types:
        raise click.ClickException(f"Invalid relation type. Must be one of: {', '.join(relation_types)}")

    if property.lower() in relation_types:
        relation_type = property.upper()
        targets = [t.strip() for t in value.split(',') if t.strip()]

        if add:
            item.relations.setdefault(relation_type, []).extend(targets)
        elif remove:
            if relation_type not in item.relations:
                raise click.ClickException(f"No {relation_type} relations to remove")
            item.relations[relation_type] = [t for t in item.relations.get(relation_type, []) if t not in targets]

        # Prevent cycles when modifying REQUIRES relations
        if relation_type == 'REQUIRES':
            temp_graph = graph.copy()
            temp_graph.items[item.id] = item.copy()
            try:
                temp_graph.topological_sort()
            except CycleDetectedException as e:
                raise click.ClickException(f"Adding these requirements would create a cycle: {' -> '.join(e.cycle_items)}")

    # Handle standard properties
    else:
        if property == 'title':
            item.title = value
        elif property == 'duration':
            item.duration = Duration.parse(value)
        elif property == 'priority':
            try:
                item.priority = Priority[value.upper()]
            except KeyError:
                raise click.ClickException(f"Invalid priority. Must be one of: {', '.join(p.name for p in Priority)}")
        elif property == 'status':
            try:
                item.status = Status[value.upper()]
            except KeyError:
                raise click.ClickException(f"Invalid status. Must be one of: {', '.join(s.name for s in Status)}")
        elif property == 'charts':
            item.charts = [c.strip() for c in value.split(',') if c.strip()]
        elif property == 'tags':
            item.tags = [t.strip() for t in value.split(',') if t.strip()]
        else:
            raise click.ClickException(
                f"Unknown property. Must be one of: title, duration, priority, status, charts, {', '.join(relation_types)}, or tags")

    save_graph_files(file, occlude_file, graph)
    click.echo(f"Modified {property} of item '{item.id}'")


@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
def sort(file: str, occlude_file: str):
    """Sort items in topological order and save."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    graph = load_graph(file, occlude_file)
    try:
        save_graph_files(file, occlude_file, graph)
        click.echo("Successfully sorted and saved items.")
    except CycleDetectedException as e:
        click.echo(f"Error: {str(e)}", err=True)
        click.echo("\nPlease resolve the cycle before sorting.", err=True)
    except ValueError as e:
        click.echo(f"Error: {str(e)}", err=True)
        click.echo("\nPlease fix invalid dependencies before sorting.", err=True)

#giantt touch command: same as sort (just load and save) but for both graph and logs
@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--recursive', '-r', is_flag=True, help='Show recursive includes')
def includes(file: str, recursive: bool):
    """Show the include structure of a Giantt items file."""
    file = file or get_default_giantt_path()
    
    def process_file(filepath: str, depth: int = 0, visited: Optional[Set[str]] = None) -> None:
        if visited is None:
            visited = set()
            
        if filepath in visited:
            click.echo(f"{'  ' * depth}└─ {filepath} (circular include, skipping)")
            return
            
        visited.add(filepath)
        
        if not os.path.exists(filepath):
            click.echo(f"{'  ' * depth}└─ {filepath} (file not found)")
            return
            
        click.echo(f"{'  ' * depth}└─ {filepath}")
        
        if recursive:
            includes = parse_include_directives(filepath)
            for include_path in includes:
                # Handle relative paths
                if not os.path.isabs(include_path):
                    base_dir = os.path.dirname(filepath)
                    include_path = os.path.join(base_dir, include_path)
                
                process_file(include_path, depth + 1, visited)
    
    process_file(file)

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.option('--log-file', '-l', default=None, help='Giantt log file to use')
@click.option('--occlude-log-file', '-al', default=None, help='Giantt occlude log file to use')
def touch(file: str, occlude_file: str, log_file: str, occlude_log_file: str):
    """Touch items and logs files to trigger a reload and save."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    log_file = log_file or get_default_giantt_path('logs.jsonl')
    occlude_log_file = occlude_log_file or get_default_giantt_path('logs.jsonl', occlude=True)
    graph, logs = load_graph_and_logs(file, occlude_file, log_file, occlude_log_file)
    save_graph_files(file, occlude_file, graph)
    save_log_files(log_file, occlude_log_file, logs)
    click.echo("Touched items and logs files")

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.argument('new_id')
@click.argument('before_id')
@click.argument('after_id')
@click.option('--charts', help='Comma-separated list of charts')
@click.option('--tags', help='Comma-separated list of tags')
@click.option('--duration', default='1d', help='Duration (e.g., 1d, 2w, 3mo2w5d3s)')
@click.option('--priority', type=click.Choice([p.name for p in Priority]), 
              default='NEUTRAL', help='Priority level')
def insert(file: str, occlude_file: str, new_id: str, before_id: str, after_id: str,
          charts: str, tags: str, duration: str, priority: str):
    """Insert a new item between two existing items."""
    file = file or get_default_giantt_path()
    occlude_file = occlude_file or get_default_giantt_path(occlude=True)
    graph = load_graph(file, occlude_file)

    try:
        new_item = GianttItem(
            id=new_id,
            priority=Priority[priority],
            duration=duration,
            charts=charts.split(',') if charts else [],
            tags=tags.split(',') if tags else []
        )
    except ValueError as e:
        raise click.ClickException(f"Error: {str(e)}")

    graph.insert_between(new_item, before_id, after_id)
    save_graph_files(file, occlude_file, graph)

@cli.group()
def occlude():
    """Occlude items or logs."""
    # This is not a placeholder function, it's a click command group
    pass

@occlude.command() # This is part of the occlude command group
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.option('--tag', '-t', multiple=True, help='Occlude items with specific tags')
@click.option('--dry-run/--no-dry-run', default=False, help='Show what would be occluded without making changes')
@click.argument('identifiers', nargs=-1)
def items(file: str, occlude_file: str, tag: Tuple[str, ...], dry_run: bool, identifiers: Tuple[str, ...]):
    """Occlude Giantt items.

    Can occlude by specifying IDs directly and/or by providing tags.

    Examples:
        # Occlude specific items
        giantt occluded items item1 item2

        # Occlude items by tag
        giantt occluded items -t project1 -t phase1

        # Do a dry run first
        giantt occluded items --dry-run -t project1
    """
    # Get source and destination files
    items_file = file or get_default_giantt_path('items.txt')
    items_occlude = occlude_file or get_default_giantt_path(occlude=True)

    # Load current items
    graph = load_graph(items_file, items_occlude)

    # Load metadata (we will come back to metadata)
    # try:
    #     with open(metadata_file) as f:
    #         metadata = json.load(f)
    # except (FileNotFoundError, json.JSONDecodeError):
    #     metadata = {}


    # try: (we will come back to metadata)
    #     with open(metadata_occlude) as f:
    #         occluded_metadata = json.load(f)
    # except (FileNotFoundError, json.JSONDecodeError):
    #     occluded_metadata = {}

    # Find items to occlude
    to_occlude = set()

    # Add items by ID
    for id in identifiers:
        if id in graph.included_items():
            to_occlude.add(id)
        else:
            click.echo(f"Warning: Item '{id}' not found in included items", err=True)

    # Add items by tag
    for t in tag:
        for item in graph.included_items():
            if t in graph.items[item].tags:
                to_occlude.add(item)

    if not to_occlude:
        click.echo("No included items found to occlude")
        return

    # In dry-run mode, just show what would be occluded
    if dry_run:
        click.echo("The following items would be occluded:")
        for item_id in sorted(to_occlude):
            item = graph.items[item_id]
            click.echo(f"  • {item_id}: {item.title}")
        return

    # Set occlude status for items
    for id in to_occlude:
        graph.items[id].set_occlude(True)

    # Save updated items
    save_graph_files(items_file, items_occlude, graph)
    click.echo(f"Occluded {len(to_occlude)} item" + ("s" if len(to_occlude) != 1 else ""))

@occlude.command() # This is part of the occlude command group
@click.option('--file', '-f', default=None, help='Giantt logs file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occlude logs file to use')
@click.option('--tag', '-t', multiple=True, help='Occlude logs with specific tags')
@click.option('--dry-run/--no-dry-run', default=False, help='Show what would be occluded without making changes')
@click.argument('identifiers', nargs=-1)
def logs(file: str, occlude_file: str, tag: Tuple[str, ...], dry_run: bool, identifiers: Tuple[str, ...]):
    """Occlude log entries.

    Can occlude by specifying IDs directly and/or by providing tags.

    Examples:
        # Occlude specific logs
        giantt occlude logs log1 log2

        # Occlude logs by tag
        giantt occlude logs -t debug -t test

        # Do a dry run first
        giantt occlude logs --dry-run -t debug
    """
    # Get source and destination files
    logs_file = file or get_default_giantt_path('logs.jsonl')
    logs_occlude = occlude_file or get_default_giantt_path('logs.jsonl', occlude=True)

    # Load current logs
    logs = load_logs(logs_file, logs_occlude)

    # Find logs to occlude
    to_occlude = []

    for log in logs.include_entries():
        should_occlude = False

        # Check if log has matching ID
        if log.session in identifiers:
            should_occlude = True

        # Check if log has matching tag
        if any(t in log.tags for t in tag):
            should_occlude = True

        if should_occlude:
            to_occlude.append(log)

    if not to_occlude:
        click.echo("No include logs found to occlude")
        return

    # In dry-run mode, just show what would be occluded
    if dry_run:
        click.echo("The following logs would be occluded:")
        for log in to_occlude:
            # time needs to be formatted to be human-readable
            click.echo(f"  • {log.message} ({', '.join(log.tags)}) {log.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        return

    # Set occlude status for logs
    for log in to_occlude:
        log.set_occlude(True)

    # Save updated logs
    save_log_files(logs_file, logs_occlude, logs)

    click.echo(f"Occluded {len(to_occlude)} log" + ("s" if len(to_occlude) != 1 else ""))

@cli.command()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.argument('include_path')
def add_include(file: str, include_path: str):
    """Add an include directive to a Giantt items file."""
    file = file or get_default_giantt_path()
    
    if not os.path.exists(file):
        raise click.ClickException(f"File not found: {file}")
    
    # Read the current file content
    with open(file, 'r') as f:
        content = f.readlines()
    
    # Find where to insert the include directive
    insert_pos = 0
    for i, line in enumerate(content):
        if line.strip().startswith('#include '):
            insert_pos = i + 1
        elif line.strip() and not line.strip().startswith('#'):
            break
    
    # Create a backup
    shutil.copyfile(file, increment_backup_name(file))
    
    # Insert the include directive
    content.insert(insert_pos, f"#include {include_path}\n")
    
    # Write the updated content
    with open(file, 'w') as f:
        f.writelines(content)
    
    click.echo(f"Added include directive for {include_path} to {file}")

@cli.command()
@click.option('--yes', '-y', is_flag=True, help='Skip confirmation prompt')
@click.option('--keep', '-k', default=3, help='Number of recent backups to keep')
def clean(yes: bool, keep: int):
    """Clean up backup files, keeping only the most recent few backups.

    By default, keeps the 3 most recent backups and renames them to .1.backup (oldest),
    .2.backup, and .3.backup (newest).
    """

    # Get paths using the existing function
    try:
        include_items_path = get_default_giantt_path()
        occlude_items_path = get_default_giantt_path(occlude=True)
    except click.ClickException:
        click.echo("Giantt is not initialized. Run 'giantt init' or giantt init --dev' first,\nor navigate to your local dev directory.")
        return

    # Extract directories
    include_dir = Path(include_items_path).parent
    occlude_dir = Path(occlude_items_path).parent

    items_pattern = re.compile(r'items\.txt\.(\d+)\.backup$')
    logs_pattern = re.compile(r'logs\.jsonl\.(\d+)\.backup$')

    # Find all backup files
    backup_files = []

    for directory in [include_dir, occlude_dir]:
        for filename in directory.glob('*.backup'):
            if items_pattern.search(filename.name) or logs_pattern.search(filename.name):
                backup_files.append(filename)

    if not backup_files:
        click.echo("No backup files found.")
        return

    # Group by base filename
    grouped_backups = {}
    for filepath in backup_files:
        base_name = filepath.name.split('.', 1)[0] + '.' + filepath.name.split('.', 2)[1]  # e.g., "items.txt" or "logs.jsonl"
        directory = filepath.parent
        key = (directory, base_name)

        if key not in grouped_backups:
            grouped_backups[key] = []

        backup_num = int(filepath.name.split('.')[-2])  # Extract backup number
        grouped_backups[key].append((backup_num, filepath))

    # Sort each group by backup number (descending) and determine files to delete
    to_delete = []
    to_rename = {}

    for (directory, base_name), backups in grouped_backups.items():
        # Sort by backup number (highest first)
        backups.sort(key=lambda x: x[0], reverse=True)

        # Keep only the most recent 'keep' backups
        if len(backups) > keep:
            to_delete.extend([filepath for _, filepath in backups[keep:]])

        # Rename the kept backups to .1.backup, .2.backup, etc.
        # Oldest backup gets .1.backup, newest gets .<keep>.backup
        kept_backups = backups[:keep]
        kept_backups.reverse()  # Reverse so oldest is first

        for i, (_, filepath) in enumerate(kept_backups):
            new_filename = directory / f"{base_name}.{i+1}.backup"
            to_rename[filepath] = new_filename

    # Show summary and confirm
    click.echo(f"Found {len(backup_files)} backup files across all directories.")
    click.echo(f"Will keep {min(keep, len(backup_files))} most recent backups of each file.")
    if not to_delete:
        click.echo("No files to delete.")
        return
    click.echo(f"Will delete {len(to_delete)} old backup files.")

    if to_delete and not yes:
        click.confirm("Do you want to proceed?", abort=True)

    # Perform operations
    temp_dir = Path(tempfile.mkdtemp())
    try:
        # First move files to be deleted to a temp directory
        for filepath in to_delete:
            shutil.move(str(filepath), str(temp_dir / filepath.name))

        # Then rename the files to be kept
        # We need to handle rename conflicts carefully
        rename_plan = []
        for old_path, new_path in to_rename.items():
            if old_path.exists():  # May have been deleted in cleanup
                # If target exists, need temp name
                if new_path.exists() and old_path != new_path:
                    temp_path = temp_dir / f"temp_{old_path.name}"
                    rename_plan.append((old_path, temp_path))
                    rename_plan.append((temp_path, new_path))
                else:
                    rename_plan.append((old_path, new_path))

        # Execute renames in order
        for old_path, new_path in rename_plan:
            shutil.move(str(old_path), str(new_path))

        # Finally delete the temp directory with all files to be deleted
        shutil.rmtree(temp_dir)

        click.echo("Backup cleanup completed successfully!")

    except Exception as e:
        click.echo(f"Error during cleanup: {e}", err=True)
        click.echo("Attempting to recover...", err=True)
        try:
            # Try to restore any moved files
            for filepath in to_delete:
                temp_path = temp_dir / filepath.name
                if temp_path.exists():
                    shutil.move(str(temp_path), str(filepath))
            shutil.rmtree(temp_dir)
            click.echo("Recovery completed.", err=True)
        except Exception as recovery_error:
            click.echo(f"Recovery failed: {recovery_error}", err=True)
            click.echo(f"Some files may have been moved to temporary directory: {temp_dir}", err=True)

@cli.group()
@click.option('--file', '-f', default=None, help='Giantt items file to use')
@click.option('--occlude-file', '-a', default=None, help='Giantt occluded items file to use')
@click.pass_context
def doctor(ctx, file: str, occlude_file: str):
    """Check the health of the Giantt graph and fix issues."""
    ctx.ensure_object(dict)
    ctx.obj['file'] = file or get_default_giantt_path()
    ctx.obj['occlude_file'] = occlude_file or get_default_giantt_path(occlude=True)
    ctx.obj['graph'] = load_graph(ctx.obj['file'], ctx.obj['occlude_file'])
    ctx.obj['doctor'] = GianttDoctor(ctx.obj['graph'])

@doctor.command('check')
@click.pass_context
def doctor_check(ctx):
    """Check the health of the Giantt graph and report issues."""
    doctor = ctx.obj['doctor']
    issues = doctor.full_diagnosis()

    if not issues:
        click.echo(click.style("✓ Graph is healthy!", fg='green'))
        return

    # Group issues by type
    issues_by_type: Dict[IssueType, List[Issue]] = {}
    for issue in issues:
        if issue.type not in issues_by_type:
            issues_by_type[issue.type] = []
        issues_by_type[issue.type].append(issue)

    # Print issues
    click.echo(click.style(f"\nFound {len(issues)} issue" + ("s" if len(issues) != 1 else "") + ":", fg='yellow'))
    for issue_type, type_issues in issues_by_type.items():
        click.echo(f"\n{issue_type.value} ({len(type_issues)} issues):")
        for issue in type_issues:
            click.echo(f"  • {issue.item_id}: {issue.message}")
            if issue.suggested_fix:
                click.echo(f"    Suggested fix: {issue.suggested_fix}")

@doctor.command('fix')
@click.option('--type', '-t', 'issue_type', help='Type of issue to fix (e.g., dangling_reference)')
@click.option('--item', '-i', help='Fix issues for a specific item ID')
@click.option('--all', '-a', is_flag=True, help='Fix all fixable issues')
@click.option('--dry-run', is_flag=True, help='Show what would be fixed without making changes')
@click.pass_context
def doctor_fix(ctx, issue_type: str, item: str, all: bool, dry_run: bool):
    """Fix issues in the Giantt graph.
    
    Examples:
        # Fix all dangling references
        giantt doctor fix --type dangling_reference
        
        # Fix all issues for a specific item
        giantt doctor fix --item item123
        
        # Fix all fixable issues
        giantt doctor fix --all
        
        # Do a dry run first
        giantt doctor fix --all --dry-run
    """
    doctor = ctx.obj['doctor']
    graph = ctx.obj['graph']
    file = ctx.obj['file']
    occlude_file = ctx.obj['occlude_file']
    
    # Run diagnosis first
    issues = doctor.full_diagnosis()
    
    if not issues:
        click.echo(click.style("✓ Graph is healthy! No issues to fix.", fg='green'))
        return
        
    # Filter issues based on options
    issues_to_fix = []
    
    if issue_type:
        try:
            issue_type_enum = IssueType.from_string(issue_type)
            issues_to_fix = doctor.get_issues_by_type(issue_type_enum)
            if not issues_to_fix:
                click.echo(f"No issues of type '{issue_type}' found.")
                return
        except ValueError:
            valid_types = [t.value for t in IssueType]
            click.echo(f"Invalid issue type: {issue_type}. Valid types are: {', '.join(valid_types)}")
            return
    elif item:
        issues_to_fix = [i for i in issues if i.item_id == item]
        if not issues_to_fix:
            click.echo(f"No issues found for item '{item}'.")
            return
    elif all:
        issues_to_fix = issues
    else:
        click.echo("Please specify --type, --item, or --all to indicate which issues to fix.")
        return
        
    # Show what would be fixed
    click.echo(click.style(f"\nFound {len(issues_to_fix)} issue(s) that can be fixed:", fg='yellow'))
    for issue in issues_to_fix:
        click.echo(f"  • {issue.item_id}: {issue.message}")
        if issue.suggested_fix:
            click.echo(f"    Suggested fix: {issue.suggested_fix}")
            
    if dry_run:
        click.echo("\nDry run - no changes made.")
        return
        
    # Confirm before fixing
    if not click.confirm("\nDo you want to fix these issues?"):
        click.echo("Aborted. No changes made.")
        return
        
    # Fix issues
    fixed_issues = doctor.fix_issues(
        issue_type=issue_type_enum if issue_type else None,
        item_id=item
    )
    
    if fixed_issues:
        # Save changes
        save_graph_files(file, occlude_file, graph)
        click.echo(click.style(f"\nSuccessfully fixed {len(fixed_issues)} issue(s):", fg='green'))
        for issue in fixed_issues:
            click.echo(f"  • {issue.item_id}: {issue.message}")
    else:
        click.echo("\nNo issues were fixed. Some issues may require manual intervention.")

@doctor.command('list-types')
def doctor_list_types():
    """List all available issue types that can be fixed."""
    click.echo("Available issue types:")
    for issue_type in IssueType:
        click.echo(f"  • {issue_type.value}")

if __name__ == '__main__':
    cli()


# =====================================================================
# FILE: docs/port_reference/giantt_core.py
# =====================================================================

from typing import List, Optional, Tuple, Dict, Set
from dataclasses import dataclass, field
import re
from enum import Enum
import json
from datetime import datetime, timezone
from pathlib import Path

class Status(Enum):
    NOT_STARTED = "○"
    IN_PROGRESS = "◑"
    BLOCKED = "⊘"
    COMPLETED = "●"

class Priority(Enum):
    LOWEST = ",,,"
    LOW = "..."
    NEUTRAL = ""
    UNSURE = "?"
    MEDIUM = "!"
    HIGH = "!!"
    CRITICAL = "!!!"

### Relations
# Each relation type uses a symbol followed by comma-separated target IDs:
# - `⊢` REQUIRES (left side tick)
# - `⋲` ANYOF (alternate paths)
# - `≫` SUPERCHARGES (much greater than - suggests enhancement)
# - `∴` INDICATES (therefore - suggests consequence)
# - `∪` TOGETHER (shows combination)
# - `⊟` CONFLICTS (suggests blocking)
# - `►` (redundancy symbol) BLOCKS (shows all items with ⊢ [REQUIRES] to this item)
# - `≻` (redundancy symbol) SUFFICIENT (shows all items with ⋲ [ANY] to this item)

class RelationType(Enum):
    REQUIRES = "⊢"
    ANYOF = "⋲"
    SUPERCHARGES = "≫"
    INDICATES = "∴"
    TOGETHER = "∪"
    CONFLICTS = "⊟"
    BLOCKS = "►"
    SUFFICIENT = "≻"

class TimeConstraintType(Enum):
    WINDOW = "window"
    DEADLINE = "deadline"
    RECURRING = "recurring"

class ConsequenceType(Enum):
    SEVERE = "severe"
    WARNING = "warn"
    ESCALATING = "escalating"

class EscalationRate(Enum):
    LOWEST = ",,,"
    LOW = "..."
    NEUTRAL = ""
    UNSURE = "?"
    MEDIUM = "!"
    HIGH = "!!"
    CRITICAL = "!!!"

class TimeConstraintType(Enum):
    WINDOW = "window"
    DEADLINE = "deadline"
    RECURRING = "recurring"

class ConsequenceType(Enum):
    SEVERE = "severe"
    WARNING = "warn"
    ESCALATING = "escalating"

class EscalationRate(Enum):
    LOWEST = ",,,"
    LOW = "..."
    NEUTRAL = ""
    UNSURE = "?"
    MEDIUM = "!"
    HIGH = "!!"
    CRITICAL = "!!!"


@dataclass(frozen=True)
class DurationPart:
    """Represents a single part of a duration with an amount and unit."""
    amount: float
    unit: str

    _UNIT_SECONDS = {
        's': 1,
        'min': 60,
        'h': 3600,
        'hr': 3600,
        'd': 86400,
        'w': 604800,
        'mo': 2592000,  # 30 days
        'y': 31536000,  # 365 days
    }

    _UNIT_NORMALIZE = {
        'hr': 'h',
        'minute': 'min',
        'minutes': 'min',
        'hour': 'h',
        'hours': 'h',
        'day': 'd',
        'days': 'd',
        'week': 'w',
        'weeks': 'w',
        'month': 'mo',
        'months': 'mo',
        'year': 'y',
        'years': 'y'
    }

    @classmethod
    def create(cls, amount: float, unit: str) -> 'DurationPart':
        """Factory method to create a normalized DurationPart."""
        normalized_unit = cls._UNIT_NORMALIZE.get(unit, unit)

        if normalized_unit not in cls._UNIT_SECONDS:
            raise ValueError(f"Invalid duration unit: {unit}")

        return cls(amount, normalized_unit)

    def __post_init__(self):
        """Validate the unit."""
        if self.unit not in self._UNIT_SECONDS:
            raise ValueError(f"Invalid duration unit: {self.unit}")

    @property
    def total_seconds(self) -> float:
        """Get total seconds."""
        return self.amount * self._UNIT_SECONDS[self.unit]

    def __str__(self):
        # For whole numbers, display as integers
        amount_str = str(int(self.amount)) if self.amount.is_integer() else f"{self.amount}"
        return f"{amount_str}{self.unit}"

    def __hash__(self):
        return hash((self.amount, self.unit))


@dataclass(frozen=True)
class Duration:
    """Handles compound durations like '6mo8d3.5s'."""

    parts: List[DurationPart] = field(default_factory=list)

    @classmethod
    def parse(cls, duration_str: str) -> 'Duration':
        """Parse a duration string into a Duration object."""
        if not duration_str:
            raise ValueError("Empty duration string")

        pattern = r'(\d+\.?\d*)([a-zA-Z]+)'
        matches = re.finditer(pattern, duration_str)
        parts = []

        for match in matches:
            amount = float(match.group(1))
            unit = match.group(2)
            parts.append(DurationPart.create(amount, unit))

        if not parts:
            raise ValueError(f"No valid duration parts found in: {duration_str}")

        return cls(parts)

    def total_seconds(self):
        """Get total duration in seconds."""
        return sum(part.total_seconds for part in self.parts)

    def __str__(self):
        """String representation of duration."""
        if not self.parts:
            return "0s"
        return "".join(str(part) for part in self.parts)

    def __add__(self, other):
        """Add two durations."""
        total_seconds = self.total_seconds() + other.total_seconds()

        # Convert back to largest sensible unit
        for unit, seconds in sorted(self._UNIT_SECONDS.items(), 
                                  key=lambda x: x[1], reverse=True):
            if total_seconds >= seconds:
                amount = total_seconds / seconds
                if amount.is_integer():
                    amount = int(amount)
                return Duration([DurationPart(amount, unit)])

        return Duration([DurationPart(total_seconds, 's')])

    def __eq__(self, other):
        """Compare two durations."""
        if not isinstance(other, Duration):
            return NotImplemented
        return self.total_seconds() == other.total_seconds()

    def __lt__(self, other):
        """Compare two durations."""
        if not isinstance(other, Duration):
            return NotImplemented
        return self.total_seconds() < other.total_seconds()

    def __gt__(self, other):
        """Compare two durations."""
        if not isinstance(other, Duration):
            return NotImplemented
        return self.total_seconds() > other.total_seconds()

    def __le__(self, other):
        """Compare two durations."""
        if not isinstance(other, Duration):
            return NotImplemented
        return self.total_seconds() <= other.total_seconds()

    def __ge__(self, other):
        """Compare two durations."""
        if not isinstance(other, Duration):
            return NotImplemented
        return self.total_seconds() >= other.total_seconds()

    def __hash__(self):
        return hash(tuple(self.parts))


@dataclass
class TimeWindow:
    """Represents a time window with an optional grace period."""
    window: DurationPart
    grace_period: Optional[DurationPart] = None

    @classmethod
    def parse(cls, window_str: str) -> 'TimeWindow':
        """Parse a time window string.

        Args:
            window_str: String like '5d' or '5d:2d' (with grace period)

        Returns:
            TimeWindow object
        """
        parts = window_str.split(':')
        window = DurationPart.parse(parts[0])
        grace_period = DurationPart.parse(parts[1]) if len(parts) > 1 else None
        return cls(window, grace_period)

    def __str__(self) -> str:
        """String representation of time window."""
        base = str(self.window)
        if self.grace_period:
            base += f":{self.grace_period}"
        return base


@dataclass
class TimeConstraint:
    type: TimeConstraintType
    duration: Duration
    grace_period: Optional[Duration] = None
    consequence_type: ConsequenceType = ConsequenceType.WARNING
    escalation_rate: EscalationRate = EscalationRate.NEUTRAL
    due_date: Optional[str] = None
    interval: Optional[Duration] = None
    stack: bool = False

    @classmethod
    def from_string(cls, constraint_str: str) -> Optional['TimeConstraint']:
        if not constraint_str:
            return None

        # Parse window constraints
        window_match = re.match(r'window\((\d+[smhdwy])(:\d+[smhdwy])?,([^)]+)\)', constraint_str)
        if window_match:
            window = Duration.parse(window_match.group(1))
            grace = Duration.parse(window_match.group(2)[1:]) if window_match.group(2) else None
            consequence = cls._parse_consequence(window_match.group(3))

            return cls(
                type=TimeConstraintType.WINDOW,
                duration=window,
                grace_period=grace,
                consequence_type=consequence['type'],
                escalation_rate=consequence['rate']
            )

        # Parse deadline constraints
        deadline_match = re.match(r'due\((\d{4}-\d{2}-\d{2})(:\d+[smhdwy])?,([^)]+)\)', constraint_str)
        if deadline_match:
            due_date = deadline_match.group(1)
            grace = Duration.parse(deadline_match.group(2)[1:]) if deadline_match.group(2) else None
            consequence = cls._parse_consequence(deadline_match.group(3))

            return cls(
                type=TimeConstraintType.DEADLINE,
                duration=Duration.parse('1d'), # Default to 1 day for deadline
                grace_period=grace,
                consequence_type=consequence['type'],
                escalation_rate=consequence['rate'],
                due_date=due_date
            )

        # Parse recurring constraints
        recurring_match = re.match(r'every\((\d+[smhdwy])(:\d+[smhdwy])?,([^)]+)\)', constraint_str)
        if recurring_match:
            interval = Duration.parse(recurring_match.group(1))
            grace = Duration.parse(recurring_match.group(2)[1:]) if recurring_match.group(2) else None
            consequence_str = recurring_match.group(3)

            stack = 'stack' in consequence_str
            consequence_str = consequence_str.replace(',stack', '')
            consequence = cls._parse_consequence(consequence_str)

            return cls(
                type=TimeConstraintType.RECURRING,
                duration=interval,
                grace_period=grace,
                consequence_type=consequence['type'],
                escalation_rate=consequence['rate'],
                interval=interval,
                stack=stack
            )

        raise ValueError(f"Invalid time constraint format: {constraint_str}")

    def __str__(self):
        base_str = {
            TimeConstraintType.WINDOW: f"window({self.duration}",
            TimeConstraintType.DEADLINE: f"due({self.due_date}",
            TimeConstraintType.RECURRING: f"every({self.interval}",
        }[self.type]

        if self.grace_period:
            base_str += f":{self.grace_period}"

        base_str += f",{self.consequence_type.value}"
        if self.escalation_rate != EscalationRate.NEUTRAL:
            base_str += f",escalate:{self.escalation_rate.value}"

        if self.type == TimeConstraintType.RECURRING and self.stack:
            base_str += ",stack"

        return base_str + ")"

    @staticmethod
    def _parse_consequence(consequence_str: str) -> dict:
        parts = consequence_str.split(',')
        base_consequence = parts[0].strip()

        if len(parts) > 1 and parts[1].startswith('escalate:'):
            rate_str = parts[1][9:]  # Remove 'escalate:'
            return {
                'type': ConsequenceType.ESCALATING,
                'rate': EscalationRate(rate_str) if rate_str else EscalationRate.NEUTRAL
            }

        return {
            'type': ConsequenceType(base_consequence),
            'rate': EscalationRate.NEUTRAL
        }


def parse_pre_title_section(pre_title: str) -> Tuple[str, str, str]:
    """Parse the pre-title section into status, id+priority, and duration."""
    # Updated pattern to be more flexible with whitespace
    pattern = r'^([○◑⊘●])\s+([^\s]+)\s+([^\s"]+)'
    match = re.match(pattern, pre_title)

    if not match:
        raise ValueError(f"Invalid pre-title format: {pre_title}")

    status = match.group(1)
    id_priority = match.group(2)
    duration = match.group(3).strip()

    return status, id_priority, duration

@dataclass
class GianttItem:
    id: str
    title: str = ""
    description: str = ""
    status: Status = Status.NOT_STARTED
    priority: Priority = Priority.NEUTRAL
    duration: Duration = Duration()
    charts: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    relations: dict = field(default_factory=dict)
    time_constraint: Optional[TimeConstraint] = None
    user_comment: Optional[str] = None
    auto_comment: Optional[str] = None
    occlude: bool = False

    # type-check everything
    def __init__(self, id: str, title: str, description: str, status: Status, priority: Priority, duration: Duration, charts: List[str], tags: List[str], relations: dict, time_constraint: Optional[TimeConstraint], user_comment: Optional[str], auto_comment: Optional[str], occlude: bool = False):
        if not isinstance(id, str):
            raise TypeError(f"id must be a string, not {type(id)}")
        if not isinstance(title, str):
            raise TypeError(f"title must be a string, not {type(title)}")
        if not isinstance(description, str):
            raise TypeError(f"description must be a string, not {type(description)}")
        if not isinstance(status, Status):
            raise TypeError(f"status must be a Status, not {type(status)}")
        if not isinstance(priority, Priority):
            raise TypeError(f"priority must be a Priority, not {type(priority)}")
        if not isinstance(duration, Duration):
            raise TypeError(f"duration must be a Duration, not {type(duration)}")
        if not isinstance(charts, list):
            raise TypeError(f"charts must be a list, not {type(charts)}")
        if not all(isinstance(i, str) for i in charts):
            raise TypeError(f"all elements of charts must be a string")
        if not isinstance(tags, list):
            raise TypeError(f"tags must be a list, not {type(tags)}")
        if not all(isinstance(i, str) for i in tags):
            raise TypeError(f"all elements of tags must be a string")
        if not isinstance(relations, dict):
            raise TypeError(f"relations must be a dict, not {type(relations)}")
        if not all(isinstance(k, str) for k in relations.keys()):
            raise TypeError(f"all keys of relations must be a string")
        if not all(isinstance(v, list) for v in relations.values()):
            raise TypeError(f"all values of relations must be a list")
        if not all(all(isinstance(i, str) for i in v) for v in relations.values()):
            raise TypeError(f"all elements of all values of relations must be a string")
        if not isinstance(time_constraint, (TimeConstraint, type(None))):
            raise TypeError(f"time_constraint must be a TimeConstraint or None, not {type(time_constraint)}")
        if not isinstance(user_comment, (str, type(None))):
            raise TypeError(f"user_comment must be a string or None, not {type(user_comment)}")
        if not isinstance(auto_comment, (str, type(None))):
            raise TypeError(f"auto_comment must be a string or None, not {type(auto_comment)}")
        if not isinstance(occlude, bool):
            raise TypeError(f"occlude must be a bool, not {type(occlude)}")

        self.id = id
        self.title = title
        self.description = description
        self.status = status
        self.priority = priority
        self.duration = duration
        self.charts = charts
        self.tags = tags
        self.relations = relations
        self.time_constraint = time_constraint
        self.user_comment = user_comment
        self.auto_comment = auto_comment
        self.occlude = occlude

    @classmethod
    def from_string(cls, line: str, occlude: bool = False) -> 'GianttItem':
        """Parse a line into a GianttItem."""
        line = line.strip()

        # Parse the pre-title section
        pre_title = line[:line.find('"')].strip()
        status_str, id_priority_str, duration_str = parse_pre_title_section(pre_title)
        status = Status(status_str)

        # Parse the title
        title_start = line.find('"')
        title_end = line.find('"', title_start + 1)
        while title_end != -1 and line[title_end - 1] == '\\':
            title_end = line.find('"', title_end + 1)

        if title_end == -1:
            raise ValueError("No ending quote found for title")

        title = json.loads(line[title_start:title_end + 1])
        post_title = line[title_end + 1:].strip()

        # Extract ID and priority
        priority_symbols = ['!!!', '!!', '!', '?', '...', ',,,']
        id_str = id_priority_str
        priority = ''
        for symbol in priority_symbols:
            if id_priority_str.endswith(symbol):
                id_str = id_priority_str[:-len(symbol)]
                priority = symbol
                break
        # must be type Priority
        priority = Priority(priority)

        # Parse duration
        duration = Duration.parse(duration_str)

        # Parse post-title section
        charts_pattern = re.compile(r'^\s*(\{[^}]+\})\s*(.*)$')
        charts_match = charts_pattern.match(post_title)
        if not charts_match:
            raise ValueError("Invalid charts format")

        charts_str = charts_match.group(1)
        remainder = charts_match.group(2)

        # Split remainder into tags, relations, and constraints
        parts = remainder.split('>>>')
        tags_str = parts[0].strip()
        relations_str = parts[1].strip() if len(parts) > 1 else ""

        # Split relations section into relations and time constraints
        constraint_parts = relations_str.split('@@@')
        relations_str = constraint_parts[0].strip()
        time_constraint_str = constraint_parts[1].strip() if len(constraint_parts) > 1 else None

        # Parse charts
        charts = [c.strip().strip('"') for c in charts_str[1:-1].split(",") if c.strip()]

        # Parse tags
        tags = [t.strip() for t in tags_str.split(",") if t.strip()]

        # Parse relations
        relations = {}
        rel_symbols = {r.value: r.name for r in RelationType}

        for symbol, rel_type in rel_symbols.items():
            pattern = f"{symbol}\\[([^]]+)\\]"
            matches = re.findall(pattern, relations_str)
            if matches:
                relations[rel_type] = [t.strip() for t in matches[0].split(",")]

        return cls(
            id=id_str,
            title=title,
            description="",  # Not currently supported
            status=status,
            priority=priority,
            duration=duration,
            charts=charts,
            tags=tags,
            relations=relations,
            time_constraint=time_constraint_str,
            user_comment=None,
            auto_comment=None,
            occlude=occlude
        )


    def to_string(self) -> str:
        charts_str = '{"' + '","'.join(self.charts) + '"}'
        tags_str = ' ' + ','.join(self.tags) if self.tags else ""

        rel_parts = []
        for rel_type, targets in self.relations.items():
            if targets:
                symbol = RelationType[rel_type].value
                rel_parts.append(f"{symbol}[{','.join(targets)}]")
        relations_str = ' >>> ' + ' '.join(rel_parts) if rel_parts else ""

        # JSON encode the title to handle special characters properly
        title_str = json.dumps(self.title)

        user_comment_str = f" # {self.user_comment}" if self.user_comment else ""
        auto_comment_str = f" ### {self.auto_comment}" if self.auto_comment else ""

        # Note: occlusion status is not included in the string representation because it only dictates where the string is saved

        return f"{self.status.value} {self.id}{self.priority.value} {self.duration} {title_str} {charts_str}{tags_str}{relations_str}{user_comment_str}{auto_comment_str}"

    def set_occlude(self, occlude: bool):
        self.occlude = occlude

    def copy(self):
        return GianttItem(
            self.id,
            self.title,
            self.description,
            self.status,
            self.priority,
            self.duration,
            self.charts.copy(),
            self.tags.copy(),
            self.relations.copy(),
            self.time_constraint,
            self.user_comment,
            self.auto_comment,
            self.occlude
        )

class CycleDetectedException(Exception):
    def __init__(self, cycle_items):
        self.cycle_items = cycle_items
        cycle_str = " -> ".join(cycle_items)
        super().__init__(f"Cycle detected in dependencies: {cycle_str}")


class GianttGraph:
    def __init__(self):
        self.items: dict[str, GianttItem] = {}

    def add_item(self, item: GianttItem):
        self.items[item.id] = item

    def find_by_substring(self, substring: str) -> GianttItem:
        matches = [item for item in self.items.values() if substring.lower() in item.title.lower() or substring == item.id]
        if not matches:
            raise ValueError(f"No items with ID '{substring}' or title containing '{substring}' found")
        if len(matches) > 1:
            raise ValueError(f"Multiple matches found: {', '.join(item.id for item in matches)}")
        return matches[0]

    def _safe_topological_sort(self, in_memory_copy=None):
        """
        Performs a safe topological sort that detects cycles and provides detailed error information.

        Args:
            items: Dictionary mapping item IDs to their GianttItem objects
            in_memory_copy: Optional dictionary to use for sorting attempt (to avoid modifying original)

        Returns:
            List of sorted GianttItem objects

        Raises:
            CycleDetectedException: If a dependency cycle is detected, with details about the cycle
        """
        # Build adjacency list for strict relations
        adj_list = {item.id: set() for item in self.items.values()}
        for item in self.items.values():
            for rel_type in ['REQUIRES']:
                if rel_type in item.relations:
                    for target in item.relations[rel_type]:
                        if target not in adj_list:
                            continue # Skip non-existent items
                        adj_list[item.id].add(target)

        # Calculate in-degrees
        in_degree = {node: 0 for node in adj_list}
        for node in adj_list:
            for neighbor in adj_list[node]:
                in_degree[neighbor] = in_degree.get(neighbor, 0) + 1

        # Find nodes with no dependencies
        queue = [node for node, degree in in_degree.items() if degree == 0]
        sorted_items = []
        visited = set()

        while queue:
            node = queue.pop(0)
            sorted_items.append(self.items[node])
            visited.add(node)

            for neighbor in adj_list[node]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        # If we haven't visited all nodes, there must be a cycle
        if len(sorted_items) != len(self.items):
            # Find the cycle for better error reporting
            def find_cycle():
                unvisited = set(self.items.keys()) - visited
                stack = []
                path = []

                def dfs(current):
                    if current in stack:
                        cycle_start = stack.index(current)
                        return stack[cycle_start:]
                    if current in visited:
                        return None

                    stack.append(current)
                    for neighbor in adj_list[current]:
                        cycle = dfs(neighbor)
                        if cycle:
                            return cycle
                    stack.pop()
                    return None

                # Start DFS from any unvisited node
                start_node = next(iter(unvisited))
                cycle = dfs(start_node)
                if cycle:
                    # Add one more occurrence of first node to show complete cycle
                    cycle.append(cycle[0])
                return cycle or []

            cycle = find_cycle()
            raise CycleDetectedException(cycle)

        sorted_items.reverse()
        return sorted_items

    def topological_sort(self) -> List[GianttItem]:
        """
        Performs a deterministic topological sort of the graph.
        Returns sorted items in a completely deterministic order.
        """
        # First get basic topological sort
        sorted_items = self._safe_topological_sort()

        # Now within each "level" (items with same dependencies depth),
        # sort by deterministic criteria
        def get_item_sort_key(item):
            return (
                # Primary sort by topological depth
                self._get_dependency_depth(item),
                # Secondary sort by ID (deterministic tie-breaker)
                item.id,
                # Could add more deterministic criteria here
            )

        return sorted(sorted_items, key=get_item_sort_key)

    def _get_dependency_depth(self, item):
        """Get the maximum dependency depth of an item."""
        if 'REQUIRES' not in item.relations:
            return 0

        max_depth = 0
        for dep_id in item.relations['REQUIRES']:
            if dep_id in self.items:
                dep_depth = self._get_dependency_depth(self.items[dep_id])
                max_depth = max(max_depth, dep_depth + 1)
        return max_depth

    def insert_between(self, new_item: GianttItem, before_id: str, after_id: str):
        if before_id not in self.items or after_id not in self.items:
            raise ValueError("Both before and after items must exist")

        before_item = self.items[before_id]
        after_item = self.items[after_id]

        # Update relations
        new_item.relations['REQUIRES'] = [before_id]
        new_item.relations['BLOCKS'] = [after_id]

        # Update existing items
        if 'BLOCKS' in before_item.relations:
            before_item.relations['BLOCKS'].remove(after_id)
            before_item.relations['BLOCKS'].append(new_item.id)

        if 'REQUIRES' in after_item.relations:
            after_item.relations['REQUIRES'].remove(before_id)
            after_item.relations['REQUIRES'].append(new_item.id)

        self.add_item(new_item)

    def included_items(self):
        """Get all items that are not occluded."""
        return {item_id: item for item_id, item in self.items.items() if not item.occlude}

    def occluded_items(self):
        """Get all items that are occluded."""
        return {item_id: item for item_id, item in self.items.items() if item.occlude}

    def copy(self):
        new_graph = GianttGraph()
        for item in self.items.values():
            new_graph.add_item(item.copy())
        return new_graph

    def plus(self, other: 'GianttGraph') -> 'GianttGraph':
        new_graph = self.copy()
        for item in other.items.values():
            new_graph.add_item(item.copy())
        return new_graph

    def __add__(self, other: 'GianttGraph') -> 'GianttGraph':
        return self.plus(other)


@dataclass
class LogEntry:
    """A single log entry recording an event or thought."""
    session: str
    timestamp: datetime
    message: str
    tags: Set[str]
    metadata: Dict[str, str] = field(default_factory=dict)
    occlude: bool = False

    @classmethod
    def create(cls, session_tag: str, message: str, additional_tags: Optional[List[str]] = None, occlude: bool = False) -> 'LogEntry':
        """Create a new log entry with current timestamp."""
        tags = {session_tag}
        if additional_tags:
            tags.update(additional_tags)

        return cls(
            session=session_tag,
            timestamp=datetime.now(timezone.utc),
            message=message,
            tags=tags,
            metadata={},
            occlude=occlude
        )

    def has_tag(self, tag: str) -> bool:
        """Check if entry has a specific tag."""
        return tag in self.tags

    def has_any_tags(self, tags: List[str]) -> bool:
        """Check if entry has any of the specified tags."""
        return bool(self.tags.intersection(tags))

    def has_all_tags(self, tags: List[str]) -> bool:
        """Check if entry has all of the specified tags."""
        return self.tags.issuperset(tags)

    def add_tag(self, tag: str) -> None:
        """Add a tag to the entry."""
        self.tags.add(tag)

    def remove_tag(self, tag: str) -> None:
        """Remove a tag from the entry."""
        self.tags.discard(tag)

    def set_occlude(self, occlude: bool) -> None:
        """Set the occlusion status of the entry."""
        self.occlude = occlude

    def __str__(self):
        return f"{self.timestamp.isoformat()} - {self.message} ({', '.join(self.tags)})"

    def from_dict(data: dict, occlude: bool = False) -> 'LogEntry':
        """Create a LogEntry object from a dictionary."""
        return LogEntry(
            session=data['s'],
            timestamp=datetime.fromisoformat(data['t']),
            message=data['m'],
            tags=set(data['tags']),
            metadata=data.get('meta', {}),
            occlude=occlude
        )

    def from_line(line: str, occlude: bool = False) -> 'LogEntry':
        """Create a LogEntry object from a jsonl line."""
        data = json.loads(line)
        return LogEntry.from_dict(data, occlude)

    def to_dict(self) -> dict:
        """Convert the LogEntry to a dictionary."""
        # occlusion status is not added to the dictionary because it only dictates where the string is saved
        return {
            's': self.session,
            't': self.timestamp.isoformat(),
            'm': self.message,
            'tags': sorted(list(self.tags)),
            'meta': self.metadata
        }

    def to_line(self) -> str:
        """Convert the LogEntry to a jsonl line."""
        # occlusion status is not added to the dictionary because it only dictates where the string is saved
        return json.dumps(self.to_dict(), sort_keys=True)


class LogCollection:
    """A collection of log entries with query capabilities."""

    def __init__(self, entries: Optional[List[LogEntry]] = None):
        self.entries = entries or []

    def add_entry(self, entry: LogEntry) -> None:
        """Add a new entry to the collection."""
        index = self.get_first_index_after_timestamp(entry.timestamp)
        self.entries.insert(index + 1, entry)

    def add_occlude_entry(self, entry: LogEntry) -> None:
        """Add a new entry to the collection ensuring occluded status."""
        entry.occlude = True
        self.add_entry(entry)

    def add_entries(self, entries: List[LogEntry]) -> None:
        self.entries.extend(entries)
        self.sort()

    def create_entry(self, session_tag: str, message: str, additional_tags: Optional[List[str]] = None, occlude: bool = False) -> LogEntry:
        """Create and add a new entry."""
        entry = LogEntry.create(session_tag, message, additional_tags, occlude)
        self.add_entry(entry)
        return entry

    def sort(self) -> None:
        """Sort entries by timestamp."""
        self.entries.sort(key=lambda e: e.timestamp)

    def get_by_session(self, session_tag: str) -> List[LogEntry]:
        """Get all entries with a specific session tag."""
        return [entry for entry in self.entries if entry.session == session_tag]

    def get_by_tags(self, tags: List[str], require_all: bool = False) -> List[LogEntry]:
        """Get entries with specified tags.

        Args:
            tags: List of tags to match
            require_all: If True, entries must have all tags; if False, any tag matches
        """
        if require_all:
            return [entry for entry in self.entries if entry.has_all_tags(tags)]
        return [entry for entry in self.entries if entry.has_any_tags(tags)]

    def get_by_date_range(self, start: datetime, end: Optional[datetime] = None) -> List[LogEntry]:
        """Get entries within a date range."""
        end = end or datetime.now(timezone.utc)
        return [
            entry for entry in self.entries 
            if start <= entry.timestamp <= end
        ]

    def get_by_substring(self, substring: str) -> List[LogEntry]:
        """Get entries with a specific substring in the message."""
        return [entry for entry in self.entries if substring.lower() in entry.message.lower()]

    def get_first_index_after_timestamp(self, timestamp: datetime) -> int:
        """Get the index of the first entry after a timestamp."""
        if not self.entries:
            return 0
        if timestamp >= self.entries[-1].timestamp:
            return len(self.entries) - 1
        if timestamp < self.entries[0].timestamp:
            return 0
        low = 0
        high = len(self.entries) - 1
        while low < high:
            mid = (low + high) // 2
            if self.entries[mid].timestamp < timestamp:
                low = mid + 1
            else:
                high = mid
        return low

    def include_entries(self) -> List[LogEntry]:
        """Get all entries that are not occluded."""
        return [entry for entry in self.entries if not entry.occlude]

    def occluded_entries(self) -> List[LogEntry]:
        """Get all entries that are occluded."""
        return [entry for entry in self.entries if entry.occlude]

    def __iter__(self):
        return iter(self.entries)


class IssueType(Enum):
    DANGLING_REFERENCE = "dangling_reference"
    ORPHANED_ITEM = "orphaned_item"
    INCOMPLETE_CHAIN = "incomplete_chain"
    CHART_INCONSISTENCY = "chart_inconsistency"
    TAG_INCONSISTENCY = "tag_inconsistency"
    
    @classmethod
    def from_string(cls, value: str) -> 'IssueType':
        """Convert a string to an IssueType."""
        for issue_type in cls:
            if issue_type.value == value:
                return issue_type
        raise ValueError(f"Invalid issue type: {value}")

@dataclass
class Issue:
    type: IssueType
    item_id: str
    message: str
    related_ids: List[str]
    suggested_fix: Optional[str] = None

class GianttDoctor:
    def __init__(self, graph: 'GianttGraph'):
        self.graph = graph
        self.issues: List[Issue] = []
        self.fixed_issues: List[Issue] = []

    def quick_check(self) -> int:
        """Run a quick check and return number of issues found."""
        self.issues = []
        self._check_references()
        return len(self.issues)

    def full_diagnosis(self) -> List[Issue]:
        """Run all checks and return detailed issues."""
        self.issues = []
        self._check_references()
        # Not clear that any commented below are actually issues
        # self._check_orphans()
        self._check_chains()
        # self._check_charts()
        # self._check_tags()
        return self.issues
        
    def get_issues_by_type(self, issue_type: IssueType) -> List[Issue]:
        """Get all issues of a specific type."""
        return [issue for issue in self.issues if issue.type == issue_type]
    
    def fix_issues(self, issue_type: Optional[IssueType] = None, item_id: Optional[str] = None) -> List[Issue]:
        """Fix issues of a specific type or for a specific item."""
        # Filter issues to fix
        issues_to_fix = self.issues
        if issue_type:
            issues_to_fix = [issue for issue in issues_to_fix if issue.type == issue_type]
        if item_id:
            issues_to_fix = [issue for issue in issues_to_fix if issue.item_id == item_id]
            
        fixed = []
        for issue in issues_to_fix:
            if self._fix_issue(issue):
                fixed.append(issue)
                
        # Remove fixed issues from the issues list
        for issue in fixed:
            if issue in self.issues:
                self.issues.remove(issue)
                
        self.fixed_issues.extend(fixed)
        return fixed
    
    def _fix_issue(self, issue: Issue) -> bool:
        """Fix a specific issue. Returns True if fixed, False otherwise."""
        if issue.type == IssueType.DANGLING_REFERENCE:
            return self._fix_dangling_reference(issue)
        elif issue.type == IssueType.INCOMPLETE_CHAIN:
            return self._fix_incomplete_chain(issue)
        # Add more issue type handlers as needed
        return False
        
    def _fix_dangling_reference(self, issue: Issue) -> bool:
        """Fix a dangling reference issue."""
        item = self.graph.items.get(issue.item_id)
        if not item:
            return False
            
        # Find the relation type and target from the message
        rel_type = None
        target = None
        for rel_name in RelationType._member_names_:
            if rel_name.lower() in issue.message.lower():
                rel_type = rel_name
                break
                
        if not rel_type:
            return False
            
        # Extract the target ID from the message
        import re
        match = re.search(r"non-existent item '([^']+)'", issue.message)
        if not match:
            return False
            
        target = match.group(1)
        
        # Remove the dangling reference
        if rel_type in item.relations and target in item.relations[rel_type]:
            item.relations[rel_type].remove(target)
            if not item.relations[rel_type]:
                del item.relations[rel_type]
            return True
            
        return False
        
    def _fix_incomplete_chain(self, issue: Issue) -> bool:
        """Fix an incomplete chain issue."""
        if not issue.related_ids or not issue.suggested_fix:
            return False
            
        item = self.graph.items.get(issue.item_id)
        related_item = self.graph.items.get(issue.related_ids[0])
        if not item or not related_item:
            return False
            
        # Parse the suggested fix to determine what to do
        parts = issue.suggested_fix.split()
        if len(parts) < 4:
            return False
            
        target_id = parts[2]
        action = parts[3]
        rel_type = parts[4].upper() if len(parts) > 4 else None
        
        if target_id != issue.item_id and target_id != issue.related_ids[0]:
            return False
            
        if "add" in action.lower() and rel_type:
            target_item = self.graph.items.get(target_id)
            if not target_item:
                return False
                
            # Add the relation
            target_item.relations.setdefault(rel_type, [])
            if parts[5] not in target_item.relations[rel_type]:
                target_item.relations[rel_type].append(parts[5])
            return True
            
        return False

    def _check_references(self):
        """Check for dangling references in relations."""
        for item_id, item in self.graph.items.items():
            for rel_type, targets in item.relations.items():
                for target in targets:
                    if target not in self.graph.items:
                        self.issues.append(Issue(
                            type=IssueType.DANGLING_REFERENCE,
                            item_id=item_id,
                            message=f"References non-existent item '{target}' in {rel_type.lower()} relation",
                            related_ids=[target],
                            suggested_fix=f"giantt modify {item_id} --remove {rel_type.lower()} {target}"
                        ))

    def _check_orphans(self):
        """Find items with no incoming or outgoing relations."""
        for item_id, item in self.graph.items.items():
            has_incoming = any(
                target == item_id
                for other in self.graph.items.values()
                for targets in other.relations.values()
                for target in targets
            )
            has_outgoing = bool(item.relations)

            if not has_incoming and not has_outgoing:
                self.issues.append(Issue(
                    type=IssueType.ORPHANED_ITEM,
                    item_id=item_id,
                    message="Item has no relations to other items",
                    related_ids=[],
                    suggested_fix="Consider connecting this item to related tasks"
                ))

    def _check_chains(self):
        """Check for incomplete dependency chains."""
        blocks_map = {
            item_id: set(targets)
            for item_id, item in self.graph.items.items()
            for targets in [item.relations.get('BLOCKS', [])]
        }
        requires_map = {
            item_id: set(targets)
            for item_id, item in self.graph.items.items()
            for targets in [item.relations.get('REQUIRES', [])]
        }
        sufficient_map = {
            item_id: set(targets)
            for item_id, item in self.graph.items.items()
            for targets in [item.relations.get('SUFFICIENT', [])]
        }
        anyof_map = {
            item_id: set(targets)
            for item_id, item in self.graph.items.items()
            for targets in [item.relations.get('ANY', [])]
        }

        # Check for items that block something but aren't required by it or vice versa
        for item_id, blocks_items in blocks_map.items():
            for blocked in blocks_items:
                if blocked in self.graph.items:
                    if item_id not in requires_map.get(blocked, set()):
                        self.issues.append(Issue(
                            type=IssueType.INCOMPLETE_CHAIN,
                            item_id=item_id,
                            message=f"Item blocks '{blocked}' but isn't required by it",
                            related_ids=[blocked],
                            suggested_fix=f"giantt modify {blocked} --add requires {item_id}"
                        ))
        for item_id, requires_items in requires_map.items():
            for required in requires_items:
                if required in self.graph.items:
                    if item_id not in blocks_map.get(required, set()):
                        self.issues.append(Issue(
                            type=IssueType.INCOMPLETE_CHAIN,
                            item_id=item_id,
                            message=f"Item requires '{required}' but isn't blocked by it",
                            related_ids=[required],
                            suggested_fix=f"giantt modify {required} --add blocks {item_id}"
                        ))
        # Check for items that are sufficient for something but aren't in an any relation with it, or vice versa
        for item_id, sufficient_items in sufficient_map.items():
            for sufficient in sufficient_items:
                if sufficient in self.graph.items:
                    if item_id not in anyof_map.get(sufficient, set()):
                        self.issues.append(Issue(
                            type=IssueType.INCOMPLETE_CHAIN,
                            item_id=item_id,
                            message=f"Item is sufficient for '{sufficient}' but doesn't have any-of relation with it",
                            related_ids=[sufficient],
                            suggested_fix=f"giantt modify {sufficient} --add any {item_id}"
                        ))
        for item_id, anyof_items in anyof_map.items():
            for anyof_item in anyof_items:
                if anyof_item in self.graph.items:
                    if item_id not in sufficient_map.get(anyof_item, set()):
                        self.issues.append(Issue(
                            type=IssueType.INCOMPLETE_CHAIN,
                            item_id=item_id,
                            message=f"Item has any-of relation with '{anyof_item}' but isn't sufficient for it",
                            related_ids=[anyof_item],
                            suggested_fix=f"giantt modify {anyof_item} --add sufficient {item_id}"
                        ))

    def _check_charts(self):
        """Check for chart consistency issues."""
        # Find all unique charts
        all_charts = set()
        chart_items: Dict[str, Set[str]] = {}

        for item_id, item in self.graph.items.items():
            for chart in item.charts:
                all_charts.add(chart)
                if chart not in chart_items:
                    chart_items[chart] = set()
                chart_items[chart].add(item_id)

        # Check for items that should probably be in certain charts
        for chart in all_charts:
            chart_set = chart_items[chart]
            for item_id in chart_set:
                item = self.graph.items[item_id]
                # Check if any required items or blocked items in this chart
                # aren't also in this chart
                related_items = set(item.relations.get('REQUIRES', []) + 
                                 item.relations.get('BLOCKS', []))
                for related_id in related_items:
                    if (related_id in self.graph.items and 
                        related_id not in chart_set and
                        any(c == chart for c in self.graph.items[related_id].charts)):
                        self.issues.append(Issue(
                            type=IssueType.CHART_INCONSISTENCY,
                            item_id=related_id,
                            message=f"Item is related to items in chart '{chart}' but isn't in it",
                            related_ids=[item_id],
                            suggested_fix=""
                        ))

    def _check_tags(self):
        """Check for tag consistency issues."""
        # Find all unique tags
        all_tags = set()
        tag_items: Dict[str, Set[str]] = {}

        for item_id, item in self.graph.items.items():
            for tag in item.tags:
                all_tags.add(tag)
                if tag not in tag_items:
                    tag_items[tag] = set()
                tag_items[tag].add(item_id)

        # Check for items that should probably have certain tags
        for tag in all_tags:
            tag_set = tag_items[tag]
            for item_id in tag_set:
                item = self.graph.items[item_id]
                # Check if any required items with this tag aren't also tagged
                related_items = set(item.relations.get('REQUIRES', []) + 
                                 item.relations.get('BLOCKS', []))
                for related_id in related_items:
                    if (related_id in self.graph.items and 
                        related_id not in tag_set and
                        any(t == tag for t in self.graph.items[related_id].tags)):
                        self.issues.append(Issue(
                            type=IssueType.TAG_INCONSISTENCY,
                            item_id=related_id,
                            message=f"Item is related to items with tag '{tag}' but doesn't have it",
                            related_ids=[item_id],
                            suggested_fix=""
                        ))


// =====================================================================
// FILE: android/app/src/main/java/io/flutter/plugins/GeneratedPluginRegistrant.java
// =====================================================================

package io.flutter.plugins;

import androidx.annotation.Keep;
import androidx.annotation.NonNull;
import io.flutter.Log;

import io.flutter.embedding.engine.FlutterEngine;

/**
 * Generated file. Do not edit.
 * This file is generated by the Flutter tool based on the
 * plugins that support the Android platform.
 */
@Keep
public final class GeneratedPluginRegistrant {
  private static final String TAG = "GeneratedPluginRegistrant";
  public static void registerWith(@NonNull FlutterEngine flutterEngine) {
  }
}


// =====================================================================
// FILE: packages/ai_chat_flutter/lib/ai_chat_flutter.dart
// =====================================================================

library ai_chat_flutter;

export 'src/chat_widget.dart';
export 'src/action_registry.dart';
export 'src/models/chat_message.dart';
export 'src/models/action_call.dart';



// =====================================================================
// FILE: packages/ai_chat_flutter/lib/src/chat_widget.dart
// =====================================================================

import 'package:flutter/material.dart';

class AiChatWidget extends StatefulWidget {
  final ActionRegistry actionRegistry;
  final String? systemPrompt;
  final Function(String)? onUserMessage;

  const AiChatWidget({
    super.key,
    required this.actionRegistry,
    this.systemPrompt,
    this.onUserMessage,
  });

  @override
  State<AiChatWidget> createState() => _AiChatWidgetState();
}

class _AiChatWidgetState extends State<AiChatWidget> {
  final List<ChatMessage> _messages = [];
  final TextEditingController _controller = TextEditingController();

  @override
  Widget build(BuildContext context) {
    return Column(
      children: [
        Expanded(
          child: ListView.builder(
            itemCount: _messages.length,
            itemBuilder: (context, index) => _MessageBubble(
              message: _messages[index],
              onActionTap: _executeAction,
            ),
          ),
        ),
        _ChatInput(
          controller: _controller,
          onSend: _sendMessage,
        ),
      ],
    );
  }

  void _sendMessage(String text) async {
    if (text.trim().isEmpty) return;
    
    final userMessage = ChatMessage.user(text);
    setState(() => _messages.add(userMessage));
    
    widget.onUserMessage?.call(text);
    _controller.clear();

    // Send to AI and handle response with action calls
    await _processAiResponse(text);
  }

  Future<void> _processAiResponse(String userInput) async {
    // Implementation for AI API call and action parsing
    // This would parse responses for action calls like:
    // ```action:create_task
    // {"title": "New task", "due_date": "2024-01-01"}
    // ```
  }

  Future<void> _executeAction(ActionCall actionCall) async {
    try {
      final result = await widget.actionRegistry.executeAction(
        actionCall.actionName,
        actionCall.parameters,
      );
      
      setState(() {
        _messages.add(ChatMessage.system(
          'Action "${actionCall.actionName}" completed: $result'
        ));
      });
    } catch (e) {
      setState(() {
        _messages.add(ChatMessage.system(
          'Action "${actionCall.actionName}" failed: $e'
        ));
      });
    }
  }
}


// =====================================================================
// FILE: packages/ai_chat_flutter/lib/src/action_registry.dart
// =====================================================================

import 'dart:async';

typedef ActionHandler<T> = Future<T> Function(Map<String, dynamic> parameters);

class ActionRegistry {
  final Map<String, ActionHandler> _handlers = {};
  
  void registerAction<T>(String actionName, ActionHandler<T> handler) {
    _handlers[actionName] = handler;
  }
  
  Future<dynamic> executeAction(String actionName, Map<String, dynamic> parameters) async {
    final handler = _handlers[actionName];
    if (handler == null) {
      throw Exception('Unknown action: $actionName');
    }
    return await handler(parameters);
  }
  
  List<String> get availableActions => _handlers.keys.toList();
}


// =====================================================================
// FILE: packages/soradyne_core/examples/visual_block_test.rs
// =====================================================================

//! Visual Block Storage Test
//! 
//! This example downloads real images, stores them using the block storage system,
//! and then retrieves them for visual verification. It creates before/after files
//! that you can open and compare to verify the storage system works correctly.

use std::path::PathBuf;
use std::sync::Arc;
use tokio;
use sha2::{Sha256, Digest};

use soradyne::storage::block_manager::BlockManager;
use soradyne::types::media::PhotoStorage;

/// High-quality test images from reliable sources
const TEST_IMAGES: &[(&str, &str, &str)] = &[
    // Small test images
    ("https://httpbin.org/image/jpeg", "httpbin_test.jpg", "A simple JPEG test image"),
    ("https://httpbin.org/image/png", "httpbin_test.png", "A simple PNG test image"),
    
    // Random images from Picsum (Lorem Ipsum for photos) - more reliable
    ("https://picsum.photos/300/200", "picsum_300x200.jpg", "Random photo 300x200"),
    ("https://picsum.photos/400/300", "picsum_400x300.jpg", "Random photo 400x300"),
    ("https://picsum.photos/500/300", "picsum_500x300.jpg", "Random photo 500x300"),
    ("https://picsum.photos/600/400", "picsum_600x400.jpg", "Random photo 600x400"),
    ("https://picsum.photos/800/600", "picsum_800x600.jpg", "Random photo 800x600"),
    ("https://picsum.photos/1024/768", "picsum_1024x768.jpg", "Random photo 1024x768"),
];

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("🖼️  Visual Block Storage Test");
    println!("=============================");
    println!("This test downloads real images, stores them using block storage,");
    println!("and creates before/after files for visual verification.\n");
    
    let mut test_session = VisualTestSession::new().await?;
    
    // Run the comprehensive test
    test_session.run_visual_test().await?;
    
    // Show results and instructions
    test_session.show_results();
    
    Ok(())
}

struct VisualTestSession {
    block_manager: Arc<BlockManager>,
    photo_storage: PhotoStorage,
    test_dir: PathBuf,
    results_dir: PathBuf,
    client: reqwest::Client,
    test_results: Vec<TestResult>,
}

#[derive(Debug)]
struct TestResult {
    name: String,
    description: String,
    original_size: usize,
    retrieved_size: usize,
    hash_match: bool,
    storage_time: std::time::Duration,
    retrieval_time: std::time::Duration,
    original_path: PathBuf,
    retrieved_path: PathBuf,
    success: bool,
}

impl VisualTestSession {
    async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        // Create test directory structure
        let test_dir = std::env::current_dir()?.join("visual_block_test");
        let results_dir = test_dir.join("results");
        
        // Clean and recreate directories
        if test_dir.exists() {
            std::fs::remove_dir_all(&test_dir)?;
        }
        std::fs::create_dir_all(&test_dir)?;
        std::fs::create_dir_all(&results_dir)?;
        
        // Create rimsd directories (hidden .rimsd subdirectories)
        let mut rimsd_dirs = Vec::new();
        for i in 0..4 {
            let device_dir = test_dir.join(format!("rimsd_{}", i));
            let rimsd_dir = device_dir.join(".rimsd");
            std::fs::create_dir_all(&rimsd_dir)?;
            rimsd_dirs.push(rimsd_dir);
        }
        
        // Set up block manager with good redundancy
        let metadata_path = test_dir.join("metadata.json");
        let threshold = 3; // Need 3 out of 4 shards
        let total_shards = 4;
        
        let block_manager = Arc::new(BlockManager::new(
            rimsd_dirs,
            metadata_path,
            threshold,
            total_shards,
        )?);
        
        let photo_storage = PhotoStorage::new(block_manager.clone());
        
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .user_agent("Mozilla/5.0 (compatible; SoradyneTest/1.0)")
            .build()?;
        
        println!("✅ Test environment created at: {:?}", test_dir);
        println!("📁 Results will be saved to: {:?}\n", results_dir);
        
        Ok(Self {
            block_manager,
            photo_storage,
            test_dir,
            results_dir,
            client,
            test_results: Vec::new(),
        })
    }
    
    async fn run_visual_test(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("🚀 Starting visual test with {} images...\n", TEST_IMAGES.len());
        
        for (i, (url, name, description)) in TEST_IMAGES.iter().enumerate() {
            println!("📸 Test {}/{}: {}", i + 1, TEST_IMAGES.len(), name);
            println!("   Description: {}", description);
            println!("   URL: {}", url);
            
            match self.test_single_image(url, name, description).await {
                Ok(result) => {
                    if result.success {
                        println!("   ✅ SUCCESS - Hash match: {}, Size: {} bytes", 
                                result.hash_match, result.original_size);
                    } else {
                        println!("   ❌ FAILED - Check logs for details");
                    }
                    self.test_results.push(result);
                }
                Err(e) => {
                    println!("   ❌ ERROR: {}", e);
                    // Create a failed result entry
                    self.test_results.push(TestResult {
                        name: name.to_string(),
                        description: description.to_string(),
                        original_size: 0,
                        retrieved_size: 0,
                        hash_match: false,
                        storage_time: std::time::Duration::from_secs(0),
                        retrieval_time: std::time::Duration::from_secs(0),
                        original_path: PathBuf::new(),
                        retrieved_path: PathBuf::new(),
                        success: false,
                    });
                }
            }
            println!();
        }
        
        Ok(())
    }
    
    async fn test_single_image(&self, url: &str, name: &str, description: &str) -> Result<TestResult, Box<dyn std::error::Error>> {
        // Download the image
        println!("   📥 Downloading...");
        let response = self.client.get(url).send().await?;
        let image_data = response.bytes().await?.to_vec();
        
        if image_data.is_empty() {
            return Err("Downloaded empty file".into());
        }
        
        println!("   📊 Downloaded {} bytes", image_data.len());
        
        // Calculate hash for integrity verification
        let original_hash = calculate_hash(&image_data);
        
        // Determine MIME type
        let mime_type = if name.ends_with(".jpg") || name.ends_with(".jpeg") {
            "image/jpeg"
        } else if name.ends_with(".png") {
            "image/png"
        } else {
            "image/png" // Default
        };
        
        // Save original for comparison
        let original_path = self.results_dir.join(format!("original_{}", name));
        std::fs::write(&original_path, &image_data)?;
        
        // Store using block storage
        println!("   💾 Storing in block storage...");
        let storage_start = std::time::Instant::now();
        let metadata = self.photo_storage.save_photo(name, mime_type, &image_data).await?;
        let storage_time = storage_start.elapsed();
        
        println!("   🔑 Block ID: {}", metadata.id);
        
        // Retrieve from block storage
        println!("   📤 Retrieving from block storage...");
        let retrieval_start = std::time::Instant::now();
        let retrieved_data = self.photo_storage.load_photo(&metadata).await?;
        let retrieval_time = retrieval_start.elapsed();
        
        // Save retrieved for comparison
        let retrieved_path = self.results_dir.join(format!("retrieved_{}", name));
        std::fs::write(&retrieved_path, &retrieved_data)?;
        
        // Verify integrity
        let retrieved_hash = calculate_hash(&retrieved_data);
        let hash_match = original_hash == retrieved_hash;
        let size_match = image_data.len() == retrieved_data.len();
        let data_match = image_data == retrieved_data;
        
        let success = hash_match && size_match && data_match;
        
        println!("   ⏱️  Storage: {:.2?}, Retrieval: {:.2?}", storage_time, retrieval_time);
        println!("   🔍 Integrity: Hash={}, Size={}, Data={}", 
                hash_match, size_match, data_match);
        
        Ok(TestResult {
            name: name.to_string(),
            description: description.to_string(),
            original_size: image_data.len(),
            retrieved_size: retrieved_data.len(),
            hash_match,
            storage_time,
            retrieval_time,
            original_path,
            retrieved_path,
            success,
        })
    }
    
    fn show_results(&self) {
        println!("📊 TEST RESULTS SUMMARY");
        println!("=======================");
        
        let successful = self.test_results.iter().filter(|r| r.success).count();
        let total = self.test_results.len();
        
        println!("✅ Successful: {}/{} ({:.1}%)", 
                successful, total, 
                successful as f64 / total as f64 * 100.0);
        
        if successful < total {
            println!("❌ Failed: {}", total - successful);
        }
        
        println!("\n📋 Detailed Results:");
        for (i, result) in self.test_results.iter().enumerate() {
            let status = if result.success { "✅" } else { "❌" };
            println!("{}. {} {} - {} bytes", 
                    i + 1, status, result.name, result.original_size);
            
            if result.success {
                println!("     Storage: {:.2?}, Retrieval: {:.2?}", 
                        result.storage_time, result.retrieval_time);
            }
        }
        
        println!("\n🔍 VISUAL VERIFICATION INSTRUCTIONS:");
        println!("=====================================");
        println!("1. Open the results directory: {:?}", self.results_dir);
        println!("2. For each image, compare:");
        println!("   - original_[name] (downloaded image)");
        println!("   - retrieved_[name] (reconstructed from block storage)");
        println!("3. They should be visually identical!");
        
        println!("\n💡 Quick verification commands:");
        println!("   # Open results directory:");
        println!("   open {:?}", self.results_dir);
        println!("   # Or on Linux:");
        println!("   xdg-open {:?}", self.results_dir);
        
        // Create an HTML comparison page
        if let Err(e) = self.create_comparison_html() {
            println!("⚠️  Could not create HTML comparison: {}", e);
        } else {
            let html_path = self.results_dir.join("comparison.html");
            println!("\n🌐 HTML Comparison created:");
            println!("   open {:?}", html_path);
        }
        
        // Performance summary
        if !self.test_results.is_empty() {
            let avg_storage_time: std::time::Duration = self.test_results.iter()
                .map(|r| r.storage_time)
                .sum::<std::time::Duration>() / self.test_results.len() as u32;
            
            let avg_retrieval_time: std::time::Duration = self.test_results.iter()
                .map(|r| r.retrieval_time)
                .sum::<std::time::Duration>() / self.test_results.len() as u32;
            
            let total_bytes: usize = self.test_results.iter()
                .map(|r| r.original_size)
                .sum();
            
            println!("\n⚡ Performance Summary:");
            println!("   Average storage time: {:.2?}", avg_storage_time);
            println!("   Average retrieval time: {:.2?}", avg_retrieval_time);
            println!("   Total data processed: {} bytes ({:.2} MB)", 
                    total_bytes, total_bytes as f64 / 1_000_000.0);
        }
    }
    
    fn create_comparison_html(&self) -> Result<(), Box<dyn std::error::Error>> {
        let html_path = self.results_dir.join("comparison.html");
        let mut html = String::new();
        
        html.push_str("<!DOCTYPE html>\n<html>\n<head>\n");
        html.push_str("<title>Soradyne Block Storage Visual Test Results</title>\n");
        html.push_str("<style>\n");
        html.push_str("body { font-family: Arial, sans-serif; margin: 20px; }\n");
        html.push_str(".test-result { border: 1px solid #ccc; margin: 20px 0; padding: 15px; }\n");
        html.push_str(".success { border-color: #4CAF50; background-color: #f9fff9; }\n");
        html.push_str(".failure { border-color: #f44336; background-color: #fff9f9; }\n");
        html.push_str(".image-comparison { display: flex; gap: 20px; margin: 10px 0; }\n");
        html.push_str(".image-container { text-align: center; }\n");
        html.push_str("img { max-width: 300px; max-height: 300px; border: 1px solid #ddd; }\n");
        html.push_str(".stats { background-color: #f5f5f5; padding: 10px; margin: 10px 0; }\n");
        html.push_str("</style>\n</head>\n<body>\n");
        
        html.push_str("<h1>🖼️ Soradyne Block Storage Visual Test Results</h1>\n");
        html.push_str(&format!("<p>Test completed with {}/{} successful results</p>\n", 
                              self.test_results.iter().filter(|r| r.success).count(),
                              self.test_results.len()));
        
        for result in &self.test_results {
            let class = if result.success { "success" } else { "failure" };
            let status = if result.success { "✅ SUCCESS" } else { "❌ FAILED" };
            
            html.push_str(&format!("<div class=\"test-result {}\">\n", class));
            html.push_str(&format!("<h2>{} {}</h2>\n", status, result.name));
            html.push_str(&format!("<p><strong>Description:</strong> {}</p>\n", result.description));
            
            if result.success {
                html.push_str("<div class=\"image-comparison\">\n");
                html.push_str("<div class=\"image-container\">\n");
                html.push_str(&format!("<h3>Original</h3>\n"));
                html.push_str(&format!("<img src=\"{}\" alt=\"Original {}\">\n", 
                                      result.original_path.file_name().unwrap().to_string_lossy(),
                                      result.name));
                html.push_str(&format!("<p>{} bytes</p>\n", result.original_size));
                html.push_str("</div>\n");
                
                html.push_str("<div class=\"image-container\">\n");
                html.push_str(&format!("<h3>Retrieved</h3>\n"));
                html.push_str(&format!("<img src=\"{}\" alt=\"Retrieved {}\">\n", 
                                      result.retrieved_path.file_name().unwrap().to_string_lossy(),
                                      result.name));
                html.push_str(&format!("<p>{} bytes</p>\n", result.retrieved_size));
                html.push_str("</div>\n");
                html.push_str("</div>\n");
                
                html.push_str("<div class=\"stats\">\n");
                html.push_str(&format!("<p><strong>Hash Match:</strong> {}</p>\n", result.hash_match));
                html.push_str(&format!("<p><strong>Storage Time:</strong> {:.2?}</p>\n", result.storage_time));
                html.push_str(&format!("<p><strong>Retrieval Time:</strong> {:.2?}</p>\n", result.retrieval_time));
                html.push_str("</div>\n");
            }
            
            html.push_str("</div>\n");
        }
        
        html.push_str("</body>\n</html>\n");
        
        std::fs::write(html_path, html)?;
        Ok(())
    }
}

fn calculate_hash(data: &[u8]) -> [u8; 32] {
    let mut hasher = Sha256::new();
    hasher.update(data);
    let result = hasher.finalize();
    let mut hash = [0u8; 32];
    hash.copy_from_slice(&result);
    hash
}


// =====================================================================
// FILE: packages/soradyne_core/examples/init_sd_card.rs
// =====================================================================

use std::path::PathBuf;
use std::io::{self, Write};
use tokio;
use soradyne::storage::device_identity::{fingerprint_device, discover_soradyne_volumes};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("🔧 Soradyne SD Card Initialization Tool");
    println!("========================================");
    println!("This tool will initialize SD cards for Soradyne storage.\n");
    
    loop {
        println!("Options:");
        println!("1. Initialize a specific SD card path");
        println!("2. Auto-discover and initialize SD cards");
        println!("3. List existing Soradyne volumes");
        println!("4. Exit");
        print!("Choose an option (1-4): ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let choice = input.trim();
        
        match choice {
            "1" => {
                initialize_specific_path().await?;
            }
            
            "2" => {
                auto_discover_and_initialize().await?;
            }
            
            "3" => {
                list_existing_volumes().await?;
            }
            
            "4" => {
                println!("👋 Goodbye!");
                break;
            }
            
            _ => {
                println!("❌ Invalid option. Please choose 1-4.");
            }
        }
        
        println!();
    }
    
    Ok(())
}

async fn initialize_specific_path() -> Result<(), Box<dyn std::error::Error>> {
    println!("\n📱 Initialize Specific SD Card");
    println!("==============================");
    println!("Enter the mount path of your SD card:");
    println!("Examples:");
    println!("  macOS:   /Volumes/SDCARD");
    println!("  Linux:   /media/username/SDCARD or /mnt/sdcard");
    println!("  Windows: D:\\ or E:\\");
    print!("\nSD card path: ");
    io::stdout().flush()?;
    
    let mut path_input = String::new();
    io::stdin().read_line(&mut path_input)?;
    let sd_path = std::path::Path::new(path_input.trim());
    
    if !sd_path.exists() {
        println!("❌ Path does not exist: {}", sd_path.display());
        return Ok(());
    }
    
    let rimsd_path = sd_path.join(".rimsd");
    
    // Check if already initialized
    if rimsd_path.exists() {
        println!("⚠️  .rimsd directory already exists at: {}", rimsd_path.display());
        print!("Do you want to reinitialize? (y/N): ");
        io::stdout().flush()?;
        
        let mut confirm = String::new();
        io::stdin().read_line(&mut confirm)?;
        if !confirm.trim().to_lowercase().starts_with('y') {
            println!("❌ Initialization cancelled.");
            return Ok(());
        }
    }
    
    println!("\n🔍 Initializing Soradyne storage at: {}", rimsd_path.display());
    
    // Create .rimsd directory
    tokio::fs::create_dir_all(&rimsd_path).await?;
    
    // Generate device fingerprint
    match fingerprint_device(&rimsd_path).await {
        Ok(fingerprint) => {
            println!("✅ Successfully initialized Soradyne storage!");
            print_fingerprint_info(&fingerprint, &rimsd_path).await?;
        }
        Err(e) => {
            println!("❌ Failed to generate device fingerprint: {}", e);
            println!("   The .rimsd directory was created but fingerprinting failed.");
        }
    }
    
    Ok(())
}

async fn auto_discover_and_initialize() -> Result<(), Box<dyn std::error::Error>> {
    println!("\n🔍 Auto-Discover SD Cards");
    println!("=========================");
    println!("Scanning for mounted volumes that could be SD cards...");
    
    // Get potential mount points
    let mount_points = get_potential_sd_cards().await?;
    
    if mount_points.is_empty() {
        println!("❌ No potential SD card mount points found.");
        println!("   Please ensure your SD cards are mounted and try option 1 for manual initialization.");
        return Ok(());
    }
    
    println!("Found {} potential SD card mount points:", mount_points.len());
    for (i, path) in mount_points.iter().enumerate() {
        let rimsd_path = path.join(".rimsd");
        let status = if rimsd_path.exists() { "✅ Already initialized" } else { "❌ Not initialized" };
        println!("   {}. {} - {}", i + 1, path.display(), status);
    }
    
    print!("\nEnter the number of the SD card to initialize (or 0 to cancel): ");
    io::stdout().flush()?;
    
    let mut input = String::new();
    io::stdin().read_line(&mut input)?;
    let choice: usize = input.trim().parse().unwrap_or(0);
    
    if choice == 0 || choice > mount_points.len() {
        println!("❌ Cancelled or invalid choice.");
        return Ok(());
    }
    
    let selected_path = &mount_points[choice - 1];
    let rimsd_path = selected_path.join(".rimsd");
    
    println!("\n🔍 Initializing: {}", selected_path.display());
    
    // Create .rimsd directory
    tokio::fs::create_dir_all(&rimsd_path).await?;
    
    // Generate device fingerprint
    match fingerprint_device(&rimsd_path).await {
        Ok(fingerprint) => {
            println!("✅ Successfully initialized Soradyne storage!");
            print_fingerprint_info(&fingerprint, &rimsd_path).await?;
        }
        Err(e) => {
            println!("❌ Failed to generate device fingerprint: {}", e);
        }
    }
    
    Ok(())
}

async fn list_existing_volumes() -> Result<(), Box<dyn std::error::Error>> {
    println!("\n📋 Existing Soradyne Volumes");
    println!("============================");
    
    match discover_soradyne_volumes().await {
        Ok(volumes) => {
            if volumes.is_empty() {
                println!("❌ No Soradyne volumes found.");
                println!("   Use options 1 or 2 to initialize SD cards.");
            } else {
                println!("Found {} Soradyne volumes:", volumes.len());
                for (i, rimsd_path) in volumes.iter().enumerate() {
                    println!("\n{}. {}", i + 1, rimsd_path.display());
                    
                    // Try to read the device fingerprint
                    match fingerprint_device(rimsd_path).await {
                        Ok(fingerprint) => {
                            println!("   Device ID: {:?}", fingerprint.soradyne_device_id);
                            println!("   Capacity: {:.2} GB", fingerprint.capacity_bytes as f64 / (1024.0 * 1024.0 * 1024.0));
                            if let Some(hw_id) = &fingerprint.hardware_id {
                                println!("   Hardware: {}", hw_id);
                            }
                        }
                        Err(e) => {
                            println!("   ❌ Error reading fingerprint: {}", e);
                        }
                    }
                }
            }
        }
        Err(e) => {
            println!("❌ Failed to discover volumes: {}", e);
        }
    }
    
    Ok(())
}

async fn print_fingerprint_info(fingerprint: &soradyne::storage::device_identity::BasicFingerprint, rimsd_path: &std::path::Path) -> Result<(), Box<dyn std::error::Error>> {
    println!("\n📊 Device Fingerprint:");
    println!("   Soradyne Device ID: {:?}", fingerprint.soradyne_device_id);
    println!("   Hardware ID: {:?}", fingerprint.hardware_id);
    println!("   Filesystem UUID: {:?}", fingerprint.filesystem_uuid);
    println!("   Capacity: {:.2} GB", fingerprint.capacity_bytes as f64 / (1024.0 * 1024.0 * 1024.0));
    println!("   Bad block signature: 0x{:016x}", fingerprint.bad_block_signature);
    
    println!("\n💾 Files created:");
    let device_id_file = rimsd_path.join("soradyne_device_id.txt");
    if device_id_file.exists() {
        if let Ok(content) = tokio::fs::read_to_string(&device_id_file).await {
            println!("   {}/soradyne_device_id.txt", rimsd_path.display());
            println!("   Device ID: {}", content.trim());
        }
    }
    
    println!("\n🎉 SD card is now ready for Soradyne storage!");
    
    Ok(())
}

async fn get_potential_sd_cards() -> Result<Vec<PathBuf>, Box<dyn std::error::Error>> {
    let mut mount_points = Vec::new();
    
    #[cfg(target_os = "macos")]
    {
        // Check /Volumes for mounted drives
        if let Ok(entries) = std::fs::read_dir("/Volumes") {
            for entry in entries.flatten() {
                let path = entry.path();
                // Skip system volumes
                let name = path.file_name().unwrap_or_default().to_string_lossy().to_lowercase();
                if !name.contains("macintosh") && !name.contains("system") && !name.contains("data") {
                    mount_points.push(path);
                }
            }
        }
    }
    
    #[cfg(target_os = "linux")]
    {
        // Check common mount points
        for base_dir in &["/media", "/mnt", "/run/media"] {
            if let Ok(entries) = std::fs::read_dir(base_dir) {
                for entry in entries.flatten() {
                    if base_dir == "/run/media" {
                        // /run/media has user subdirectories
                        if let Ok(user_entries) = std::fs::read_dir(entry.path()) {
                            for user_entry in user_entries.flatten() {
                                mount_points.push(user_entry.path());
                            }
                        }
                    } else {
                        mount_points.push(entry.path());
                    }
                }
            }
        }
    }
    
    #[cfg(target_os = "windows")]
    {
        // Check removable drives
        for letter in 'D'..='Z' {
            let drive_path = PathBuf::from(format!("{}:\\", letter));
            if drive_path.exists() {
                // Try to determine if it's removable (simplified check)
                mount_points.push(drive_path);
            }
        }
    }
    
    Ok(mount_points)
}


// =====================================================================
// FILE: packages/soradyne_core/examples/album_sync_test.rs
// =====================================================================

//! Test album synchronization locally
//! 
//! This example demonstrates creating albums, adding media, and syncing
//! between two local "replicas" to test the CRDT functionality.

use tokio;
use soradyne::album::*;
use tempfile::TempDir;
use std::path::PathBuf;
use std::sync::Arc;
use soradyne::storage::block_manager::BlockManager;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("🎬 Album Sync Test");
    println!("==================");
    println!("Testing CRDT-based album synchronization locally\n");
    
    // Create two sync managers (simulating two devices)
    let (mut alice_sync, _temp_dir_alice) = create_test_sync_manager();
    let (mut bob_sync, _temp_dir_bob) = create_test_sync_manager();
    
    // Alice creates an album
    println!("📸 Alice creates a photo album...");
    let album_id = alice_sync.create_album("Family Vacation".to_string())?;
    println!("   Album ID: {}", album_id);
    
    // Alice adds a photo
    println!("📷 Alice adds a photo...");
    let photo_data = create_mock_photo_data();
    alice_sync.add_media_to_album(
        &album_id,
        "photo1".to_string(),
        &photo_data,
        MediaType::Photo,
        "beach.jpg".to_string()
    ).await?;
    
    // Alice adds a comment
    println!("💬 Alice adds a comment...");
    let comment_op = EditOp::add_comment(
        "alice".to_string(),
        "Beautiful sunset!".to_string(),
        None
    );
    alice_sync.apply_operation(&album_id, &"photo1".to_string(), comment_op)?;
    
    // Show Alice's album state
    println!("\n📊 Alice's album state:");
    if let Some(album) = alice_sync.get_album(&album_id) {
        let states = album.reduce_all();
        for (media_id, state) in &states {
            println!("   Media {}: {} comments, rotation: {}°", 
                    media_id, state.comments.len(), state.rotation);
            if let Some(media) = &state.media {
                println!("     File: {} ({} bytes)", media.filename, media.size);
            }
            for comment in &state.comments {
                println!("     Comment: \"{}\" by {}", comment.text, comment.author);
            }
        }
    }
    
    // Simulate sync: Alice sends album to Bob
    println!("\n🔄 Syncing album from Alice to Bob...");
    if let Some(alice_album) = alice_sync.get_album(&album_id).cloned() {
        bob_sync.merge_album(alice_album)?;
    }
    
    // Bob adds his own comment
    println!("💬 Bob adds a comment...");
    let bob_comment_op = EditOp::add_comment(
        "bob".to_string(),
        "I love this place!".to_string(),
        None
    );
    bob_sync.apply_operation(&album_id, &"photo1".to_string(), bob_comment_op)?;
    
    // Bob rotates the photo
    println!("🔄 Bob rotates the photo...");
    let rotate_op = EditOp::rotate("bob".to_string(), 90.0);
    bob_sync.apply_operation(&album_id, &"photo1".to_string(), rotate_op)?;
    
    // Show Bob's album state
    println!("\n📊 Bob's album state:");
    if let Some(album) = bob_sync.get_album(&album_id) {
        let states = album.reduce_all();
        for (media_id, state) in &states {
            println!("   Media {}: {} comments, rotation: {}°", 
                    media_id, state.comments.len(), state.rotation);
            for comment in &state.comments {
                println!("     Comment: \"{}\" by {}", comment.text, comment.author);
            }
        }
    }
    
    // Sync back: Bob sends updates to Alice
    println!("\n🔄 Syncing updates from Bob back to Alice...");
    if let Some(bob_album) = bob_sync.get_album(&album_id).cloned() {
        alice_sync.merge_album(bob_album)?;
    }
    
    // Show final Alice state (should have both comments and rotation)
    println!("\n📊 Alice's final album state (after merge):");
    if let Some(album) = alice_sync.get_album(&album_id) {
        let states = album.reduce_all();
        for (media_id, state) in &states {
            println!("   Media {}: {} comments, rotation: {}°", 
                    media_id, state.comments.len(), state.rotation);
            for comment in &state.comments {
                println!("     Comment: \"{}\" by {}", comment.text, comment.author);
            }
        }
    }
    
    // Test concurrent edits
    println!("\n⚡ Testing concurrent edits...");
    
    // Both Alice and Bob add reactions at the same time
    let alice_reaction = EditOp::add_reaction(
        "alice".to_string(),
        uuid::Uuid::new_v4(), // React to first comment (simplified)
        "❤️".to_string()
    );
    
    let bob_reaction = EditOp::add_reaction(
        "bob".to_string(),
        uuid::Uuid::new_v4(), // React to first comment (simplified)
        "👍".to_string()
    );
    
    // Apply reactions to separate replicas
    alice_sync.apply_operation(&album_id, &"photo1".to_string(), alice_reaction)?;
    bob_sync.apply_operation(&album_id, &"photo1".to_string(), bob_reaction)?;
    
    // Merge both ways
    if let Some(alice_album) = alice_sync.get_album(&album_id).cloned() {
        bob_sync.merge_album(alice_album)?;
    }
    if let Some(bob_album) = bob_sync.get_album(&album_id).cloned() {
        alice_sync.merge_album(bob_album)?;
    }
    
    // Show final state with reactions
    println!("\n📊 Final state with reactions:");
    if let Some(album) = alice_sync.get_album(&album_id) {
        let states = album.reduce_all();
        for (media_id, state) in &states {
            println!("   Media {}: {} comments, {} reaction types", 
                    media_id, state.comments.len(), state.reactions.len());
            for (emoji, users) in &state.reactions {
                println!("     Reaction {}: {} users", emoji, users.len());
            }
        }
    }
    
    println!("\n✅ Album sync test completed successfully!");
    println!("   - Created album with media");
    println!("   - Added comments from multiple users");
    println!("   - Applied edits (rotation)");
    println!("   - Merged changes bidirectionally");
    println!("   - Handled concurrent reactions");
    println!("   - All data converged correctly");
    
    Ok(())
}

fn create_test_sync_manager() -> (AlbumSyncManager, TempDir) {
    let temp_dir = TempDir::new().unwrap();
    let test_dir = temp_dir.path().to_path_buf();
    
    // Create rimsd directories
    let mut rimsd_dirs = Vec::new();
    for i in 0..3 {
        let device_dir = test_dir.join(format!("rimsd_{}", i));
        let rimsd_dir = device_dir.join(".rimsd");
        std::fs::create_dir_all(&rimsd_dir).unwrap();
        rimsd_dirs.push(rimsd_dir);
    }
    
    let metadata_path = test_dir.join("metadata.json");
    let block_manager = Arc::new(BlockManager::new(
        rimsd_dirs,
        metadata_path,
        2, // threshold
        3, // total_shards
    ).unwrap());
    
    let sync_manager = AlbumSyncManager::new(block_manager, "test_replica".to_string());
    
    (sync_manager, temp_dir)
}

fn create_mock_photo_data() -> Vec<u8> {
    // Create mock JPEG data
    let mut data = Vec::new();
    
    // JPEG header
    data.extend_from_slice(b"\xFF\xD8\xFF\xE0");
    
    // Add some metadata
    data.extend_from_slice(b"Mock photo data for testing");
    
    // Add some mock image data
    for i in 0..1000 {
        data.push((i % 256) as u8);
    }
    
    // JPEG footer
    data.extend_from_slice(b"\xFF\xD9");
    
    data
}


// =====================================================================
// FILE: packages/soradyne_core/examples/renderer_test.rs
// =====================================================================

//! Test the media renderer with various edits and resolutions
//! 
//! This example demonstrates rendering media with crops, rotations, and markup
//! at different resolutions to verify the rendering pipeline works correctly.

use tokio;
use soradyne::album::*;
use soradyne::album::renderer::*;
use std::fs;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("🎨 Media Renderer Test");
    println!("======================");
    println!("Testing multi-resolution rendering with edits and markup\n");
    
    // Create output directory for test results
    let output_dir = std::path::Path::new("renderer_test_output");
    if output_dir.exists() {
        std::fs::remove_dir_all(output_dir)?;
    }
    std::fs::create_dir_all(output_dir)?;
    println!("📁 Created output directory: {:?}\n", output_dir);
    
    // Create a test media state with various edits
    let mut media_state = MediaState::default();
    
    // Add a crop
    media_state.crop = Some(CropData {
        left: 0.1,
        top: 0.1,
        right: 0.9,
        bottom: 0.9,
    });
    
    // Add rotation
    media_state.rotation = 90.0;
    
    // Add some markup
    let circle_data = serde_json::to_value(CircleMarkup {
        center_x: 0.3,
        center_y: 0.3,
        radius: 0.1,
        color: [255, 0, 0, 255], // Red
        filled: false,
        stroke_width: 0.02,
    })?;
    
    media_state.markup.push(MarkupElement {
        id: uuid::Uuid::new_v4(),
        markup_type: MarkupType::Circle,
        data: circle_data,
        author: "test".to_string(),
        timestamp: 1,
    });
    
    let rect_data = serde_json::to_value(RectangleMarkup {
        x: 0.6,
        y: 0.6,
        width: 0.3,
        height: 0.2,
        color: [0, 255, 0, 128], // Semi-transparent green
        filled: true,
        stroke_width: 0.01,
    })?;
    
    media_state.markup.push(MarkupElement {
        id: uuid::Uuid::new_v4(),
        markup_type: MarkupType::Rectangle,
        data: rect_data,
        author: "test".to_string(),
        timestamp: 2,
    });
    
    // Create a simple test image
    let test_image_data = create_test_image();
    
    // Test rendering at different resolutions
    println!("🖼️  Rendering at different resolutions...");
    
    // Thumbnail
    println!("   📱 Generating thumbnail...");
    let thumbnail = media_state.render_thumbnail(&test_image_data)?;
    let thumbnail_path = output_dir.join("thumbnail.png");
    fs::write(&thumbnail_path, &thumbnail)?;
    println!("      Saved: {:?} ({} bytes)", thumbnail_path, thumbnail.len());
    
    // Preview
    println!("   🖥️  Generating preview...");
    let preview = media_state.render_preview(&test_image_data)?;
    let preview_path = output_dir.join("preview.png");
    fs::write(&preview_path, &preview)?;
    println!("      Saved: {:?} ({} bytes)", preview_path, preview.len());
    
    // Full resolution
    println!("   🎯 Generating full resolution...");
    let full = media_state.render(&test_image_data, RenderResolution::Full)?;
    let full_path = output_dir.join("full_resolution.png");
    fs::write(&full_path, &full)?;
    println!("      Saved: {:?} ({} bytes)", full_path, full.len());
    
    // Custom resolution
    println!("   ⚙️  Generating custom 400x300...");
    let custom = media_state.render(&test_image_data, RenderResolution::Custom(400, 300))?;
    let custom_path = output_dir.join("custom_400x300.png");
    fs::write(&custom_path, &custom)?;
    println!("      Saved: {:?} ({} bytes)", custom_path, custom.len());
    
    println!("\n✅ Renderer test completed successfully!");
    println!("   Generated files in {:?}:", output_dir);
    println!("   - thumbnail.png (thumbnail)");
    println!("   - preview.png (preview)");
    println!("   - full_resolution.png (full resolution)");
    println!("   - custom_400x300.png (400x300 custom)");
    println!("\n💡 Open these files to verify:");
    println!("   - Crop applied (10% border removed)");
    println!("   - 90° rotation applied");
    println!("   - Red circle at top-left");
    println!("   - Green rectangle at bottom-right");
    println!("\n🌐 Quick view command:");
    println!("   open {:?}", output_dir);
    
    Ok(())
}

fn create_test_image() -> Vec<u8> {
    use image::{RgbImage, Rgb};
    
    // Create a 200x200 test image with a gradient
    let mut img = RgbImage::new(200, 200);
    
    for (x, y, pixel) in img.enumerate_pixels_mut() {
        let r = (x as f32 / 200.0 * 255.0) as u8;
        let g = (y as f32 / 200.0 * 255.0) as u8;
        let b = 128;
        *pixel = Rgb([r, g, b]);
    }
    
    // Encode to PNG
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png).unwrap();
    
    buffer
}


// =====================================================================
// FILE: packages/soradyne_core/examples/web_album_server.rs
// =====================================================================

//! Web Album Server
//! 
//! A locally-hosted web server that provides a photo album interface
//! using Soradyne's block storage and CRDT album system.

use std::collections::HashMap;
use std::sync::Arc;
use std::path::PathBuf;
use tokio::sync::RwLock;
use warp::{Filter, Reply};
use serde::{Deserialize, Serialize};
use uuid::Uuid;
use std::process::Command;
use soradyne::album::album::*;
use soradyne::album::operations::*;
use soradyne::album::crdt::*;
use soradyne::storage::block_manager::BlockManager;
use soradyne::storage::block_file::BlockFile;
use soradyne::video::{generate_video_at_size, generate_image_at_size, is_video_file, is_audio_file};

#[derive(Debug, Serialize, Deserialize)]
struct CreateAlbumRequest {
    name: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct AlbumResponse {
    id: String,
    name: String,
    item_count: usize,
}

#[derive(Debug, Serialize, Deserialize)]
struct MediaItemResponse {
    id: String,
    filename: String,
    media_type: String,
    size: usize,
    comments: Vec<CommentResponse>,
    rotation: f32,
    has_crop: bool,
    markup_count: usize,
}

#[derive(Debug, Serialize, Deserialize)]
struct CommentResponse {
    author: String,
    text: String,
    timestamp: u64,
}

#[derive(Debug, Serialize, Deserialize)]
struct AddCommentRequest {
    text: String,
    author: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct RotateRequest {
    degrees: f32,
    author: String,
}

pub struct WebAlbumServer {
    albums: Arc<RwLock<HashMap<String, MediaAlbum>>>, // album_id -> album
    block_manager: Arc<BlockManager>,
    data_dir: PathBuf,
    albums_index_block_id: Option<[u8; 32]>, // Block ID containing the album index
}

impl WebAlbumServer {
    pub fn new() -> Result<Self, Box<dyn std::error::Error>> {
        // Use a persistent data directory in the user's home directory
        let home_dir = std::env::var("HOME").unwrap_or_else(|_| ".".to_string());
        let data_dir = PathBuf::from(home_dir).join(".soradyne_albums");
        std::fs::create_dir_all(&data_dir)?;
        
        // Create rimsd directories
        let mut rimsd_dirs = Vec::new();
        for i in 0..4 {
            let device_dir = data_dir.join(format!("rimsd_{}", i));
            let rimsd_dir = device_dir.join(".rimsd");
            std::fs::create_dir_all(&rimsd_dir)?;
            rimsd_dirs.push(rimsd_dir);
        }
        
        let metadata_path = data_dir.join("metadata.json");
        
        let block_manager = Arc::new(BlockManager::new(
            rimsd_dirs,
            metadata_path,
            3, // threshold
            4, // total_shards
        )?);
        
        let server = Self {
            albums: Arc::new(RwLock::new(HashMap::new())),
            block_manager,
            data_dir,
            albums_index_block_id: None,
        };
        
        Ok(server)
    }
    
    pub async fn initialize(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // Load existing albums from block storage
        self.load_albums_from_blocks().await?;
        Ok(())
    }
    
    async fn load_albums_from_blocks(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // Try to load the albums index from a known location
        let index_file = self.data_dir.join("albums_index.txt");
        
        if index_file.exists() {
            let index_content = std::fs::read_to_string(&index_file)?;
            if let Ok(block_id_bytes) = hex::decode(index_content.trim()) {
                if block_id_bytes.len() == 32 {
                    let mut block_id = [0u8; 32];
                    block_id.copy_from_slice(&block_id_bytes);
                    self.albums_index_block_id = Some(block_id);
                    
                    // Load the albums index
                    if let Ok(index_data) = self.block_manager.read_block(&block_id).await {
                        if let Ok(index_json) = String::from_utf8(index_data) {
                            if let Ok(album_index) = serde_json::from_str::<HashMap<String, [u8; 32]>>(&index_json) {
                                // Load each album from its block
                                let mut albums = HashMap::new();
                                for (album_id, album_block_id) in album_index {
                                    if let Ok(album_data) = self.block_manager.read_block(&album_block_id).await {
                                        if let Ok(album_json) = String::from_utf8(album_data) {
                                            if let Ok(mut album) = serde_json::from_str::<MediaAlbum>(&album_json) {
                                                // Restore the block manager reference
                                                album.block_manager = Some(Arc::clone(&self.block_manager));
                                                albums.insert(album_id, album);
                                            }
                                        }
                                    }
                                }
                                
                                *self.albums.write().await = albums;
                                println!("Loaded {} albums from block storage", self.albums.read().await.len());
                            }
                        }
                    }
                }
            }
        }
        
        Ok(())
    }
    
    async fn save_albums_to_blocks(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        let albums = self.albums.read().await;
        
        // Create an index mapping album IDs to their block IDs
        let mut album_index = HashMap::new();
        
        // Save each album to its own block
        for (album_id, album) in albums.iter() {
            // Create a serializable version without the block manager
            let mut serializable_album = album.clone();
            serializable_album.block_manager = None;
            
            let album_json = serde_json::to_string_pretty(&serializable_album)?;
            let album_data = album_json.as_bytes();
            
            // Store the album in a block
            let album_block_id = self.block_manager.write_direct_block(album_data).await?;
            album_index.insert(album_id.clone(), album_block_id);
        }
        
        // Save the index to a block
        let index_json = serde_json::to_string_pretty(&album_index)?;
        let index_data = index_json.as_bytes();
        let index_block_id = self.block_manager.write_direct_block(index_data).await?;
        
        // Store the index block ID in a simple file for bootstrapping
        let index_file = self.data_dir.join("albums_index.txt");
        std::fs::write(&index_file, hex::encode(index_block_id))?;
        
        self.albums_index_block_id = Some(index_block_id);
        
        Ok(())
    }
    
    pub async fn start_server(self: Arc<Self>, port: u16) -> Result<(), Box<dyn std::error::Error>> {
        let server = Arc::clone(&self);
        
        // Enable better logging
        env_logger::init();
        
        // CORS headers for local development - make it more permissive
        let cors = warp::cors()
            .allow_any_origin()
            .allow_headers(vec!["content-type", "authorization", "accept"])
            .allow_methods(vec!["GET", "POST", "PUT", "DELETE", "OPTIONS"]);
        
        // Static files route
        let static_files = warp::path("static")
            .and(warp::fs::dir("web_static"));
        
        // API routes
        let api = warp::path("api");
        
        // Get all albums
        let get_albums = api
            .and(warp::path("albums"))
            .and(warp::path::end())
            .and(warp::get())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_get_albums);
        
        // Create album
        let create_album = api
            .and(warp::path("albums"))
            .and(warp::path::end())
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_create_album);
        
        // Get album details
        let get_album = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path::end())
            .and(warp::get())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_get_album);
        
        // Upload media to album
        let upload_media = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path("media"))
            .and(warp::path::end())
            .and(warp::post())
            .and(warp::body::bytes())
            .and(warp::header::<String>("content-type"))
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_upload_media);
        
        // Get media thumbnail
        let get_thumbnail = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path("media"))
            .and(warp::path::param::<String>())
            .and(warp::path("thumbnail"))
            .and(warp::path::end())
            .and(warp::get())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_get_thumbnail);
        
        // Get media medium resolution
        let get_medium = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path("media"))
            .and(warp::path::param::<String>())
            .and(warp::path("medium"))
            .and(warp::path::end())
            .and(warp::get())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_get_medium);
        
        // Get media high resolution
        let get_high = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path("media"))
            .and(warp::path::param::<String>())
            .and(warp::path("high"))
            .and(warp::path::end())
            .and(warp::get())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_get_high);
        
        // Add comment to media
        let add_comment = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path("media"))
            .and(warp::path::param::<String>())
            .and(warp::path("comments"))
            .and(warp::path::end())
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_add_comment);
        
        // Rotate media
        let rotate_media = api
            .and(warp::path("albums"))
            .and(warp::path::param::<String>())
            .and(warp::path("media"))
            .and(warp::path::param::<String>())
            .and(warp::path("rotate"))
            .and(warp::path::end())
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(Arc::clone(&server)))
            .and_then(handle_rotate_media);
        
        // Root route serves a simple HTML page
        let root = warp::path::end()
            .and(warp::get())
            .map(|| warp::reply::html(r#"
<!DOCTYPE html>
<html>
<head>
    <title>Soradyne Web Album</title>
    <meta charset="utf-8">
    <style>
        * {
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }
        
        h1 {
            text-align: center;
            color: #4a5568;
            margin-bottom: 30px;
            font-size: 2.5em;
            font-weight: 300;
        }
        
        h2 {
            color: #2d3748;
            border-bottom: 2px solid #e2e8f0;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        
        h3 {
            color: #4a5568;
            margin-bottom: 15px;
        }
        
        .album {
            background: white;
            border: 1px solid #e2e8f0;
            border-radius: 12px;
            margin: 15px 0;
            padding: 25px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
        }
        
        .album:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
        }
        
        .media-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        
        .media-item {
            background: white;
            border-radius: 12px;
            padding: 15px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
        }
        
        .media-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
        }
        
        .thumbnail {
            width: 150px;
            height: 150px;
            object-fit: cover;
            border-radius: 8px;
            margin-bottom: 10px;
            border: 2px solid #e2e8f0;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .modal-image {
            max-width: 90vw;
            max-height: 90vh;
            width: 90vw;
            height: auto;
            object-fit: contain;
            border-radius: 8px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5);
        }
        
        .thumbnail:hover {
            transform: scale(1.05);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.2);
        }
        
        .media-filename {
            font-size: 0.9em;
            color: #4a5568;
            margin-bottom: 10px;
            word-break: break-word;
        }
        
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 500;
            transition: all 0.3s ease;
            margin: 5px;
        }
        
        button:hover {
            transform: translateY(-1px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        button:active {
            transform: translateY(0);
        }
        
        .btn-secondary {
            background: linear-gradient(135deg, #718096 0%, #4a5568 100%);
        }
        
        .btn-secondary:hover {
            box-shadow: 0 4px 12px rgba(113, 128, 150, 0.4);
        }
        
        .upload-area {
            border: 3px dashed #cbd5e0;
            border-radius: 12px;
            padding: 40px;
            text-align: center;
            margin: 20px 0;
            background: #f7fafc;
            transition: all 0.3s ease;
            cursor: pointer;
        }
        
        .upload-area.drag-over {
            border-color: #667eea;
            background: #edf2f7;
            transform: scale(1.02);
        }
        
        .upload-area:hover {
            border-color: #a0aec0;
            background: #edf2f7;
        }
        
        .upload-text {
            color: #4a5568;
            font-size: 1.1em;
            margin-bottom: 15px;
        }
        
        .upload-subtext {
            color: #718096;
            font-size: 0.9em;
        }
        
        input[type="file"] {
            display: none;
        }
        
        .controls {
            display: flex;
            gap: 10px;
            align-items: center;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }
        
        .back-button {
            background: linear-gradient(135deg, #718096 0%, #4a5568 100%);
        }
        
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #667eea;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin-left: 10px;
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        
        .notification {
            position: fixed;
            top: 20px;
            right: 20px;
            background: #48bb78;
            color: white;
            padding: 15px 20px;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            transform: translateX(400px);
            transition: transform 0.3s ease;
            z-index: 1000;
        }
        
        .notification.show {
            transform: translateX(0);
        }
        
        .notification.error {
            background: #f56565;
        }
        
        /* Modal/Lightbox styles */
        .modal {
            display: none;
            position: fixed;
            z-index: 2000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.9);
            backdrop-filter: blur(5px);
        }
        
        .modal.show {
            display: flex;
            align-items: center;
            justify-content: center;
            animation: fadeIn 0.3s ease;
        }
        
        .modal-content {
            position: relative;
            max-width: 90vw;
            max-height: 90vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .modal-image {
            max-width: 100%;
            max-height: 100%;
            object-fit: contain;
            border-radius: 8px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5);
            transition: opacity 0.5s ease;
        }
        
        .modal-image.loading {
            opacity: 0.7;
            filter: blur(2px);
        }
        
        .modal-close {
            position: absolute;
            top: 20px;
            right: 30px;
            color: white;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
            z-index: 2001;
            background: rgba(0, 0, 0, 0.5);
            border-radius: 50%;
            width: 60px;
            height: 60px;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
        }
        
        .modal-close:hover {
            background: rgba(0, 0, 0, 0.8);
            transform: scale(1.1);
        }
        
        .modal-loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: white;
            font-size: 18px;
            z-index: 2001;
        }
        
        .resolution-indicator {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.7);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            z-index: 2001;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>🎨 Soradyne Web Album</h1>
        <div id="app">
            <h2>Albums</h2>
            <div id="albums"></div>
            <button onclick="createAlbum()">✨ Create New Album</button>
        </div>
    </div>
    
    <div id="notification" class="notification"></div>
    
    <!-- Modal for full-size image viewing -->
    <div id="imageModal" class="modal">
        <span class="modal-close" onclick="closeModal()">&times;</span>
        <div class="modal-content">
            <img id="modalImage" class="modal-image" src="" alt="Full size image" />
            <div id="modalLoading" class="modal-loading" style="display: none;">Loading higher resolution...</div>
            <div id="resolutionIndicator" class="resolution-indicator">Thumbnail</div>
        </div>
    </div>
    
    <script>
        let currentAlbumId = null;
        
        function showNotification(message, isError = false) {
            const notification = document.getElementById('notification');
            notification.textContent = message;
            notification.className = `notification ${isError ? 'error' : ''} show`;
            setTimeout(() => {
                notification.className = 'notification';
            }, 3000);
        }
        
        async function loadAlbums() {
            try {
                const response = await fetch('/api/albums');
                const albums = await response.json();
                const container = document.getElementById('albums');
                container.innerHTML = albums.map(album => `
                    <div class="album">
                        <h3>📁 ${album.name}</h3>
                        <p>Items: ${album.item_count}</p>
                        <button onclick="viewAlbum('${album.id}')">View Album</button>
                    </div>
                `).join('');
                currentAlbumId = null;
            } catch (error) {
                showNotification('Failed to load albums', true);
            }
        }
        
        async function createAlbum() {
            const name = prompt('Album name:');
            if (name) {
                try {
                    await fetch('/api/albums', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ name })
                    });
                    showNotification('Album created successfully!');
                    loadAlbums();
                } catch (error) {
                    showNotification('Failed to create album', true);
                }
            }
        }
        
        async function viewAlbum(albumId) {
            currentAlbumId = albumId;
            try {
                const response = await fetch(`/api/albums/${albumId}`);
                const items = await response.json();
                const container = document.getElementById('albums');
                container.innerHTML = `
                    <div class="controls">
                        <button class="back-button" onclick="loadAlbums()">← Back to Albums</button>
                        <h3>Album Contents</h3>
                    </div>
                    
                    <div class="upload-area" id="uploadArea" onclick="document.getElementById('fileInput').click()">
                        <div class="upload-text">📎 Drop files here or click to upload</div>
                        <div class="upload-subtext">Supports images, videos, and audio files</div>
                        <input type="file" id="fileInput" accept="image/*,video/*,audio/*" multiple />
                    </div>
                    
                    <div class="media-grid">
                        ${items.map(item => `
                            <div class="media-item">
                                <img src="/api/albums/${albumId}/media/${item.id}/thumbnail" 
                                     class="thumbnail" 
                                     onclick="openModal('${albumId}', '${item.id}')" />
                            </div>
                        `).join('')}
                    </div>
                `;
                
                setupDragAndDrop();
                setupFileInput();
            } catch (error) {
                showNotification('Failed to load album', true);
            }
        }
        
        function setupDragAndDrop() {
            const uploadArea = document.getElementById('uploadArea');
            if (!uploadArea) return;
            
            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {
                uploadArea.addEventListener(eventName, preventDefaults, false);
                document.body.addEventListener(eventName, preventDefaults, false);
            });
            
            ['dragenter', 'dragover'].forEach(eventName => {
                uploadArea.addEventListener(eventName, highlight, false);
            });
            
            ['dragleave', 'drop'].forEach(eventName => {
                uploadArea.addEventListener(eventName, unhighlight, false);
            });
            
            uploadArea.addEventListener('drop', handleDrop, false);
            
            function preventDefaults(e) {
                e.preventDefault();
                e.stopPropagation();
            }
            
            function highlight(e) {
                uploadArea.classList.add('drag-over');
            }
            
            function unhighlight(e) {
                uploadArea.classList.remove('drag-over');
            }
            
            function handleDrop(e) {
                const dt = e.dataTransfer;
                const files = dt.files;
                handleFiles(files);
            }
        }
        
        function setupFileInput() {
            const fileInput = document.getElementById('fileInput');
            if (fileInput) {
                fileInput.addEventListener('change', function(e) {
                    handleFiles(e.target.files);
                });
            }
        }
        
        async function handleFiles(files) {
            if (!currentAlbumId) return;
            
            for (let file of files) {
                await uploadSingleFile(file);
            }
            
            // Refresh the album view
            viewAlbum(currentAlbumId);
        }
        
        async function uploadSingleFile(file) {
            try {
                const formData = new FormData();
                formData.append('file', file);
                
                showNotification(`Uploading ${file.name}...`);
                
                const response = await fetch(`/api/albums/${currentAlbumId}/media`, {
                    method: 'POST',
                    body: formData
                });
                
                if (response.ok) {
                    showNotification(`${file.name} uploaded successfully!`);
                } else {
                    throw new Error('Upload failed');
                }
            } catch (error) {
                showNotification(`Failed to upload ${file.name}`, true);
            }
        }
        
        async function rotateMedia(albumId, mediaId) {
            try {
                await fetch(`/api/albums/${albumId}/media/${mediaId}/rotate`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ degrees: 90, author: 'web_user' })
                });
                showNotification('Media rotated!');
                viewAlbum(albumId);
            } catch (error) {
                showNotification('Failed to rotate media', true);
            }
        }
        
        // Modal functionality
        function openModal(albumId, mediaId) {
            const modal = document.getElementById('imageModal');
            const modalImage = document.getElementById('modalImage');
            const resolutionIndicator = document.getElementById('resolutionIndicator');
            const modalLoading = document.getElementById('modalLoading');
            
            // Show modal with thumbnail first
            const thumbnailUrl = `/api/albums/${albumId}/media/${mediaId}/thumbnail`;
            modalImage.src = thumbnailUrl;
            modal.classList.add('show');
            resolutionIndicator.textContent = 'Thumbnail';
            
            // Start loading medium resolution
            setTimeout(() => {
                loadMediumResolution(albumId, mediaId);
            }, 100);
        }
        
        function loadMediumResolution(albumId, mediaId) {
            const modalImage = document.getElementById('modalImage');
            const resolutionIndicator = document.getElementById('resolutionIndicator');
            const modalLoading = document.getElementById('modalLoading');
            
            modalLoading.style.display = 'block';
            modalLoading.textContent = 'Loading medium resolution...';
            resolutionIndicator.textContent = 'Loading medium...';
            
            // Create a new image to preload medium resolution
            const mediumImg = new Image();
            mediumImg.onload = function() {
                modalImage.src = mediumImg.src;
                resolutionIndicator.textContent = 'Medium Resolution';
                modalLoading.style.display = 'none';
                
                // Start loading high resolution after a short delay
                setTimeout(() => {
                    loadHighResolution(albumId, mediaId);
                }, 500);
            };
            mediumImg.onerror = function() {
                modalLoading.style.display = 'none';
                resolutionIndicator.textContent = 'Thumbnail (medium failed)';
                // Still try to load high resolution
                setTimeout(() => {
                    loadHighResolution(albumId, mediaId);
                }, 500);
            };
            
            // Use the medium resolution endpoint
            mediumImg.src = `/api/albums/${albumId}/media/${mediaId}/medium`;
        }
        
        function loadHighResolution(albumId, mediaId) {
            const modalImage = document.getElementById('modalImage');
            const resolutionIndicator = document.getElementById('resolutionIndicator');
            const modalLoading = document.getElementById('modalLoading');
            
            modalLoading.style.display = 'block';
            modalLoading.textContent = 'Loading high resolution...';
            resolutionIndicator.textContent = 'Loading high...';
            
            // Create a new image to preload high resolution
            const highImg = new Image();
            highImg.onload = function() {
                // Smoothly transition without blanking out
                modalImage.src = highImg.src;
                resolutionIndicator.textContent = 'High Resolution';
                modalLoading.style.display = 'none';
            };
            highImg.onerror = function() {
                modalLoading.style.display = 'none';
                resolutionIndicator.textContent = 'Medium Resolution (high failed)';
            };
            
            // Use the high resolution endpoint
            highImg.src = `/api/albums/${albumId}/media/${mediaId}/high`;
        }
        
        function closeModal() {
            const modal = document.getElementById('imageModal');
            const modalImage = document.getElementById('modalImage');
            const modalLoading = document.getElementById('modalLoading');
            
            modal.classList.remove('show');
            modalLoading.style.display = 'none';
            
            // Clear the image source after animation
            setTimeout(() => {
                modalImage.src = '';
                modalImage.classList.remove('loading');
            }, 300);
        }
        
        // Close modal when clicking outside the image
        document.getElementById('imageModal').addEventListener('click', function(e) {
            if (e.target === this) {
                closeModal();
            }
        });
        
        // Close modal with Escape key
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                closeModal();
            }
        });
        
        // Load albums on page load
        loadAlbums();
    </script>
</body>
</html>
            "#));
        
        // Add logging filter to see all requests
        let log = warp::log("api");
        
        let routes = root
            .or(static_files)
            .or(get_albums)
            .or(create_album)
            .or(get_album)
            .or(upload_media)
            .or(get_thumbnail)
            .or(get_medium)
            .or(get_high)
            .or(add_comment)
            .or(rotate_media)
            .recover(handle_rejection)
            .with(cors)
            .with(log);
        
        println!("🌐 Starting web album server on http://localhost:{}", port);
        println!("📁 Album interface available at http://localhost:{}", port);
        
        // Try to bind to the port, if it fails, try the next few ports
        let mut current_port = port;
        let max_attempts = 10;
        
        for attempt in 0..max_attempts {
            match std::net::TcpListener::bind((std::net::Ipv4Addr::new(127, 0, 0, 1), current_port)) {
                Ok(listener) => {
                    drop(listener); // Release the port for warp to use
                    if current_port != port {
                        println!("🔄 Port {} was busy, using port {} instead", port, current_port);
                        println!("🌐 Starting web album server on http://localhost:{}", current_port);
                        println!("📁 Album interface available at http://localhost:{}", current_port);
                    }
                    
                    warp::serve(routes)
                        .run((std::net::Ipv4Addr::new(127, 0, 0, 1), current_port))
                        .await;
                    return Ok(());
                }
                Err(_) => {
                    current_port += 1;
                    if attempt == max_attempts - 1 {
                        eprintln!("❌ Could not bind to any port from {} to {}", port, current_port);
                        return Err("Failed to bind to any available port".into());
                    }
                }
            }
        }
        
        Ok(())
    }
}

fn with_server(server: Arc<WebAlbumServer>) -> impl Filter<Extract = (Arc<WebAlbumServer>,), Error = std::convert::Infallible> + Clone {
    warp::any().map(move || Arc::clone(&server))
}

async fn handle_get_albums(server: Arc<WebAlbumServer>) -> Result<impl Reply, warp::Rejection> {
    let albums = server.albums.read().await;
    let album_list: Vec<AlbumResponse> = albums.iter().map(|(id, album)| {
        AlbumResponse {
            id: id.clone(),
            name: album.metadata.title.clone(),
            item_count: album.items.len(),
        }
    }).collect();
    
    Ok(warp::reply::json(&album_list))
}

async fn handle_create_album(req: CreateAlbumRequest, server: Arc<WebAlbumServer>) -> Result<impl Reply, warp::Rejection> {
    let album_id = Uuid::new_v4().to_string();
    let album_id_clone = album_id.clone();
    
    let album = MediaAlbum {
        album_id: album_id.clone(),
        items: HashMap::new(),
        metadata: AlbumMetadata {
            title: req.name.clone(),
            created_by: "web_user".to_string(),
            created_at: chrono::Utc::now().timestamp() as u64,
            shared_with: HashMap::new(),
        },
        block_manager: Some(Arc::clone(&server.block_manager)),
    };
    
    let mut albums = server.albums.write().await;
    albums.insert(album_id.clone(), album);
    drop(albums); // Release the lock before saving
    
    // Save albums to block storage
    let server_clone = Arc::clone(&server);
    tokio::spawn(async move {
        if let Err(e) = save_album_update(server_clone, album_id_clone).await {
            eprintln!("Failed to save albums to blocks: {}", e);
        }
    });
    
    let response = AlbumResponse {
        id: album_id,
        name: req.name,
        item_count: 0,
    };
    Ok(warp::reply::json(&response))
}

async fn handle_get_album(album_id: String, server: Arc<WebAlbumServer>) -> Result<impl Reply, warp::Rejection> {
    let albums = server.albums.read().await;
    
    if let Some(album) = albums.get(&album_id) {
        let media_items: Vec<MediaItemResponse> = album.items.iter().map(|(media_id, crdt)| {
            let state = crdt.reduce();
            
            MediaItemResponse {
                id: media_id.clone(),
                filename: format!("media_{}", media_id), // Placeholder filename
                media_type: "image/jpeg".to_string(), // Placeholder type
                size: 0, // Placeholder size
                comments: Vec::new(), // TODO: Extract comments from state
                rotation: state.rotation,
                has_crop: state.crop.is_some(),
                markup_count: state.markup.len(),
            }
        }).collect();
        
        Ok(warp::reply::json(&media_items))
    } else {
        Ok(warp::reply::json(&serde_json::json!({"error": "Album not found"})))
    }
}

async fn handle_upload_media(
    album_id: String,
    body: bytes::Bytes,
    content_type: String,
    server: Arc<WebAlbumServer>
) -> Result<impl Reply, warp::Rejection> {
    
    println!("Received upload request for album: {}", album_id);
    println!("Content-Type: {}", content_type);
    println!("Body size: {} bytes", body.len());
    
    // Parse the boundary from content-type
    let boundary = content_type
        .split("boundary=")
        .nth(1)
        .ok_or_else(|| {
            eprintln!("No boundary found in content-type");
            warp::reject::reject()
        })?;
    
    println!("Boundary: {}", boundary);
    
    // Use multer to parse the multipart data
    let mut multipart = multer::Multipart::new(futures_util::stream::once(async { Ok::<_, std::io::Error>(body) }), boundary);
    
    while let Some(field) = multipart.next_field().await.map_err(|e| {
        eprintln!("Multipart parsing error: {}", e);
        warp::reject::reject()
    })? {
        let name = field.name().unwrap_or("").to_string();
        println!("Processing field: {}", name);
        
        if name == "file" {
            let filename = field.file_name().unwrap_or("unknown").to_string();
            let data = field.bytes().await.map_err(|e| {
                eprintln!("Failed to read field data: {}", e);
                warp::reject::reject()
            })?;
            
            let media_id = Uuid::new_v4().to_string();
            
            println!("Uploading file: {} ({} bytes)", filename, data.len());
            
            // Store the media data in block storage using BlockFile for large files
            let block_file = BlockFile::new(Arc::clone(&server.block_manager));
            match block_file.append(&data).await {
                Ok(()) => {
                    let block_id = block_file.root_block().await
                        .ok_or_else(|| {
                            eprintln!("Failed to get root block ID after write");
                            warp::reject::reject()
                        })?;
                    // Detect media type
                    let media_type = if is_video_file(&data) {
                        "video"
                    } else if is_audio_file(&data) {
                        "audio"
                    } else {
                        "image"
                    };
                    
                    // Create an operation to add media
                    let op = EditOp {
                        op_id: Uuid::new_v4(),
                        timestamp: chrono::Utc::now().timestamp() as u64,
                        author: "web_user".to_string(),
                        op_type: "add_media".to_string(),
                        payload: serde_json::json!({
                            "filename": filename,
                            "block_id": hex::encode(block_id),
                            "size": data.len(),
                            "media_type": media_type
                        }),
                    };
                    
                    // Add to album
                    let mut albums = server.albums.write().await;
                    if let Some(album) = albums.get_mut(&album_id) {
                        let crdt = album.get_or_create(&media_id);
                        match crdt.apply_local(op) {
                            Ok(_) => {
                                println!("Successfully added media {} to album {}", media_id, album_id);
                                
                                // Save albums to block storage
                                drop(albums); // Release the lock before saving
                                let server_clone = Arc::clone(&server);
                                tokio::spawn(async move {
                                    // We can't easily get a mutable reference here, so we'll implement
                                    // a different approach for saving individual album updates
                                    if let Err(e) = save_album_update(server_clone, album_id.clone()).await {
                                        eprintln!("Failed to save album update: {}", e);
                                    }
                                });
                                
                                return Ok(warp::reply::json(&serde_json::json!({
                                    "success": true,
                                    "media_id": media_id
                                })));
                            }
                            Err(e) => {
                                eprintln!("Failed to apply operation: {}", e);
                                return Ok(warp::reply::json(&serde_json::json!({
                                    "error": "Failed to apply operation"
                                })));
                            }
                        }
                    } else {
                        eprintln!("Album {} not found", album_id);
                        return Ok(warp::reply::json(&serde_json::json!({
                            "error": "Album not found"
                        })));
                    }
                }
                Err(e) => {
                    eprintln!("Failed to store media data: {}", e);
                    return Ok(warp::reply::json(&serde_json::json!({
                        "error": "Failed to store media"
                    })));
                }
            }
        }
    }
    
    Ok(warp::reply::json(&serde_json::json!({
        "error": "No file found in upload"
    })))
}

async fn handle_get_thumbnail(
    album_id: String,
    media_id: String,
    server: Arc<WebAlbumServer>
) -> Result<impl Reply, warp::Rejection> {
    get_media_at_resolution(album_id, media_id, server, 150).await
}

async fn handle_get_medium(
    album_id: String,
    media_id: String,
    server: Arc<WebAlbumServer>
) -> Result<impl Reply, warp::Rejection> {
    get_media_at_resolution(album_id, media_id, server, 600).await
}

async fn handle_get_high(
    album_id: String,
    media_id: String,
    server: Arc<WebAlbumServer>
) -> Result<impl Reply, warp::Rejection> {
    get_media_at_resolution(album_id, media_id, server, 1200).await
}

async fn get_media_at_resolution(
    album_id: String,
    media_id: String,
    server: Arc<WebAlbumServer>,
    max_size: u32
) -> Result<impl Reply, warp::Rejection> {
    let albums = server.albums.read().await;
    
    if let Some(album) = albums.get(&album_id) {
        if let Some(crdt) = album.items.get(&media_id) {
            // Try to find the block_id from the operations
            for op in crdt.ops() {
                if op.op_type == "add_media" {
                    if let Some(block_id_hex) = op.payload.get("block_id").and_then(|v| v.as_str()) {
                        if let Ok(block_id_bytes) = hex::decode(block_id_hex) {
                            if block_id_bytes.len() == 32 {
                                let mut block_id = [0u8; 32];
                                block_id.copy_from_slice(&block_id_bytes);
                                
                                // Use BlockFile to read large files properly
                                let block_file = BlockFile::from_existing(
                                    Arc::clone(&server.block_manager),
                                    block_id,
                                    op.payload.get("size").and_then(|v| v.as_u64()).unwrap_or(0) as usize
                                );
                                
                                // Try to read the original media data using BlockFile
                                match block_file.read().await {
                                    Ok(media_data) => {
                                        println!("Successfully read {} bytes from block storage for media {}", media_data.len(), media_id);
                                        
                                        // Generate resized image from the actual media
                                        if let Ok(resized_image) = generate_resized_media(&media_data, max_size) {
                                            return Ok(warp::reply::with_header(
                                                resized_image,
                                                "content-type",
                                                "image/jpeg"
                                            ));
                                        }
                                    }
                                    Err(e) => {
                                        println!("Failed to read media data from BlockFile: {}", e);
                                        
                                        // Fall back to direct block read for smaller files
                                        if let Ok(media_data) = server.block_manager.read_block(&block_id).await {
                                            println!("Fallback: read {} bytes directly from block manager", media_data.len());
                                            
                                            if let Ok(resized_image) = generate_resized_media(&media_data, max_size) {
                                                return Ok(warp::reply::with_header(
                                                    resized_image,
                                                    "content-type",
                                                    "image/jpeg"
                                                ));
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    
    println!("No media data found for album {} media {}, returning placeholder", album_id, media_id);
    Ok(warp::reply::with_header(
        create_placeholder_thumbnail(),
        "content-type",
        "image/png"
    ))
}

async fn handle_add_comment(
    album_id: String,
    media_id: String,
    req: AddCommentRequest,
    server: Arc<WebAlbumServer>
) -> Result<impl Reply, warp::Rejection> {
    let comment_op = EditOp {
        op_id: Uuid::new_v4(),
        timestamp: chrono::Utc::now().timestamp() as u64,
        author: req.author,
        op_type: "add_comment".to_string(),
        payload: serde_json::json!({
            "text": req.text
        }),
    };
    
    let mut albums = server.albums.write().await;
    if let Some(album) = albums.get_mut(&album_id) {
        let crdt = album.get_or_create(&media_id);
        match crdt.apply_local(comment_op) {
            Ok(_) => Ok(warp::reply::json(&serde_json::json!({"success": true}))),
            Err(e) => {
                eprintln!("Failed to add comment: {}", e);
                Ok(warp::reply::json(&serde_json::json!({"error": "Failed to add comment"})))
            }
        }
    } else {
        Ok(warp::reply::json(&serde_json::json!({"error": "Album not found"})))
    }
}

async fn handle_rotate_media(
    album_id: String,
    media_id: String,
    req: RotateRequest,
    server: Arc<WebAlbumServer>
) -> Result<impl Reply, warp::Rejection> {
    let rotate_op = EditOp {
        op_id: Uuid::new_v4(),
        timestamp: chrono::Utc::now().timestamp() as u64,
        author: req.author,
        op_type: "rotate".to_string(),
        payload: serde_json::json!({
            "degrees": req.degrees
        }),
    };
    
    let mut albums = server.albums.write().await;
    if let Some(album) = albums.get_mut(&album_id) {
        let crdt = album.get_or_create(&media_id);
        match crdt.apply_local(rotate_op) {
            Ok(_) => Ok(warp::reply::json(&serde_json::json!({"success": true}))),
            Err(e) => {
                eprintln!("Failed to rotate media: {}", e);
                Ok(warp::reply::json(&serde_json::json!({"error": "Failed to rotate media"})))
            }
        }
    } else {
        Ok(warp::reply::json(&serde_json::json!({"error": "Album not found"})))
    }
}

async fn handle_rejection(err: warp::Rejection) -> Result<impl warp::Reply, std::convert::Infallible> {
    eprintln!("Request rejection: {:?}", err);
    
    let code;
    let message;
    
    if err.is_not_found() {
        code = warp::http::StatusCode::NOT_FOUND;
        message = "NOT_FOUND";
    } else if let Some(_) = err.find::<warp::filters::body::BodyDeserializeError>() {
        code = warp::http::StatusCode::BAD_REQUEST;
        message = "BAD_REQUEST";
    } else if let Some(_) = err.find::<warp::reject::MethodNotAllowed>() {
        code = warp::http::StatusCode::METHOD_NOT_ALLOWED;
        message = "METHOD_NOT_ALLOWED";
    } else {
        code = warp::http::StatusCode::INTERNAL_SERVER_ERROR;
        message = "INTERNAL_SERVER_ERROR";
    }
    
    let json = warp::reply::json(&serde_json::json!({
        "error": message,
        "code": code.as_u16()
    }));
    
    Ok(warp::reply::with_status(json, code))
}

fn generate_thumbnail(media_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    generate_resized_media(media_data, 150)
}

fn generate_resized_media(media_data: &[u8], max_size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // Try to detect the media type by checking the first few bytes
    let is_video = is_video_file(media_data);
    let is_audio = is_audio_file(media_data);
    let is_image = is_image_file(media_data);
    
    println!("Generating resized media: is_video={}, is_audio={}, is_image={}, data_len={}, max_size={}", is_video, is_audio, is_image, media_data.len(), max_size);
    
    if media_data.len() >= 12 {
        println!("First 12 bytes: {:?}", &media_data[0..12]);
        println!("First 12 bytes as hex: {}", hex::encode(&media_data[0..12]));
    }
    
    if is_video {
        println!("Generating video at size {}", max_size);
        generate_video_at_size(media_data, max_size)
    } else if is_audio {
        println!("Generating audio waveform at size {}", max_size);
        generate_audio_at_size(media_data, max_size)
    } else {
        println!("Generating image at size {}", max_size);
        generate_image_at_size(media_data, max_size)
    }
}


fn is_image_file(data: &[u8]) -> bool {
    if data.len() < 4 {
        return false;
    }
    
    // JPEG
    if data.len() >= 2 && data[0] == 0xFF && data[1] == 0xD8 {
        return true;
    }
    
    // PNG
    if data.len() >= 8 && &data[0..8] == b"\x89PNG\r\n\x1a\n" {
        return true;
    }
    
    // GIF
    if data.len() >= 6 && (&data[0..6] == b"GIF87a" || &data[0..6] == b"GIF89a") {
        return true;
    }
    
    // BMP
    if data.len() >= 2 && &data[0..2] == b"BM" {
        return true;
    }
    
    // TIFF
    if data.len() >= 4 && (&data[0..4] == b"II*\0" || &data[0..4] == b"MM\0*") {
        return true;
    }
    
    // WebP
    if data.len() >= 12 && &data[0..4] == b"RIFF" && &data[8..12] == b"WEBP" {
        return true;
    }
    
    false
}


fn generate_image_thumbnail(image_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    generate_image_at_size(image_data, 150)
}

fn generate_video_thumbnail(video_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    generate_video_at_size(video_data, 150)
}


fn extract_audio_waveform(audio_data: &[u8]) -> Result<Vec<f32>, Box<dyn std::error::Error>> {
    // Use FFmpeg to extract raw audio samples
    let temp_dir = std::env::temp_dir();
    let input_path = temp_dir.join(format!("audio_input_{}.tmp", uuid::Uuid::new_v4()));
    let output_path = temp_dir.join(format!("audio_output_{}.raw", uuid::Uuid::new_v4()));
    
    // Write audio data to temporary file
    std::fs::write(&input_path, audio_data)?;
    
    // Extract raw audio samples using FFmpeg
    let output = Command::new("ffmpeg")
        .args(&[
            "-i", input_path.to_str().unwrap(),
            "-f", "f32le",          // 32-bit float little-endian
            "-ac", "1",             // Convert to mono
            "-ar", "8000",          // Sample rate 8kHz (sufficient for waveform visualization)
            "-y",                   // Overwrite output
            output_path.to_str().unwrap()
        ])
        .output();
    
    // Clean up input file
    let _ = std::fs::remove_file(&input_path);
    
    match output {
        Ok(result) if result.status.success() => {
            // Read the raw audio samples
            let raw_data = std::fs::read(&output_path)?;
            let _ = std::fs::remove_file(&output_path);
            
            // Convert bytes to f32 samples
            let mut samples = Vec::new();
            for chunk in raw_data.chunks_exact(4) {
                let sample = f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]);
                samples.push(sample);
            }
            
            // Downsample for visualization (keep every Nth sample based on length)
            let target_samples = 2000; // Target number of samples for visualization
            if samples.len() > target_samples {
                let step = samples.len() / target_samples;
                let downsampled: Vec<f32> = samples.iter().step_by(step).cloned().collect();
                Ok(downsampled)
            } else {
                Ok(samples)
            }
        }
        _ => {
            let _ = std::fs::remove_file(&output_path);
            Err("FFmpeg audio extraction failed".into())
        }
    }
}

fn generate_audio_thumbnail(_audio_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    generate_audio_at_size(_audio_data, 150)
}

fn generate_audio_at_size(audio_data: &[u8], max_size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // Try to extract waveform data from the audio
    if let Ok(waveform_data) = extract_audio_waveform(audio_data) {
        // Generate waveform visualization based on the requested size
        if max_size <= 200 {
            // Square thumbnail for small sizes
            return generate_square_waveform(&waveform_data, max_size);
        } else {
            // Wide rectangular waveform for larger sizes
            return generate_wide_waveform(&waveform_data, max_size);
        }
    }
    
    // Fall back to placeholder if waveform extraction fails
    create_audio_placeholder_at_size_local(max_size)
}

fn create_audio_placeholder_thumbnail() -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    create_audio_placeholder_at_size_local(150)
}

fn generate_square_waveform(samples: &[f32], size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{RgbImage, Rgb};
    
    let mut img = RgbImage::new(size, size);
    let center_y = size / 2;
    
    // Dark background
    for pixel in img.pixels_mut() {
        *pixel = Rgb([15, 20, 30]);
    }
    
    // Calculate how many samples to show in the square
    let samples_to_show = (size as usize).min(samples.len());
    let sample_step = if samples.len() > samples_to_show {
        samples.len() / samples_to_show
    } else {
        1
    };
    
    // Draw circular waveform for square thumbnail
    let radius = (size / 2) as f32 * 0.8;
    let center_x = size / 2;
    
    for i in 0..samples_to_show {
        let sample_idx = i * sample_step;
        if sample_idx >= samples.len() { break; }
        
        let amplitude = samples[sample_idx].abs().min(1.0);
        let angle = (i as f32 / samples_to_show as f32) * 2.0 * std::f32::consts::PI;
        
        // Inner and outer radius based on amplitude
        let inner_radius = radius * 0.3;
        let outer_radius = inner_radius + (radius * 0.4 * amplitude);
        
        let cos_angle = angle.cos();
        let sin_angle = angle.sin();
        
        // Draw line from inner to outer radius
        let steps = ((outer_radius - inner_radius) as u32).max(1);
        for step in 0..steps {
            let r = inner_radius + (step as f32 / steps as f32) * (outer_radius - inner_radius);
            let x = (center_x as f32 + r * cos_angle) as u32;
            let y = (center_y as f32 + r * sin_angle) as u32;
            
            if x < size && y < size {
                let intensity = (255.0 * (1.0 - step as f32 / steps as f32)) as u8;
                img.put_pixel(x, y, Rgb([intensity / 4, intensity / 2, intensity]));
            }
        }
    }
    
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    Ok(buffer)
}

fn generate_wide_waveform(samples: &[f32], max_size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{RgbImage, Rgb};
    
    // Create a wide aspect ratio (3:1 or 4:1 depending on size)
    let width = max_size;
    let height = max_size / 3;
    
    let mut img = RgbImage::new(width, height);
    let center_y = height / 2;
    
    // Dark background
    for pixel in img.pixels_mut() {
        *pixel = Rgb([15, 20, 30]);
    }
    
    // Draw traditional horizontal waveform
    let samples_per_pixel = (samples.len() as f32 / width as f32).max(1.0);
    
    for x in 0..width {
        let sample_start = (x as f32 * samples_per_pixel) as usize;
        let sample_end = ((x + 1) as f32 * samples_per_pixel) as usize;
        
        // Find min and max amplitude in this pixel's range
        let mut min_amp = 0.0f32;
        let mut max_amp = 0.0f32;
        let mut rms = 0.0f32;
        let mut count = 0;
        
        for i in sample_start..sample_end.min(samples.len()) {
            let sample = samples[i];
            min_amp = min_amp.min(sample);
            max_amp = max_amp.max(sample);
            rms += sample * sample;
            count += 1;
        }
        
        if count > 0 {
            rms = (rms / count as f32).sqrt();
            
            // Scale amplitudes to pixel coordinates
            let max_height = (height / 2) as f32 * 0.9;
            let min_y = (center_y as f32 - max_amp * max_height) as u32;
            let max_y = (center_y as f32 - min_amp * max_height) as u32;
            let rms_y1 = (center_y as f32 - rms * max_height) as u32;
            let rms_y2 = (center_y as f32 + rms * max_height) as u32;
            
            // Draw the waveform line
            for y in min_y..=max_y.min(height - 1) {
                if y < height {
                    // Brighter color for the main waveform
                    img.put_pixel(x, y, Rgb([100, 150, 255]));
                }
            }
            
            // Draw RMS envelope with different color
            if rms_y1 < height {
                img.put_pixel(x, rms_y1, Rgb([255, 200, 100]));
            }
            if rms_y2 < height {
                img.put_pixel(x, rms_y2, Rgb([255, 200, 100]));
            }
        }
    }
    
    // Add subtle grid lines
    let grid_spacing = width / 10;
    for i in 1..10 {
        let grid_x = i * grid_spacing;
        if grid_x < width {
            for y in 0..height {
                if y % 4 == 0 {
                    img.put_pixel(grid_x, y, Rgb([30, 35, 45]));
                }
            }
        }
    }
    
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    Ok(buffer)
}

fn create_audio_placeholder_at_size_local(size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{RgbImage, Rgb};
    
    let mut img = RgbImage::new(size, size);
    let _center_x = size / 2;
    let center_y = size / 2;
    
    // Create a dark background with audio waveform-like visualization
    for (x, y, pixel) in img.enumerate_pixels_mut() {
        // Create a dark audio-like background
        *pixel = Rgb([20, 25, 35]); // Dark blue-gray background
        
        // Draw simplified waveform bars scaled to size
        let bar_width = (size / 40).max(2);
        let bar_spacing = (size / 25).max(3);
        let num_bars = size / bar_spacing;
        
        for i in 0..num_bars {
            let bar_x = i * bar_spacing + size / 15;
            let bar_height = (size / 8) + ((i * 7) % (size / 4)); // Varying heights for waveform effect
            let bar_top = center_y - bar_height / 2;
            let bar_bottom = center_y + bar_height / 2;
            
            if x >= bar_x && x < bar_x + bar_width && y >= bar_top && y <= bar_bottom {
                // Create gradient effect for bars
                let intensity = 255 - ((y as i32 - bar_top as i32).abs() * 100 / bar_height as i32).min(100) as u8;
                *pixel = Rgb([intensity / 3, intensity / 2, intensity]); // Blue-ish gradient
            }
        }
        
        // Add a subtle border
        let border_width = (size / 75).max(1);
        if x < border_width || x >= size - border_width || y < border_width || y >= size - border_width {
            *pixel = Rgb([60, 70, 90]); // Lighter border
        }
    }
    
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    Ok(buffer)
}

fn create_video_placeholder_thumbnail() -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    create_video_placeholder_at_size(150)
}

fn create_video_placeholder_at_size(size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{RgbImage, Rgb};
    
    let mut img = RgbImage::new(size, size);
    let center_x = size / 2;
    let center_y = size / 2;
    
    // Create a dark background with a prominent play button
    for (x, y, pixel) in img.enumerate_pixels_mut() {
        // Create a dark video-like background
        *pixel = Rgb([30, 30, 30]);
        
        // Draw a large white play triangle scaled to size
        let triangle_size = size / 5;
        
        // Draw a proper right-pointing triangle (play button)
        let triangle_left = center_x - triangle_size / 3;
        let triangle_right = center_x + triangle_size / 3;
        
        // Check if we're in the triangle area
        if x >= triangle_left && x <= triangle_right {
            let relative_x = x as i32 - triangle_left as i32;
            let triangle_width = (triangle_right - triangle_left) as i32;
            
            // Calculate the triangle bounds at this x position (right-pointing)
            let half_height_at_x = (relative_x * triangle_size as i32) / (triangle_width * 2);
            let top_bound = center_y as i32 - half_height_at_x;
            let bottom_bound = center_y as i32 + half_height_at_x;
            
            if y as i32 >= top_bound && y as i32 <= bottom_bound {
                *pixel = Rgb([255, 255, 255]); // White play button
            }
        }
        
        // Add a subtle border to make it look more like a video thumbnail
        let border_width = (size / 50).max(1);
        if x < border_width || x >= size - border_width || y < border_width || y >= size - border_width {
            *pixel = Rgb([100, 100, 100]); // Gray border
        }
    }
    
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    Ok(buffer)
}

async fn save_album_update(server: Arc<WebAlbumServer>, _album_id: String) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    // This is a simplified version that saves all albums
    // In a production system, you'd want to save only the changed album
    let albums = server.albums.read().await;
    
    // Create an index mapping album IDs to their block IDs
    let mut album_index = HashMap::new();
    
    // Save each album to its own block
    for (id, album) in albums.iter() {
        // Create a serializable version without the block manager
        let mut serializable_album = album.clone();
        serializable_album.block_manager = None;
        
        let album_json = serde_json::to_string_pretty(&serializable_album)?;
        let album_data = album_json.as_bytes();
        
        // Store the album in a block
        let album_block_id = server.block_manager.write_direct_block(album_data).await?;
        album_index.insert(id.clone(), album_block_id);
    }
    
    // Save the index to a block
    let index_json = serde_json::to_string_pretty(&album_index)?;
    let index_data = index_json.as_bytes();
    let index_block_id = server.block_manager.write_direct_block(index_data).await?;
    
    // Store the index block ID in a simple file for bootstrapping
    let index_file = server.data_dir.join("albums_index.txt");
    std::fs::write(&index_file, hex::encode(index_block_id))?;
    
    Ok(())
}

fn create_placeholder_thumbnail() -> Vec<u8> {
    // Create a simple 150x150 placeholder image with a camera icon pattern
    use image::{RgbImage, Rgb};
    
    let mut img = RgbImage::new(150, 150);
    for (x, y, pixel) in img.enumerate_pixels_mut() {
        // Create a simple camera icon pattern
        let center_x = 75;
        let center_y = 75;
        let distance = ((x as i32 - center_x).pow(2) + (y as i32 - center_y).pow(2)) as f32;
        
        if distance < 30.0 * 30.0 {
            *pixel = Rgb([100, 100, 100]); // Dark gray circle
        } else if distance < 50.0 * 50.0 {
            *pixel = Rgb([200, 200, 200]); // Light gray ring
        } else {
            *pixel = Rgb([240, 240, 240]); // Very light gray background
        }
    }
    
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png).unwrap();
    buffer
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut server = WebAlbumServer::new()?;
    server.initialize().await?;
    let server = Arc::new(server);
    server.start_server(3030).await?;
    Ok(())
}


// =====================================================================
// FILE: packages/soradyne_core/examples/block_storage_example.rs
// =====================================================================

use std::path::PathBuf;
use std::sync::Arc;
use tokio;

use soradyne::storage::block_manager::BlockManager;
use soradyne::types::media::{PhotoStorage, VideoStorage};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("Setting up Soradyne Block Storage Example");
    
    // 1. Create test .rimsd directories
    let rimsd_dirs = setup_test_rimsd_directories()?;
    println!("Created {} test .rimsd directories", rimsd_dirs.len());
    
    // 2. Set up metadata directory
    let metadata_dir = std::env::temp_dir().join("soradyne_metadata");
    std::fs::create_dir_all(&metadata_dir)?;
    let metadata_path = metadata_dir.join("block_metadata.json");
    
    // 3. Create BlockManager with erasure coding configuration
    let threshold = 2; // Need at least 2 shards to reconstruct
    let total_shards = 3; // Create 3 shards total
    
    let block_manager = Arc::new(BlockManager::new(
        rimsd_dirs.clone(),
        metadata_path,
        threshold,
        total_shards,
    )?);
    
    println!("BlockManager created with threshold={}, total_shards={}", threshold, total_shards);
    
    // 4. Test basic block operations
    test_basic_block_operations(&block_manager).await?;
    
    // 5. Test photo storage
    test_photo_storage(&block_manager).await?;
    
    // 6. Test video storage  
    test_video_storage(&block_manager).await?;
    
    println!("All tests completed successfully!");
    println!("Test directories created at:");
    for dir in &rimsd_dirs {
        println!("  {}", dir.display());
    }
    
    Ok(())
}

/// Set up test .rimsd directories in the system temp directory
fn setup_test_rimsd_directories() -> Result<Vec<PathBuf>, Box<dyn std::error::Error>> {
    let base_temp = std::env::temp_dir();
    let mut rimsd_dirs = Vec::new();
    
    // Create 3 test directories with hidden .rimsd subdirectories
    for i in 0..3 {
        let test_dir = base_temp.join(format!("soradyne_test_{}", i));
        let rimsd_dir = test_dir.join(".rimsd");
        
        // Remove existing directory if it exists
        if test_dir.exists() {
            std::fs::remove_dir_all(&test_dir)?;
        }
        
        // Create the test directory and hidden .rimsd subdirectory
        std::fs::create_dir_all(&rimsd_dir)?;
        rimsd_dirs.push(rimsd_dir);
    }
    
    Ok(rimsd_dirs)
}

/// Test basic block read/write operations
async fn test_basic_block_operations(block_manager: &Arc<BlockManager>) -> Result<(), Box<dyn std::error::Error>> {
    println!("\n--- Testing Basic Block Operations ---");
    
    // Test writing a small block
    let test_data = b"Hello, Soradyne Block Storage!";
    println!("Writing test data: {:?}", std::str::from_utf8(test_data)?);
    
    let block_id = block_manager.write_direct_block(test_data).await?;
    println!("Block written with ID: {}", hex::encode(block_id));
    
    // Test reading the block back
    let read_data = block_manager.read_block(&block_id).await?;
    println!("Read back data: {:?}", std::str::from_utf8(&read_data)?);
    
    // Verify data integrity
    assert_eq!(test_data, read_data.as_slice());
    println!("✓ Data integrity verified");
    
    // Test writing a larger block
    let large_data = vec![42u8; 1024 * 1024]; // 1MB of data
    println!("Writing 1MB test block...");
    
    let large_block_id = block_manager.write_direct_block(&large_data).await?;
    println!("Large block written with ID: {}", hex::encode(large_block_id));
    
    let read_large_data = block_manager.read_block(&large_block_id).await?;
    assert_eq!(large_data, read_large_data);
    println!("✓ Large block integrity verified");
    
    Ok(())
}

/// Test photo storage functionality
async fn test_photo_storage(block_manager: &Arc<BlockManager>) -> Result<(), Box<dyn std::error::Error>> {
    println!("\n--- Testing Photo Storage ---");
    
    let photo_storage = PhotoStorage::new(block_manager.clone());
    
    // Create some mock photo data (in reality this would be JPEG/PNG data)
    let mock_photo_data = create_mock_image_data("JPEG", 1920, 1080);
    println!("Created mock photo data: {} bytes", mock_photo_data.len());
    
    // Store the photo
    let photo_metadata = photo_storage.save_photo("test_photo.jpg", "image/jpeg", &mock_photo_data).await?;
    println!("Photo stored with ID: {}", photo_metadata.id);
    
    // Retrieve the photo
    let retrieved_photo = photo_storage.load_photo(&photo_metadata).await?;
    assert_eq!(mock_photo_data, retrieved_photo);
    println!("✓ Photo storage and retrieval verified");
    
    Ok(())
}

/// Test video storage functionality
async fn test_video_storage(block_manager: &Arc<BlockManager>) -> Result<(), Box<dyn std::error::Error>> {
    println!("\n--- Testing Video Storage ---");
    
    let video_storage = VideoStorage::new(block_manager.clone());
    
    // Create some mock video data (in reality this would be MP4/AVI data)
    let mock_video_data = create_mock_video_data(1920, 1080, 30); // 30 seconds at 1080p
    println!("Created mock video data: {} bytes", mock_video_data.len());
    
    // Store the video
    let video_metadata = video_storage.save_video("test_video.mp4", "video/mp4", &mock_video_data).await?;
    println!("Video stored with ID: {}", video_metadata.id);
    
    // Retrieve the video
    let retrieved_video = video_storage.load_video(&video_metadata).await?;
    assert_eq!(mock_video_data, retrieved_video);
    println!("✓ Video storage and retrieval verified");
    
    Ok(())
}

/// Create mock image data for testing
fn create_mock_image_data(format: &str, width: u32, height: u32) -> Vec<u8> {
    let mut data = Vec::new();
    
    // Mock JPEG header
    data.extend_from_slice(b"\xFF\xD8\xFF\xE0");
    
    // Add some metadata
    data.extend_from_slice(format!("{}x{} {}", width, height, format).as_bytes());
    
    // Add some mock image data
    for i in 0..(width * height / 100) {
        data.push((i % 256) as u8);
    }
    
    // Mock JPEG footer
    data.extend_from_slice(b"\xFF\xD9");
    
    data
}

/// Create mock video data for testing
fn create_mock_video_data(width: u32, height: u32, duration_seconds: u32) -> Vec<u8> {
    let mut data = Vec::new();
    
    // Mock MP4 header
    data.extend_from_slice(b"ftypisom");
    
    // Add some metadata
    data.extend_from_slice(format!("{}x{}@{}s", width, height, duration_seconds).as_bytes());
    
    // Add some mock video frames (much larger than photo)
    let frame_size = (width * height * 3) / 1000; // Simplified frame size
    for frame in 0..(duration_seconds * 30) { // 30 FPS
        for i in 0..frame_size {
            data.push(((frame + i) % 256) as u8);
        }
    }
    
    data
}


// =====================================================================
// FILE: packages/soradyne_core/examples/robot_joints.rs
// =====================================================================

//! Robot Joints Example
//!
//! This example demonstrates using the Diffable trait for efficient updates
//! to robot joint positions.

use uuid::Uuid;
use soradyne::flow::examples::robot_joints::{
    RobotJointDiff, JointDiff, create_robot_joint_flow
};

fn main() {
    println!("Starting Robot Joints Demo");
    
    let owner_id = Uuid::new_v4();
    let flow = create_robot_joint_flow("robot-1", owner_id);
    
    // Subscribe to updates
    let _subscription_id = flow.subscribe(Box::new(|state| {
        println!("Robot state updated:");
        println!("  Robot ID: {}", state.robot_id);
        println!("  Last update: {}", state.last_update);
        println!("  Joints:");
        for (name, joint) in &state.joints {
            println!("    {}: angle={:.2}, velocity={:.2}", 
                     name, joint.angle, joint.velocity);
        }
        println!();
    }));
    
    // Update using full state
    if let Some(current_state) = flow.get_value() {
        let mut new_state = current_state.clone();
        
        // Update shoulder joint
        if let Some(joint) = new_state.joints.get_mut("shoulder") {
            joint.angle = 0.5;
            joint.velocity = 0.1;
            joint.timestamp = 1;
        }
        
        new_state.last_update = 1;
        flow.update(new_state);
    }
    
    // Update using diff (more efficient)
    let diff = RobotJointDiff {
        changes: vec![
            JointDiff {
                name: "elbow".to_string(),
                angle: Some(0.75),
                velocity: Some(0.2),
                timestamp: 2,
            },
            JointDiff {
                name: "wrist".to_string(),
                angle: Some(0.3),
                velocity: None, // Don't change velocity
                timestamp: 2,
            },
        ],
        timestamp: 2,
    };
    
    flow.update_with_diff(&diff);
    
    // Create a new joint with diff
    let new_joint_diff = RobotJointDiff {
        changes: vec![
            JointDiff {
                name: "gripper".to_string(),
                angle: Some(0.0),
                velocity: Some(0.0),
                timestamp: 3,
            },
        ],
        timestamp: 3,
    };
    
    flow.update_with_diff(&new_joint_diff);
    
    // Update multiple joints with a single diff
    let multi_diff = RobotJointDiff {
        changes: vec![
            JointDiff {
                name: "shoulder".to_string(),
                angle: Some(0.6),
                velocity: None,
                timestamp: 4,
            },
            JointDiff {
                name: "elbow".to_string(),
                angle: Some(0.8),
                velocity: Some(0.0),
                timestamp: 4,
            },
        ],
        timestamp: 4,
    };
    
    flow.update_with_diff(&multi_diff);
    
    println!("Robot joint demo completed!");
}


// =====================================================================
// FILE: packages/soradyne_core/examples/block_storage_demo.rs
// =====================================================================

use std::path::PathBuf;
use std::io::{self, Write};
use tokio;
use soradyne::storage::{BlockManager, discover_soradyne_volumes};
use soradyne::storage::device_identity::fingerprint_device;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("🧱 Soradyne Block Storage Demo");
    println!("==============================");
    
    // Discover Soradyne devices
    println!("🔍 Discovering Soradyne storage devices...");
    let rimsd_dirs = discover_soradyne_volumes().await?;
    
    if rimsd_dirs.is_empty() {
        println!("❌ No Soradyne devices found!");
        println!("   Please initialize some SD cards with .rimsd directories first.");
        println!("   You can create test directories with:");
        println!("   mkdir -p /tmp/test_sd1/.rimsd");
        println!("   echo 'test-device-1' > /tmp/test_sd1/.rimsd/soradyne_device_id.txt");
        return Ok(());
    }
    
    println!("✅ Found {} Soradyne devices:", rimsd_dirs.len());
    for (i, dir) in rimsd_dirs.iter().enumerate() {
        println!("   {}. {}", i + 1, dir.display());
    }
    
    // Set up erasure coding parameters
    let threshold = std::cmp::min(2, rimsd_dirs.len());
    let total_shards = rimsd_dirs.len();
    
    println!("\n📊 Storage Configuration:");
    println!("   Devices: {}", total_shards);
    println!("   Threshold: {} (minimum devices needed for recovery)", threshold);
    println!("   Redundancy: {}", total_shards - threshold);
    
    // Create BlockManager
    let metadata_path = PathBuf::from("/tmp/soradyne_demo_metadata.json");
    let block_manager = BlockManager::new(rimsd_dirs, metadata_path, threshold, total_shards)?;
    
    // Interactive demo loop
    loop {
        println!("\n🎮 Demo Commands:");
        println!("   init      - Initialize .rimsd directory on SD card");
        println!("   w <text>  - Write text as a block");
        println!("   r <id>    - Read block by ID (first 8 chars)");
        println!("   l         - List all blocks");
        println!("   d <id>    - Show block distribution");
        println!("   t <id>    - Test erasure recovery");
        println!("   s         - Show storage info");
        println!("   q         - Quit");
        print!("\n> ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let parts: Vec<&str> = input.trim().split_whitespace().collect();
        
        if parts.is_empty() {
            continue;
        }
        
        match parts[0] {
            "init" => {
                println!("\n🔧 Initialize Soradyne SD Card");
                println!("===============================");
                println!("This will create a .rimsd directory and generate device fingerprints.");
                println!("Enter the mount path of your SD card:");
                println!("Examples:");
                println!("  macOS:   /Volumes/SDCARD");
                println!("  Linux:   /media/username/SDCARD or /mnt/sdcard");
                println!("  Windows: D:\\ or E:\\");
                print!("\nSD card path: ");
                io::stdout().flush()?;
                
                let mut path_input = String::new();
                io::stdin().read_line(&mut path_input)?;
                let sd_path = std::path::Path::new(path_input.trim());
                
                if !sd_path.exists() {
                    println!("❌ Path does not exist: {}", sd_path.display());
                    continue;
                }
                
                let rimsd_path = sd_path.join(".rimsd");
                
                println!("\n🔍 Initializing Soradyne storage at: {}", rimsd_path.display());
                
                // Create .rimsd directory if it doesn't exist
                if let Err(e) = tokio::fs::create_dir_all(&rimsd_path).await {
                    println!("❌ Failed to create .rimsd directory: {}", e);
                    continue;
                }
                
                // Generate device fingerprint (this will create soradyne_device_id.txt automatically)
                match fingerprint_device(&rimsd_path).await {
                    Ok(fingerprint) => {
                        println!("✅ Successfully initialized Soradyne storage!");
                        println!("\n📊 Device Fingerprint:");
                        println!("   Soradyne Device ID: {:?}", fingerprint.soradyne_device_id);
                        println!("   Hardware ID: {:?}", fingerprint.hardware_id);
                        println!("   Filesystem UUID: {:?}", fingerprint.filesystem_uuid);
                        println!("   Capacity: {:.2} GB", fingerprint.capacity_bytes as f64 / (1024.0 * 1024.0 * 1024.0));
                        println!("   Bad block signature: 0x{:016x}", fingerprint.bad_block_signature);
                        
                        println!("\n💾 Files created:");
                        println!("   {}/soradyne_device_id.txt", rimsd_path.display());
                        
                        // Verify the device ID file was created
                        let device_id_file = rimsd_path.join("soradyne_device_id.txt");
                        if device_id_file.exists() {
                            if let Ok(content) = tokio::fs::read_to_string(&device_id_file).await {
                                println!("   Device ID: {}", content.trim());
                            }
                        }
                        
                        println!("\n🎉 SD card is now ready for Soradyne storage!");
                        println!("   You can now restart this demo to use the initialized card.");
                    }
                    Err(e) => {
                        println!("❌ Failed to generate device fingerprint: {}", e);
                        println!("   The .rimsd directory was created but fingerprinting failed.");
                        println!("   You may need to run this on the actual SD card device.");
                    }
                }
            }
            
            "w" => {
                if parts.len() < 2 {
                    println!("❌ Usage: w <text>");
                    continue;
                }
                
                let text = parts[1..].join(" ");
                let data = text.as_bytes();
                
                println!("📝 Writing block: \"{}\" ({} bytes)", text, data.len());
                
                match block_manager.write_direct_block(data).await {
                    Ok(block_id) => {
                        let id_str = hex::encode(&block_id[..4]);
                        println!("\n✅ Block written successfully!");
                        println!("   Block ID: {} (use '{}' for commands)", hex::encode(block_id), id_str);
                    }
                    Err(e) => println!("❌ Failed to write block: {}", e),
                }
            }
            
            "r" => {
                if parts.len() < 2 {
                    println!("❌ Usage: r <block_id>");
                    continue;
                }
                
                let id_prefix = parts[1];
                if let Some(block_id) = find_block_by_prefix(&block_manager, id_prefix).await {
                    println!("\n📖 Reading block {}...", hex::encode(&block_id[..4]));
                    
                    match block_manager.read_block(&block_id).await {
                        Ok(data) => {
                            let text = String::from_utf8_lossy(&data);
                            println!("\n✅ Block content: \"{}\"", text);
                            println!("   Size: {} bytes", data.len());
                        }
                        Err(e) => println!("❌ Failed to read block: {}", e),
                    }
                } else {
                    println!("❌ Block not found with prefix: {}", id_prefix);
                }
            }
            
            "l" => {
                println!("📋 Listing all blocks...");
                let blocks = block_manager.list_blocks().await;
                
                if blocks.is_empty() {
                    println!("   📭 No blocks found");
                } else {
                    println!("   Found {} blocks:", blocks.len());
                    for (block_id, metadata) in blocks.iter() {
                        let id_str = hex::encode(&block_id[..4]);
                        println!("     {} - {} bytes - {}", 
                            id_str,
                            metadata.size,
                            metadata.created_at.format("%Y-%m-%d %H:%M:%S UTC"));
                    }
                    println!("\n   💡 Use block ID (first 8 chars) with 'r', 'd', or 't' commands");
                }
            }
            
            "d" => {
                if parts.len() < 2 {
                    println!("❌ Usage: d <block_id>");
                    continue;
                }
                
                let id_prefix = parts[1];
                if let Some(block_id) = find_block_by_prefix(&block_manager, id_prefix).await {
                    match block_manager.get_block_distribution(&block_id).await {
                        Ok(distribution) => {
                            println!("📊 Block Distribution for {}:", hex::encode(&block_id[..4]));
                            println!("   Original size: {} bytes", distribution.original_size);
                            println!("   Total shards: {}", distribution.total_shards);
                            println!("   Available shards: {}", distribution.available_shards.len());
                            println!("   Missing shards: {}", distribution.missing_shards.len());
                            println!("   Can reconstruct: {}", if distribution.can_reconstruct { "✅ Yes" } else { "❌ No" });
                            
                            println!("\n   Shard locations:");
                            for shard in &distribution.available_shards {
                                println!("     Shard {} → {} ({} bytes)", 
                                    shard.index, shard.device_path, shard.size);
                            }
                            
                            if !distribution.missing_shards.is_empty() {
                                println!("\n   Missing shards: {:?}", distribution.missing_shards);
                            }
                        }
                        Err(e) => println!("❌ Failed to get distribution: {}", e),
                    }
                } else {
                    println!("❌ Block not found with prefix: {}", id_prefix);
                }
            }
            
            "t" => {
                if parts.len() < 2 {
                    println!("❌ Usage: t <block_id>");
                    continue;
                }
                
                let id_prefix = parts[1];
                if let Some(block_id) = find_block_by_prefix(&block_manager, id_prefix).await {
                    // Simulate missing some shards
                    let shards_to_remove = vec![0, 1]; // Remove first two shards
                    
                    println!("🧪 Testing erasure recovery for {}...", hex::encode(&block_id[..4]));
                    println!("   Simulating missing shards: {:?}", shards_to_remove);
                    
                    match block_manager.demonstrate_erasure_recovery(&block_id, shards_to_remove).await {
                        Ok(result) => {
                            println!("📊 Recovery Test Results:");
                            println!("   Original shards: {}", result.original_shards);
                            println!("   Simulated missing: {:?}", result.simulated_missing);
                            println!("   Available shards: {}", result.available_shards);
                            println!("   Threshold required: {}", result.threshold_required);
                            println!("   Recovery successful: {}", if result.recovery_successful { "✅ Yes" } else { "❌ No" });
                            
                            if result.recovery_successful {
                                println!("   Recovered data size: {} bytes", result.recovered_data_size);
                                println!("   🎉 Data can be recovered even with {} missing shards!", result.simulated_missing.len());
                            } else {
                                println!("   ⚠️  Not enough shards available for recovery");
                                println!("   Need at least {} shards, but only {} available", 
                                    result.threshold_required, result.available_shards);
                            }
                        }
                        Err(e) => println!("❌ Failed to test recovery: {}", e),
                    }
                } else {
                    println!("❌ Block not found with prefix: {}", id_prefix);
                }
            }
            
            "s" => {
                let info = block_manager.get_storage_info();
                println!("📊 Storage Information:");
                println!("   Total devices: {}", info.total_devices);
                println!("   Erasure threshold: {}", info.threshold);
                println!("   Total shards per block: {}", info.total_shards);
                println!("   Redundancy level: {}", info.total_shards - info.threshold);
                
                println!("\n   Device paths:");
                for (i, path) in info.rimsd_paths.iter().enumerate() {
                    println!("     {}. {}", i + 1, path.display());
                }
            }
            
            "q" => {
                println!("👋 Goodbye!");
                break;
            }
            
            _ => {
                println!("❌ Unknown command: {}", parts[0]);
            }
        }
    }
    
    Ok(())
}

async fn find_block_by_prefix(block_manager: &BlockManager, prefix: &str) -> Option<[u8; 32]> {
    // Get all blocks and find one that starts with the given prefix
    let blocks = block_manager.list_blocks().await;
    
    for (block_id, _metadata) in blocks {
        let block_hex = hex::encode(block_id);
        if block_hex.starts_with(prefix) {
            return Some(block_id);
        }
    }
    
    None
}


// =====================================================================
// FILE: packages/soradyne_core/examples/heartrate_demo.rs
// =====================================================================

//! examples/heartrate_demo.rs
//!
//! A demo app that streams heartrate data, subscribes to updates, persists
//! data to disk, and syncs heartrate across devices using NetworkBridge.

use soradyne::types::heartrate::{Heartrate, HeartrateFlow};
use soradyne::network::connection::NetworkBridge;
use soradyne::network::NoOpDiscovery;
use soradyne::flow::FlowType;
use soradyne::storage::LocalFileStorage;
use uuid::Uuid;
use std::sync::Arc;
use std::time::Duration;
use tokio::time::sleep;

#[tokio::main]
async fn main() {
    let device_id = Uuid::new_v4();
    
    // Create a local file storage backend
    let storage = LocalFileStorage::new("./data").expect("Failed to create storage directory");
    
    let flow = Arc::new(HeartrateFlow::new(
        "heartrate_demo",
        device_id,
        Heartrate::new(70.0, device_id),
        FlowType::RealTimeScalar,
    ).with_storage(storage));

    // Create a network bridge with discovery
    let bridge = Arc::new(NetworkBridge::new()
        .with_discovery(NoOpDiscovery));
    
    // Start peer discovery (this is a no-op in this example)
    if let Err(e) = bridge.start_discovery() {
        eprintln!("Failed to start discovery: {}", e);
    }

    // Start listening for incoming peer connections (adjust port as needed)
    let flow_clone = flow.clone();
    let bridge_clone = bridge.clone();
    tokio::spawn(async move {
        bridge_clone
            .listen("0.0.0.0:5001", flow_clone)
            .await
            .unwrap();
    });

    // Optionally connect to a peer (uncomment and set the correct address)
    let bridge_clone2 = bridge.clone();
    tokio::spawn(async move {
        match bridge_clone2.connect("10.222.157.135:5001").await {
            Ok(_) => println!("[NetworkBridge] Successfully connected to peer on port 5001"),
            Err(e) => println!("[NetworkBridge] Failed to connect to peer on port 5001: {}", e),
        }
    });

    // Subscribe to updates
    flow.subscribe(Box::new(|value| {
        println!(
            "[Subscriber] New heartrate: {:.1} bpm from {} at {}",
            value.bpm, value.source_device_id, value.timestamp
        );
    }));

    // Simulate incoming heartrate updates
    for bpm in [72.5, 74.0, 75.3, 73.8] {
        let new_reading = Heartrate::new(bpm, device_id);
        println!(
            "[Local] Updating heartrate: {:.1} bpm at {}",
            new_reading.bpm, new_reading.timestamp
        );
        flow.update(new_reading.clone());

        // Persist after each update
        flow.persist().unwrap_or_else(|e| eprintln!("Failed to persist: {}", e));

        // Broadcast to peers
        bridge.broadcast(&new_reading);

        sleep(Duration::from_secs(2)).await;
    }

    // Simulate merging a remote value
    let remote_device = Uuid::new_v4();
    let remote_reading = Heartrate::new(76.4, remote_device);
    println!(
        "[Merge] Merging remote heartrate: {:.1} bpm",
        remote_reading.bpm
    );
    flow.merge(remote_reading);

    // Persist final state
    flow.persist().unwrap_or_else(|e| eprintln!("Failed to persist: {}", e));
}



// =====================================================================
// FILE: packages/soradyne_core/examples/large_video_test.rs
// =====================================================================

//! Large Video Block Storage Test
//! 
//! This example creates large video files that exceed the 32MB direct block limit
//! to test the indirect block storage functionality. It generates mock video data
//! of various sizes and verifies that the block storage system can handle files
//! larger than a single block.

use std::path::PathBuf;
use std::sync::Arc;
use tokio;
use sha2::{Sha256, Digest};

use soradyne::storage::block_manager::BlockManager;
use soradyne::types::media::VideoStorage;

/// Test video configurations - sizes designed to exceed 32MB limit
const TEST_VIDEOS: &[(&str, usize, &str)] = &[
    // Small video (under 32MB limit)
    ("small_video.mp4", 16 * 1024 * 1024, "16MB test video - should use direct block"),
    
    // Videos that exceed 32MB limit - should use indirect blocks
    ("medium_video.mp4", 48 * 1024 * 1024, "48MB test video - should use indirect blocks"),
    ("large_video.mp4", 64 * 1024 * 1024, "64MB test video - should use indirect blocks"),
    ("huge_video.mp4", 128 * 1024 * 1024, "128MB test video - should use multiple indirect blocks"),
    ("massive_video.mp4", 256 * 1024 * 1024, "256MB test video - stress test for block system"),
];

/// Block size limit from the storage system
const BLOCK_SIZE: usize = 32 * 1024 * 1024; // 32MB

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("🎬 Large Video Block Storage Test");
    println!("=================================");
    println!("This test creates large video files that exceed the 32MB direct block limit");
    println!("to verify that the indirect block storage system works correctly.\n");
    
    let mut test_session = LargeVideoTestSession::new().await?;
    
    // Run the large video test
    test_session.run_large_video_test().await?;
    
    // Show results
    test_session.show_results();
    
    Ok(())
}

struct LargeVideoTestSession {
    block_manager: Arc<BlockManager>,
    video_storage: VideoStorage,
    test_dir: PathBuf,
    test_results: Vec<VideoTestResult>,
}

#[derive(Debug)]
struct VideoTestResult {
    name: String,
    description: String,
    original_size: usize,
    retrieved_size: usize,
    hash_match: bool,
    storage_time: std::time::Duration,
    retrieval_time: std::time::Duration,
    uses_indirect_blocks: bool,
    success: bool,
}

impl LargeVideoTestSession {
    async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        // Create test directory structure
        let test_dir = std::env::current_dir()?.join("large_video_test");
        
        // Clean and recreate directories
        if test_dir.exists() {
            std::fs::remove_dir_all(&test_dir)?;
        }
        std::fs::create_dir_all(&test_dir)?;
        
        // Create rimsd directories (hidden .rimsd subdirectories)
        let mut rimsd_dirs = Vec::new();
        for i in 0..5 {
            let device_dir = test_dir.join(format!("rimsd_{}", i));
            let rimsd_dir = device_dir.join(".rimsd");
            std::fs::create_dir_all(&rimsd_dir)?;
            rimsd_dirs.push(rimsd_dir);
        }
        
        // Set up block manager with good redundancy for large files
        let metadata_path = test_dir.join("metadata.json");
        let threshold = 3; // Need 3 out of 5 shards
        let total_shards = 5;
        
        let block_manager = Arc::new(BlockManager::new(
            rimsd_dirs,
            metadata_path,
            threshold,
            total_shards,
        )?);
        
        let video_storage = VideoStorage::new(block_manager.clone());
        
        println!("✅ Test environment created at: {:?}", test_dir);
        println!("📊 Block size limit: {} MB", BLOCK_SIZE / (1024 * 1024));
        println!("🔧 Erasure coding: {}/{} shards\n", threshold, total_shards);
        
        Ok(Self {
            block_manager,
            video_storage,
            test_dir,
            test_results: Vec::new(),
        })
    }
    
    async fn run_large_video_test(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("🚀 Starting large video test with {} videos...\n", TEST_VIDEOS.len());
        
        for (i, (name, size, description)) in TEST_VIDEOS.iter().enumerate() {
            println!("🎥 Test {}/{}: {}", i + 1, TEST_VIDEOS.len(), name);
            println!("   Description: {}", description);
            println!("   Size: {} MB ({} bytes)", size / (1024 * 1024), size);
            
            let uses_indirect = *size > BLOCK_SIZE;
            println!("   Expected storage type: {}", 
                    if uses_indirect { "Indirect blocks" } else { "Direct block" });
            
            match self.test_single_video(name, *size, description, uses_indirect).await {
                Ok(result) => {
                    if result.success {
                        println!("   ✅ SUCCESS - Hash match: {}, Storage: {:.2?}", 
                                result.hash_match, result.storage_time);
                    } else {
                        println!("   ❌ FAILED - Check logs for details");
                    }
                    self.test_results.push(result);
                }
                Err(e) => {
                    println!("   ❌ ERROR: {}", e);
                    // Create a failed result entry
                    self.test_results.push(VideoTestResult {
                        name: name.to_string(),
                        description: description.to_string(),
                        original_size: *size,
                        retrieved_size: 0,
                        hash_match: false,
                        storage_time: std::time::Duration::from_secs(0),
                        retrieval_time: std::time::Duration::from_secs(0),
                        uses_indirect_blocks: uses_indirect,
                        success: false,
                    });
                }
            }
            println!();
        }
        
        Ok(())
    }
    
    async fn test_single_video(&self, name: &str, size: usize, description: &str, uses_indirect: bool) 
        -> Result<VideoTestResult, Box<dyn std::error::Error>> {
        
        // Generate mock video data
        println!("   🎬 Generating {} MB of mock video data...", size / (1024 * 1024));
        let video_data = generate_mock_video_data(size);
        
        if video_data.len() != size {
            return Err(format!("Generated data size mismatch: expected {}, got {}", 
                              size, video_data.len()).into());
        }
        
        // Calculate hash for integrity verification
        let original_hash = calculate_hash(&video_data);
        
        // Store using block storage
        println!("   💾 Storing in block storage...");
        let storage_start = std::time::Instant::now();
        let metadata = self.video_storage.save_video(name, "video/mp4", &video_data).await?;
        let storage_time = storage_start.elapsed();
        
        println!("   🔑 Video ID: {}", metadata.id);
        
        // Retrieve from block storage
        println!("   📤 Retrieving from block storage...");
        let retrieval_start = std::time::Instant::now();
        let retrieved_data = self.video_storage.load_video(&metadata).await?;
        let retrieval_time = retrieval_start.elapsed();
        
        // Verify integrity
        let retrieved_hash = calculate_hash(&retrieved_data);
        let hash_match = original_hash == retrieved_hash;
        let size_match = video_data.len() == retrieved_data.len();
        let data_match = video_data == retrieved_data;
        
        let success = hash_match && size_match && data_match;
        
        println!("   ⏱️  Storage: {:.2?}, Retrieval: {:.2?}", storage_time, retrieval_time);
        println!("   🔍 Integrity: Hash={}, Size={}, Data={}", 
                hash_match, size_match, data_match);
        
        // Calculate throughput
        let storage_throughput = size as f64 / storage_time.as_secs_f64() / (1024.0 * 1024.0);
        let retrieval_throughput = size as f64 / retrieval_time.as_secs_f64() / (1024.0 * 1024.0);
        println!("   📈 Throughput: Storage {:.1} MB/s, Retrieval {:.1} MB/s", 
                storage_throughput, retrieval_throughput);
        
        Ok(VideoTestResult {
            name: name.to_string(),
            description: description.to_string(),
            original_size: video_data.len(),
            retrieved_size: retrieved_data.len(),
            hash_match,
            storage_time,
            retrieval_time,
            uses_indirect_blocks: uses_indirect,
            success,
        })
    }
    
    fn show_results(&self) {
        println!("📊 LARGE VIDEO TEST RESULTS");
        println!("===========================");
        
        let successful = self.test_results.iter().filter(|r| r.success).count();
        let total = self.test_results.len();
        
        println!("✅ Successful: {}/{} ({:.1}%)", 
                successful, total, 
                successful as f64 / total as f64 * 100.0);
        
        if successful < total {
            println!("❌ Failed: {}", total - successful);
        }
        
        println!("\n📋 Detailed Results:");
        for (i, result) in self.test_results.iter().enumerate() {
            let status = if result.success { "✅" } else { "❌" };
            let block_type = if result.uses_indirect_blocks { "Indirect" } else { "Direct" };
            let size_mb = result.original_size as f64 / (1024.0 * 1024.0);
            
            println!("{}. {} {} - {:.1} MB ({})", 
                    i + 1, status, result.name, size_mb, block_type);
            
            if result.success {
                println!("     Storage: {:.2?}, Retrieval: {:.2?}", 
                        result.storage_time, result.retrieval_time);
            }
        }
        
        // Performance analysis
        if !self.test_results.is_empty() {
            println!("\n⚡ Performance Analysis:");
            
            let direct_results: Vec<_> = self.test_results.iter()
                .filter(|r| !r.uses_indirect_blocks && r.success)
                .collect();
            
            let indirect_results: Vec<_> = self.test_results.iter()
                .filter(|r| r.uses_indirect_blocks && r.success)
                .collect();
            
            if !direct_results.is_empty() {
                let avg_direct_storage: std::time::Duration = direct_results.iter()
                    .map(|r| r.storage_time)
                    .sum::<std::time::Duration>() / direct_results.len() as u32;
                
                println!("   Direct blocks - Avg storage time: {:.2?}", avg_direct_storage);
            }
            
            if !indirect_results.is_empty() {
                let avg_indirect_storage: std::time::Duration = indirect_results.iter()
                    .map(|r| r.storage_time)
                    .sum::<std::time::Duration>() / indirect_results.len() as u32;
                
                println!("   Indirect blocks - Avg storage time: {:.2?}", avg_indirect_storage);
            }
            
            let total_bytes: usize = self.test_results.iter()
                .filter(|r| r.success)
                .map(|r| r.original_size)
                .sum();
            
            println!("   Total data processed: {:.2} MB", 
                    total_bytes as f64 / (1024.0 * 1024.0));
        }
        
        println!("\n🎯 Block Storage Verification:");
        println!("   32MB limit tests:");
        for result in &self.test_results {
            if result.success {
                let expected_indirect = result.original_size > BLOCK_SIZE;
                let size_mb = result.original_size as f64 / (1024.0 * 1024.0);
                println!("   - {:.1}MB: {} (expected {})", 
                        size_mb,
                        if result.uses_indirect_blocks { "Indirect" } else { "Direct" },
                        if expected_indirect { "Indirect" } else { "Direct" });
            }
        }
        
        println!("\n💡 Test completed! The block storage system should:");
        println!("   ✓ Use direct blocks for files ≤ 32MB");
        println!("   ✓ Use indirect blocks for files > 32MB");
        println!("   ✓ Maintain data integrity regardless of block type");
        println!("   ✓ Handle large files efficiently with erasure coding");
    }
}

/// Generate mock video data of the specified size
/// This creates realistic-looking video data with headers and frame patterns
fn generate_mock_video_data(size: usize) -> Vec<u8> {
    let mut data = Vec::with_capacity(size);
    
    // Mock MP4 header (simplified)
    let header = b"ftypisom\x00\x00\x02\x00isomiso2avc1mp41";
    data.extend_from_slice(header);
    
    // Add metadata section
    let metadata = format!("MOCK_VIDEO_{}MB_TEST_DATA", size / (1024 * 1024));
    data.extend_from_slice(metadata.as_bytes());
    
    // Fill the rest with pseudo-random video frame data
    // Use a pattern that compresses poorly to simulate real video data
    let mut frame_counter = 0u64;
    
    while data.len() < size {
        // Simulate video frame header
        data.extend_from_slice(b"\x00\x00\x01\xE0"); // MPEG frame start code
        
        // Add frame number
        data.extend_from_slice(&frame_counter.to_le_bytes());
        frame_counter += 1;
        
        // Add pseudo-random frame data that doesn't compress well
        let frame_size = std::cmp::min(8192, size - data.len()); // 8KB frames
        for i in 0..frame_size {
            // Create a pattern that looks like video data but doesn't compress
            let byte = ((frame_counter * 31 + i as u64 * 17) % 256) as u8;
            data.push(byte);
            
            if data.len() >= size {
                break;
            }
        }
    }
    
    // Ensure exact size
    data.truncate(size);
    
    data
}

fn calculate_hash(data: &[u8]) -> [u8; 32] {
    let mut hasher = Sha256::new();
    hasher.update(data);
    let result = hasher.finalize();
    let mut hash = [0u8; 32];
    hash.copy_from_slice(&result);
    hash
}


// =====================================================================
// FILE: packages/soradyne_core/src/lib.rs
// =====================================================================

// Soradyne - Collaborative Media Album System

pub mod flow;
pub mod identity;
pub mod network;
pub mod types;
pub mod storage;
pub mod album;
pub mod video;
pub mod ffi;

use crate::storage::device_identity::discover_soradyne_volumes;
use std::sync::atomic::{AtomicBool, Ordering};
use tokio::time::{interval, Duration};
use tokio::sync::RwLock;

#[derive(Debug, Clone)]
pub struct StorageStatus {
    pub available_devices: usize,
    pub required_threshold: usize,
    pub can_read_data: bool,
    pub missing_devices: usize,
    pub device_paths: Vec<String>,
}



// =====================================================================
// FILE: packages/soradyne_core/src/video/mod.rs
// =====================================================================

use std::process::Command;
use image::{RgbaImage, Rgba};

#[cfg(feature = "video-thumbnails")]
use ffmpeg_next as ffmpeg;

/// Extract a video frame using FFmpeg
pub fn extract_video_frame(video_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    println!("🎬 extract_video_frame called with {} bytes", video_data.len());
    
    #[cfg(feature = "video-thumbnails")]
    {
        // Try native FFmpeg first
        match extract_video_frame_native(video_data) {
            Ok(frame_data) => {
                println!("✅ Native FFmpeg extraction succeeded: {} bytes", frame_data.len());
                return Ok(frame_data);
            }
            Err(e) => {
                println!("⚠️ Native FFmpeg failed: {}, falling back to system call", e);
            }
        }
    }
    
    // Fallback to system call
    extract_video_frame_system(video_data)
}

#[cfg(feature = "video-thumbnails")]
fn extract_video_frame_native(video_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    println!("🔧 Using native FFmpeg extraction");
    
    // Initialize FFmpeg
    ffmpeg::init()?;
    
    // Write video data to a temporary file for now
    // TODO: Implement proper memory-based input when the API supports it
    let temp_dir = std::env::temp_dir();
    let input_path = temp_dir.join(format!("video_input_{}.mp4", std::process::id()));
    std::fs::write(&input_path, video_data)?;
    
    // Open input file
    let mut input_context = ffmpeg::format::input(&input_path)?;
    
    // Find the best video stream
    let video_stream_index = input_context
        .streams()
        .best(ffmpeg::media::Type::Video)
        .ok_or("No video stream found")?
        .index();
    
    let video_stream = input_context.stream(video_stream_index).unwrap();
    let codec_context = ffmpeg::codec::Context::from_parameters(video_stream.parameters())?;
    let mut decoder = codec_context.decoder().video()?;
    
    // Seek to 1 second
    let time_base = video_stream.time_base();
    let seek_timestamp = (1.0 / f64::from(time_base.denominator()) * f64::from(time_base.numerator())) as i64;
    input_context.seek(seek_timestamp, seek_timestamp..)?;
    
    // Decode frames until we get one
    let mut frame = ffmpeg::frame::Video::empty();
    
    for (stream, packet) in input_context.packets() {
        if stream.index() == video_stream_index {
            decoder.send_packet(&packet)?;
            
            while decoder.receive_frame(&mut frame).is_ok() {
                // Convert frame to RGB
                let mut rgb_frame = ffmpeg::frame::Video::empty();
                let mut converter = ffmpeg::software::scaling::context::Context::get(
                    frame.format(),
                    frame.width(),
                    frame.height(),
                    ffmpeg::format::Pixel::RGB24,
                    frame.width(),
                    frame.height(),
                    ffmpeg::software::scaling::flag::Flags::BILINEAR,
                )?;
                
                converter.run(&frame, &mut rgb_frame)?;
                
                // Convert RGB frame to JPEG bytes
                let jpeg_data = rgb_frame_to_jpeg(&rgb_frame)?;
                println!("📸 Native FFmpeg extracted {} bytes", jpeg_data.len());
                
                // Clean up temp file
                let _ = std::fs::remove_file(&input_path);
                
                return Ok(jpeg_data);
            }
        }
    }
    
    // Clean up temp file
    let _ = std::fs::remove_file(&input_path);
    
    Err("No frame could be extracted".into())
}

#[cfg(feature = "video-thumbnails")]
fn rgb_frame_to_jpeg(frame: &ffmpeg::frame::Video) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{ImageBuffer, Rgb};
    
    let width = frame.width();
    let height = frame.height();
    let data = frame.data(0);
    
    // Create image buffer from RGB data
    let img: ImageBuffer<Rgb<u8>, Vec<u8>> = ImageBuffer::from_raw(width, height, data.to_vec())
        .ok_or("Failed to create image buffer from frame data")?;
    
    // Encode as JPEG
    let mut buffer = Vec::new();
    let dynamic_img = image::DynamicImage::ImageRgb8(img);
    dynamic_img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Jpeg(85))?;
    
    Ok(buffer)
}

fn extract_video_frame_system(video_data: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    println!("🔧 Using system FFmpeg extraction");
    
    // Check if ffmpeg is available
    let ffmpeg_check = Command::new("ffmpeg").arg("-version").output();
    match ffmpeg_check {
        Ok(output) if output.status.success() => {
            println!("✅ System ffmpeg is available");
        }
        Ok(output) => {
            println!("❌ System ffmpeg version check failed: {}", String::from_utf8_lossy(&output.stderr));
            return Err("System ffmpeg version check failed".into());
        }
        Err(e) => {
            println!("❌ System ffmpeg not found: {}", e);
            return Err(format!("System ffmpeg not found: {}", e).into());
        }
    }
    
    // Create temporary files
    let temp_dir = std::env::temp_dir();
    println!("📁 Using temp dir: {:?}", temp_dir);
    let input_path = temp_dir.join(format!("video_input_{}.mp4", std::process::id()));
    let output_path = temp_dir.join(format!("frame_output_{}.jpg", std::process::id()));
    
    // Write video data to temporary file
    println!("📝 Writing {} bytes to temp file: {:?}", video_data.len(), input_path);
    std::fs::write(&input_path, video_data)?;
    println!("✅ Successfully wrote video data to temp file");
    
    // Extract frame at 1 second using FFmpeg
    println!("🎬 Running system ffmpeg to extract frame...");
    let output = Command::new("ffmpeg")
        .args(&[
            "-i", input_path.to_str().unwrap(),
            "-ss", "00:00:01.000",  // Seek to 1 second
            "-vframes", "1",        // Extract 1 frame
            "-q:v", "2",           // High quality
            "-y",                  // Overwrite output
            output_path.to_str().unwrap()
        ])
        .output();
    
    // Clean up input file
    let _ = std::fs::remove_file(&input_path);
    
    match output {
        Ok(result) if result.status.success() => {
            println!("✅ System ffmpeg succeeded, reading extracted frame");
            // Read the extracted frame
            let frame_data = std::fs::read(&output_path)?;
            println!("📸 Successfully read {} bytes from extracted frame", frame_data.len());
            // Clean up output file
            let _ = std::fs::remove_file(&output_path);
            Ok(frame_data)
        }
        Ok(result) => {
            println!("❌ System ffmpeg failed with exit code: {:?}", result.status.code());
            println!("❌ System ffmpeg stderr: {}", String::from_utf8_lossy(&result.stderr));
            println!("❌ System ffmpeg stdout: {}", String::from_utf8_lossy(&result.stdout));
            // Clean up output file if it exists
            let _ = std::fs::remove_file(&output_path);
            Err("System FFmpeg frame extraction failed".into())
        }
        Err(e) => {
            println!("❌ Failed to execute system ffmpeg: {}", e);
            // Clean up output file if it exists
            let _ = std::fs::remove_file(&output_path);
            Err(format!("Failed to execute system ffmpeg: {}", e).into())
        }
    }
}

/// Generate video thumbnail at specific size
pub fn generate_video_at_size(video_data: &[u8], max_size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // Try to extract a frame using FFmpeg first
    if let Ok(frame_data) = extract_video_frame(video_data) {
        // Generate resized image from the extracted frame
        return generate_image_at_size(&frame_data, max_size);
    }
    
    // Fall back to placeholder if FFmpeg extraction fails
    create_video_placeholder_at_size(max_size)
}

/// Generate resized image at specific size
pub fn generate_image_at_size(image_data: &[u8], max_size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // Load the image from the data
    let img = image::load_from_memory(image_data)?;
    
    // Resize while maintaining aspect ratio
    let resized = img.thumbnail(max_size, max_size);
    
    // Use higher quality for larger sizes
    let quality = match max_size {
        0..=200 => 70,
        201..=800 => 85,
        _ => 95,
    };
    
    // Encode as JPEG
    let mut buffer = Vec::new();
    resized.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Jpeg(quality))?;
    
    Ok(buffer)
}

/// Create video placeholder at specific size
pub fn create_video_placeholder_at_size(size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // Create a size x size image with a prominent play button
    let mut img = RgbaImage::new(size, size);
    let center_x = size / 2;
    let center_y = size / 2;
    
    // Create a dark video-like background
    for (x, y, pixel) in img.enumerate_pixels_mut() {
        *pixel = Rgba([30, 30, 30, 255]);
        
        // Draw a large white play triangle scaled to size
        let triangle_size = size / 5;
        
        // Draw a proper right-pointing triangle (play button)
        let triangle_left = center_x - triangle_size / 3;
        let triangle_right = center_x + triangle_size / 3;
        
        // Check if we're in the triangle area
        if x >= triangle_left && x <= triangle_right {
            let relative_x = x as i32 - triangle_left as i32;
            let triangle_width = (triangle_right - triangle_left) as i32;
            
            // Calculate the triangle bounds at this x position (right-pointing)
            let half_height_at_x = (relative_x * triangle_size as i32) / (triangle_width * 2);
            let top_bound = center_y as i32 - half_height_at_x;
            let bottom_bound = center_y as i32 + half_height_at_x;
            
            if y as i32 >= top_bound && y as i32 <= bottom_bound {
                *pixel = Rgba([255, 255, 255, 255]); // White play button
            }
        }
        
        // Add a subtle border to make it look more like a video thumbnail
        let border_width = (size / 50).max(1);
        if x < border_width || x >= size - border_width || y < border_width || y >= size - border_width {
            *pixel = Rgba([100, 100, 100, 255]); // Gray border
        }
    }
    
    // Encode as PNG
    let mut buffer = Vec::new();
    img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    
    Ok(buffer)
}

/// Create audio placeholder at specific size
pub fn create_audio_placeholder_at_size(size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{RgbaImage, Rgba};
    
    // Create a size x size image with audio waveform visualization
    let mut img = RgbaImage::new(size, size);
    let dark_bg = Rgba([20, 25, 35, 255]);
    let waveform_color = Rgba([100, 150, 255, 255]);
    
    // Fill with dark background
    for pixel in img.pixels_mut() {
        *pixel = dark_bg;
    }
    
    // Draw simplified waveform bars
    let bar_width = (size / 40).max(2);
    let bar_spacing = (size / 25).max(3);
    let num_bars = size / bar_spacing;
    let center_y = size / 2;
    
    for i in 0..num_bars {
        let bar_x = i * bar_spacing + size / 15;
        let bar_height = (size / 8) + ((i * 7) % (size / 4)); // Varying heights
        let bar_top = center_y.saturating_sub(bar_height / 2);
        let bar_bottom = center_y + bar_height / 2;
        
        for y in bar_top..bar_bottom.min(size) {
            for x in bar_x..(bar_x + bar_width).min(size) {
                img.put_pixel(x, y, waveform_color);
            }
        }
    }
    
    // Encode as PNG
    let mut buffer = Vec::new();
    img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    
    Ok(buffer)
}

/// Detect if data is a video file
pub fn is_video_file(data: &[u8]) -> bool {
    if data.len() < 12 {
        return false;
    }
    
    // Check for common video file signatures
    // MP4/MOV files start with specific patterns
    if data.len() >= 8 {
        // Check for MP4 ftyp box
        if &data[4..8] == b"ftyp" {
            return true;
        }
    }
    
    // Check for WebM signature
    if data.len() >= 4 && &data[0..4] == b"\x1A\x45\xDF\xA3" {
        return true;
    }
    
    // Check for AVI signature
    if data.len() >= 12 && &data[0..4] == b"RIFF" && &data[8..12] == b"AVI " {
        return true;
    }
    
    // Check for QuickTime/MOV signature
    if data.len() >= 8 && &data[4..8] == b"moov" {
        return true;
    }
    
    // Check for additional MP4 variants
    if data.len() >= 12 {
        let ftyp_slice = &data[4..8];
        if ftyp_slice == b"ftyp" {
            let brand = &data[8..12];
            // Common MP4 brands
            if brand == b"isom" || brand == b"mp41" || brand == b"mp42" || 
               brand == b"avc1" || brand == b"dash" || brand == b"iso2" {
                return true;
            }
        }
    }
    
    false
}

/// Detect if data is an audio file
pub fn is_audio_file(data: &[u8]) -> bool {
    if data.len() < 4 {
        return false;
    }
    
    // Skip very small files that are likely just metadata/headers
    if data.len() < 1000 {
        return false;
    }
    
    // Check for MP3 signature - improved detection
    if data.len() >= 3 {
        // MP3 with ID3v2 tag
        if &data[0..3] == b"ID3" {
            return true;
        }
    }
    
    // Check for MP3 frame sync pattern - scan first few KB for frame headers
    for i in 0..(data.len().min(4096) - 1) {
        if data[i] == 0xFF && (data[i + 1] & 0xE0) == 0xE0 {
            // Additional validation: check if this looks like a valid MP3 frame header
            if i + 4 < data.len() {
                let header = u32::from_be_bytes([data[i], data[i+1], data[i+2], data[i+3]]);
                if is_valid_mp3_header(header) {
                    return true;
                }
            }
        }
    }
    
    // Check for FLAC signature
    if data.len() >= 4 && &data[0..4] == b"fLaC" {
        return true;
    }
    
    // Check for OGG signature (Vorbis/Opus)
    if data.len() >= 4 && &data[0..4] == b"OggS" {
        return true;
    }
    
    // Check for WAV signature - ensure it's a reasonable size
    if data.len() >= 12 && &data[0..4] == b"RIFF" && &data[8..12] == b"WAVE" {
        return true;
    }
    
    // Check for AIFF signature
    if data.len() >= 12 && &data[0..4] == b"FORM" && &data[8..12] == b"AIFF" {
        return true;
    }
    
    // Check for M4A (AAC in MP4 container)
    if data.len() >= 12 {
        let ftyp_slice = &data[4..8];
        if ftyp_slice == b"ftyp" {
            let brand = &data[8..12];
            // Common M4A brands
            if brand == b"M4A " || brand == b"mp42" || brand == b"isom" {
                return true;
            }
        }
    }
    
    // Check for AAC ADTS header
    if data.len() >= 2 && data[0] == 0xFF && (data[1] & 0xF0) == 0xF0 {
        return true;
    }
    
    // Check for AC3 signature
    if data.len() >= 2 && data[0] == 0x0B && data[1] == 0x77 {
        return true;
    }
    
    false
}

fn is_valid_mp3_header(header: u32) -> bool {
    // Check MP3 frame header validity
    // Bits 31-21: Frame sync (all 1s) - already checked
    // Bits 20-19: MPEG Audio version ID
    let version = (header >> 19) & 0x3;
    if version == 1 { return false; } // Reserved
    
    // Bits 18-17: Layer description
    let layer = (header >> 17) & 0x3;
    if layer == 0 { return false; } // Reserved
    
    // Bits 15-12: Bitrate index
    let bitrate = (header >> 12) & 0xF;
    if bitrate == 0 || bitrate == 15 { return false; } // Free or bad bitrate
    
    // Bits 11-10: Sampling rate frequency index
    let sample_rate = (header >> 10) & 0x3;
    if sample_rate == 3 { return false; } // Reserved
    
    true
}


// =====================================================================
// FILE: packages/soradyne_core/src/types/heartrate.rs
// =====================================================================

//! types/heartrate.rs
//!
//! Defines the Heartrate struct and the HeartrateFlow type, which is a concrete
//! SelfDataFlow for streaming and synchronizing heartrate data across devices.

use serde::{Serialize, Deserialize};
use chrono::{DateTime, Utc};
use uuid::Uuid;
use crate::flow::SelfDataFlow;

/// Heartrate data structure
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Heartrate {
    pub bpm: f32,
    pub source_device_id: Uuid,
    pub timestamp: DateTime<Utc>,
}

impl Heartrate {
    pub fn new(bpm: f32, source_device_id: Uuid) -> Self {
        Self {
            bpm,
            source_device_id,
            timestamp: Utc::now(),
        }
    }
}

/// A type alias for SelfDataFlow carrying Heartrate data
pub type HeartrateFlow = SelfDataFlow<Heartrate>;



// =====================================================================
// FILE: packages/soradyne_core/src/types/media.rs
// =====================================================================

use crate::storage::block_file::BlockFile;
use crate::storage::block_manager::BlockManager;
use crate::flow::FlowError;
use std::sync::Arc;
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use chrono::{DateTime, Utc};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PhotoMetadata {
    pub id: Uuid,
    pub name: String,
    pub mime_type: String,
    pub width: u32,
    pub height: u32,
    pub created_at: DateTime<Utc>,
    pub root_block: Option<[u8; 32]>,
    pub size: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VideoMetadata {
    pub id: Uuid,
    pub name: String,
    pub mime_type: String,
    pub width: u32,
    pub height: u32,
    pub duration_seconds: f64,
    pub created_at: DateTime<Utc>,
    pub root_block: Option<[u8; 32]>,
    pub size: usize,
}

pub struct PhotoStorage {
    block_manager: Arc<BlockManager>,
}

impl PhotoStorage {
    pub fn new(block_manager: Arc<BlockManager>) -> Self {
        Self { block_manager }
    }
    
    pub async fn save_photo(&self, name: &str, mime_type: &str, data: &[u8]) 
        -> Result<PhotoMetadata, FlowError> {
        let file = BlockFile::new(self.block_manager.clone());
        file.append(data).await?;
        
        let metadata = PhotoMetadata {
            id: Uuid::new_v4(),
            name: name.to_string(),
            mime_type: mime_type.to_string(),
            width: 0, // TODO: Parse from image data
            height: 0, // TODO: Parse from image data
            created_at: Utc::now(),
            root_block: file.root_block().await,
            size: data.len(),
        };
        
        Ok(metadata)
    }
    
    pub async fn load_photo(&self, metadata: &PhotoMetadata) 
        -> Result<Vec<u8>, FlowError> {
        if let Some(root_block) = metadata.root_block {
            let file = BlockFile::from_existing(
                self.block_manager.clone(), 
                root_block, 
                metadata.size
            );
            file.read().await
        } else {
            Err(FlowError::PersistenceError("Photo has no data".to_string()))
        }
    }
}

pub struct VideoStorage {
    block_manager: Arc<BlockManager>,
}

impl VideoStorage {
    pub fn new(block_manager: Arc<BlockManager>) -> Self {
        Self { block_manager }
    }
    
    pub async fn save_video(&self, name: &str, mime_type: &str, data: &[u8]) 
        -> Result<VideoMetadata, FlowError> {
        let file = BlockFile::new(self.block_manager.clone());
        file.append(data).await?;
        
        let metadata = VideoMetadata {
            id: Uuid::new_v4(),
            name: name.to_string(),
            mime_type: mime_type.to_string(),
            width: 0, // TODO: Parse from video data
            height: 0, // TODO: Parse from video data
            duration_seconds: 0.0, // TODO: Parse from video data
            created_at: Utc::now(),
            root_block: file.root_block().await,
            size: data.len(),
        };
        
        Ok(metadata)
    }
    
    pub async fn load_video(&self, metadata: &VideoMetadata) 
        -> Result<Vec<u8>, FlowError> {
        if let Some(root_block) = metadata.root_block {
            let file = BlockFile::from_existing(
                self.block_manager.clone(), 
                root_block, 
                metadata.size
            );
            file.read().await
        } else {
            Err(FlowError::PersistenceError("Video has no data".to_string()))
        }
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/types/files.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/types/mod.rs
// =====================================================================

pub mod heartrate;
pub mod media;



// =====================================================================
// FILE: packages/soradyne_core/src/types/photos.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/types/robot_state.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/types/chat.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/identity/keys.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/identity/auth.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/identity/mod.rs
// =====================================================================

// This module will contain identity-related functionality in the future
// For now, it's just a placeholder to establish the directory structure


// =====================================================================
// FILE: packages/soradyne_core/src/network/protocol.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/network/discovery.rs
// =====================================================================

//! Peer discovery mechanisms for SelfDataFlow network.

use std::net::SocketAddr;
use uuid::Uuid;
use crate::flow::FlowError;

/// Represents a discovered peer on the network
#[derive(Debug, Clone)]
pub struct DiscoveredPeer {
    /// Unique identifier for the peer
    pub id: Uuid,
    /// Network address of the peer
    pub address: SocketAddr,
    /// Optional name of the peer
    pub name: Option<String>,
    /// Optional device type
    pub device_type: Option<String>,
}

/// Trait for peer discovery mechanisms
pub trait PeerDiscovery {
    /// Start the discovery process
    fn start_discovery(&self) -> Result<(), FlowError>;
    
    /// Stop the discovery process
    fn stop_discovery(&self) -> Result<(), FlowError>;
    
    /// Get the list of currently discovered peers
    fn get_discovered_peers(&self) -> Vec<DiscoveredPeer>;
    
    /// Register a callback to be notified when a new peer is discovered
    fn on_peer_discovered(&self, callback: Box<dyn Fn(&DiscoveredPeer) + Send + Sync>);
}

/// A no-op implementation of PeerDiscovery that doesn't actually discover peers.
/// Useful as a placeholder until real discovery is implemented.
pub struct NoOpDiscovery;

impl PeerDiscovery for NoOpDiscovery {
    fn start_discovery(&self) -> Result<(), FlowError> {
        println!("[Discovery] Not implemented: start_discovery");
        Ok(())
    }
    
    fn stop_discovery(&self) -> Result<(), FlowError> {
        println!("[Discovery] Not implemented: stop_discovery");
        Ok(())
    }
    
    fn get_discovered_peers(&self) -> Vec<DiscoveredPeer> {
        println!("[Discovery] Not implemented: get_discovered_peers");
        Vec::new()
    }
    
    fn on_peer_discovered(&self, _callback: Box<dyn Fn(&DiscoveredPeer) + Send + Sync>) {
        println!("[Discovery] Not implemented: on_peer_discovered");
    }
}

/// mDNS-based peer discovery (placeholder)
pub struct MdnsDiscovery;

impl PeerDiscovery for MdnsDiscovery {
    fn start_discovery(&self) -> Result<(), FlowError> {
        println!("[Discovery] Not implemented: mDNS discovery");
        Ok(())
    }
    
    fn stop_discovery(&self) -> Result<(), FlowError> {
        println!("[Discovery] Not implemented: stop mDNS discovery");
        Ok(())
    }
    
    fn get_discovered_peers(&self) -> Vec<DiscoveredPeer> {
        println!("[Discovery] Not implemented: get mDNS discovered peers");
        Vec::new()
    }
    
    fn on_peer_discovered(&self, _callback: Box<dyn Fn(&DiscoveredPeer) + Send + Sync>) {
        println!("[Discovery] Not implemented: mDNS on_peer_discovered");
    }
}

/// LAN-based peer discovery using broadcast (placeholder)
pub struct LanDiscovery;

impl PeerDiscovery for LanDiscovery {
    fn start_discovery(&self) -> Result<(), FlowError> {
        println!("[Discovery] Not implemented: LAN discovery");
        Ok(())
    }
    
    fn stop_discovery(&self) -> Result<(), FlowError> {
        println!("[Discovery] Not implemented: stop LAN discovery");
        Ok(())
    }
    
    fn get_discovered_peers(&self) -> Vec<DiscoveredPeer> {
        println!("[Discovery] Not implemented: get LAN discovered peers");
        Vec::new()
    }
    
    fn on_peer_discovered(&self, _callback: Box<dyn Fn(&DiscoveredPeer) + Send + Sync>) {
        println!("[Discovery] Not implemented: LAN on_peer_discovered");
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/network/mod.rs
// =====================================================================

pub mod connection;
pub mod discovery;

pub use connection::NetworkBridge;
pub use discovery::{PeerDiscovery, NoOpDiscovery, MdnsDiscovery, LanDiscovery, DiscoveredPeer};



// =====================================================================
// FILE: packages/soradyne_core/src/network/connection.rs
// =====================================================================

use std::sync::Arc;
use tokio::net::{TcpListener, TcpStream};
use tokio::net::tcp::OwnedWriteHalf;
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader};
use tokio::task;
use tokio::sync::Mutex;
use serde_json;
use crate::types::heartrate::Heartrate;
use crate::flow::SelfDataFlow;
use crate::flow::FlowError;
use crate::network::discovery::{PeerDiscovery, DiscoveredPeer};

pub struct NetworkBridge {
    peers: Arc<Mutex<Vec<Arc<Mutex<OwnedWriteHalf>>>>>,
    discovery: Option<Arc<dyn PeerDiscovery + Send + Sync>>,
}

impl NetworkBridge {
    pub fn new() -> Self {
        Self {
            peers: Arc::new(Mutex::new(Vec::new())),
            discovery: None,
        }
    }
    
    /// Set the discovery mechanism for this network bridge
    pub fn with_discovery(mut self, discovery: impl PeerDiscovery + Send + Sync + 'static) -> Self {
        self.discovery = Some(Arc::new(discovery));
        self
    }
    
    /// Start peer discovery
    pub fn start_discovery(&self) -> Result<(), FlowError> {
        if let Some(discovery) = &self.discovery {
            discovery.start_discovery()?;
            
            // Set up callback for newly discovered peers
            let _peers_clone = self.peers.clone();
            discovery.on_peer_discovered(Box::new(move |peer| {
                println!("[NetworkBridge] Discovered peer: {:?}", peer);
                // In a real implementation, we would automatically connect to the peer
                // For now, just log the discovery
            }));
            
            Ok(())
        } else {
            Err(FlowError::PersistenceError("No discovery mechanism configured".to_string()))
        }
    }
    
    /// Stop peer discovery
    pub fn stop_discovery(&self) -> Result<(), FlowError> {
        if let Some(discovery) = &self.discovery {
            discovery.stop_discovery()
        } else {
            Err(FlowError::PersistenceError("No discovery mechanism configured".to_string()))
        }
    }
    
    /// Get discovered peers
    pub fn get_discovered_peers(&self) -> Vec<DiscoveredPeer> {
        if let Some(discovery) = &self.discovery {
            discovery.get_discovered_peers()
        } else {
            Vec::new()
        }
    }

    /// Listen for incoming connections and handle receiving heartrate updates
    pub async fn listen(
        &self,
        addr: &str,
        flow: Arc<SelfDataFlow<Heartrate>>,
    ) -> tokio::io::Result<()> {
        let listener = TcpListener::bind(addr).await?;
        println!("[NetworkBridge] Listening on {}", addr);

        loop {
            let (socket, peer_addr) = listener.accept().await?;
            let (read_half, write_half) = socket.into_split();

            let write_half = Arc::new(Mutex::new(write_half));
            println!("[NetworkBridge] New peer connected: {}", peer_addr);

            let flow_clone = flow.clone();
            let peers_clone = self.peers.clone();

            {
                let mut peers = peers_clone.lock().await;
                peers.push(write_half.clone());
            }

            // Handle incoming messages from this peer
            task::spawn(async move {
                let reader = BufReader::new(read_half);
                let mut lines = reader.lines();

                while let Ok(Some(line)) = lines.next_line().await {
                    match serde_json::from_str::<Heartrate>(&line) {
                        Ok(heartrate) => {
                            println!(
                                "[NetworkBridge] Received heartrate: {:.1} bpm",
                                heartrate.bpm
                            );
                            flow_clone.merge(heartrate);
                        }
                        Err(e) => {
                            eprintln!("[NetworkBridge] Failed to parse message: {:?}", e);
                        }
                    }
                }

                println!("[NetworkBridge] Peer {} disconnected", peer_addr);
            });
        }
    }

    /// Connect to a peer and add to broadcast list
    pub async fn connect(&self, addr: &str) -> tokio::io::Result<()> {
        let stream = TcpStream::connect(addr).await?;
        let (_read_half, write_half) = stream.into_split();

        {
            let mut peers = self.peers.lock().await;
            peers.push(Arc::new(Mutex::new(write_half)));
        }
        println!("[NetworkBridge] Connected to peer at {}", addr);
        Ok(())
    }

    /// Broadcast a heartrate to all connected peers
    pub fn broadcast(&self, heartrate: &Heartrate) {
        let json = serde_json::to_string(heartrate).unwrap();
        let peers = self.peers.clone();
        task::spawn(async move {
            let peers = peers.lock().await;
            for peer in peers.iter() {
                let peer = peer.clone();
                let json = json.clone();
                task::spawn(async move {
                    let mut writer = peer.lock().await;
                    if let Err(e) = writer.write_all(json.as_bytes()).await {
                        eprintln!("[NetworkBridge] Failed to send data: {:?}", e);
                    }
                    if let Err(e) = writer.write_all(b"\n").await {
                        eprintln!("[NetworkBridge] Failed to send newline: {:?}", e);
                    }
                });
            }
        });
    }
}



// =====================================================================
// FILE: packages/soradyne_core/src/utils/mod.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/utils/crypto.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/storage/dissolution.rs
// =====================================================================

//! Dissolution storage abstraction layer
//! 
//! This module provides a unified interface for different dissolution storage backends,
//! allowing the system to work with manual erasure coding, bcachefs, or future backends
//! through a common API.

use async_trait::async_trait;
use std::path::PathBuf;
use serde::{Serialize, Deserialize};
use crate::flow::FlowError;

/// Block ID type - 32 bytes for cryptographic hashing
pub type BlockId = [u8; 32];

/// Configuration for a dissolution storage backend
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DissolutionConfig {
    /// Minimum number of shards needed to reconstruct data
    pub threshold: usize,
    /// Total number of shards to create
    pub total_shards: usize,
    /// Maximum size for direct blocks (before using indirect blocks)
    pub max_direct_block_size: usize,
    /// Backend-specific configuration
    pub backend_config: BackendConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum BackendConfig {
    /// Soradyne erasure coding with custom shard distribution
    SdynErasure {
        /// Paths to rimsd directories for shard storage
        rimsd_paths: Vec<PathBuf>,
        /// Path for metadata storage
        metadata_path: PathBuf,
    },
    /// bcachefs-based dissolution (Linux only)
    BcacheFS {
        /// bcachefs device paths
        device_paths: Vec<PathBuf>,
        /// Mount options for SD card optimization
        mount_options: Vec<String>,
        /// Whether to disable CoW for longevity
        disable_cow: bool,
    },
    /// Future: ZFS with erasure coding
    ZFS {
        pool_name: String,
        redundancy_level: String,
    },
    /// Future: Custom distributed filesystem
    Custom {
        implementation_name: String,
        config: serde_json::Value,
    },
}

/// Information about a stored block
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockInfo {
    pub id: BlockId,
    pub size: usize,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub is_indirect: bool,
    pub shard_count: usize,
    pub available_shards: usize,
    pub can_reconstruct: bool,
}

/// Result of a dissolution demonstration/test
#[derive(Debug, Clone)]
pub struct DissolutionDemo {
    pub block_id: BlockId,
    pub original_shards: usize,
    pub simulated_missing: Vec<usize>,
    pub available_shards: usize,
    pub threshold_required: usize,
    pub can_reconstruct: bool,
    pub reconstruction_successful: bool,
    pub data_integrity_verified: bool,
    pub recovered_data_size: usize,
}

/// Statistics about storage usage and health
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageStats {
    pub total_blocks: usize,
    pub total_size_bytes: u64,
    pub available_devices: usize,
    pub total_devices: usize,
    pub health_score: f64, // 0.0 to 1.0
    pub device_health: Vec<DeviceHealth>,
    pub reconstruction_capability: ReconstructionStats,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceHealth {
    pub device_id: String,
    pub path: PathBuf,
    pub available: bool,
    pub free_space_bytes: u64,
    pub total_space_bytes: u64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReconstructionStats {
    pub blocks_at_risk: usize, // blocks with exactly threshold shards
    pub blocks_safe: usize,    // blocks with > threshold shards
    pub blocks_lost: usize,    // blocks with < threshold shards
}

/// Core abstraction for dissolution storage backends
#[async_trait]
pub trait DissolutionStorage: Send + Sync {
    /// Store data using dissolution, returning a block ID
    async fn store(&self, data: &[u8]) -> Result<BlockId, FlowError>;
    
    /// Retrieve data by block ID, reconstructing from shards
    async fn retrieve(&self, block_id: &BlockId) -> Result<Vec<u8>, FlowError>;
    
    /// Check if a block exists and can be reconstructed
    async fn exists(&self, block_id: &BlockId) -> Result<bool, FlowError>;
    
    /// Get information about a stored block
    async fn block_info(&self, block_id: &BlockId) -> Result<BlockInfo, FlowError>;
    
    /// Delete a block and its shards
    async fn delete(&self, block_id: &BlockId) -> Result<(), FlowError>;
    
    /// List all available blocks
    async fn list_blocks(&self) -> Result<Vec<BlockId>, FlowError>;
    
    /// Get storage statistics
    async fn storage_stats(&self) -> Result<StorageStats, FlowError>;
    
    /// Demonstrate dissolution by simulating shard loss
    async fn demonstrate_dissolution(&self, block_id: &BlockId, simulate_missing: Vec<usize>) -> Result<DissolutionDemo, FlowError>;
    
    /// Perform maintenance operations (compaction, defrag, etc.)
    async fn maintenance(&self) -> Result<(), FlowError>;
    
    /// Get the current configuration
    fn config(&self) -> &DissolutionConfig;
    
    /// Update configuration (where possible)
    async fn update_config(&mut self, config: DissolutionConfig) -> Result<(), FlowError>;
    
    /// Verify device identity and continuity
    async fn verify_device_continuity(&self) -> Result<(), FlowError>;
    
    /// Initialize device fingerprints
    async fn initialize_device_fingerprints(&self) -> Result<(), FlowError>;
}

/// High-level file interface that works with any dissolution backend
pub struct DissolutionFile {
    storage: crate::storage::backends::DissolutionBackend,
    root_block: Option<BlockId>,
    size: usize,
}

impl DissolutionFile {
    /// Create a new file
    pub fn new(storage: crate::storage::backends::DissolutionBackend) -> Self {
        Self {
            storage,
            root_block: None,
            size: 0,
        }
    }
    
    /// Open existing file from root block
    pub fn from_existing(storage: crate::storage::backends::DissolutionBackend, root_block: BlockId, size: usize) -> Self {
        Self {
            storage,
            root_block: Some(root_block),
            size,
        }
    }
    
    /// Write data to the file
    pub async fn write(&mut self, data: &[u8]) -> Result<(), FlowError> {
        let block_id = self.storage.store(data).await?;
        self.root_block = Some(block_id);
        self.size = data.len();
        Ok(())
    }
    
    /// Read the entire file
    pub async fn read(&self) -> Result<Vec<u8>, FlowError> {
        match self.root_block {
            Some(block_id) => self.storage.retrieve(&block_id).await,
            None => Ok(vec![]),
        }
    }
    
    /// Get the root block ID for storage/sharing
    pub fn root_block(&self) -> Option<BlockId> {
        self.root_block
    }
    
    /// Get file size
    pub fn size(&self) -> usize {
        self.size
    }
    
    /// Check if file exists and can be reconstructed
    pub async fn exists(&self) -> Result<bool, FlowError> {
        match self.root_block {
            Some(block_id) => self.storage.exists(&block_id).await,
            None => Ok(false),
        }
    }
    
    /// Get detailed information about the file's storage
    pub async fn info(&self) -> Result<Option<BlockInfo>, FlowError> {
        match self.root_block {
            Some(block_id) => Ok(Some(self.storage.block_info(&block_id).await?)),
            None => Ok(None),
        }
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/device_identity.rs
// =====================================================================

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::fs;
use serde::{Serialize, Deserialize};
use sha2::{Sha256, Digest};

use crate::flow::FlowError;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BasicFingerprint {
    /// Soradyne-assigned device ID (stored in .rimsd directory)
    pub soradyne_device_id: Option<String>,
    /// Combined hardware serial + manufacturer ID
    pub hardware_id: Option<String>,
    /// Filesystem UUID
    pub filesystem_uuid: Option<String>,
    /// Hash of bad block positions (monotonic - can only grow)
    pub bad_block_signature: u64,
    /// Exact capacity in bytes
    pub capacity_bytes: u64,
}

#[derive(Debug, Clone)]
pub struct EvidenceType {
    pub name: String,
    pub weight: f64,
}

#[derive(Debug, Clone)]
pub struct LikelihoodModel {
    /// P(evidence | same device)
    pub prob_same: f64,
    /// P(evidence | different device)
    pub prob_different: f64,
}

#[derive(Debug)]
pub struct BayesianDeviceIdentifier {
    /// Prior probability that a device is the same
    pub prior_same: f64,
    /// Evidence models for each fingerprint component
    pub evidence_models: HashMap<String, LikelihoodModel>,
    /// Confidence threshold for "same device" decision
    pub threshold: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceIdentityResult {
    pub is_same_device: bool,
    pub confidence: f64,
    pub evidence_summary: Vec<String>,
}

impl BasicFingerprint {
    pub fn new(
        soradyne_device_id: Option<String>,
        hardware_id: Option<String>,
        filesystem_uuid: Option<String>,
        bad_blocks: &[u64],
        capacity_bytes: u64,
    ) -> Self {
        let bad_block_signature = Self::hash_bad_blocks(bad_blocks);
        
        Self {
            soradyne_device_id,
            hardware_id,
            filesystem_uuid,
            bad_block_signature,
            capacity_bytes,
        }
    }
    
    fn hash_bad_blocks(bad_blocks: &[u64]) -> u64 {
        let mut hasher = Sha256::new();
        let mut sorted_blocks = bad_blocks.to_vec();
        sorted_blocks.sort();
        
        for block in sorted_blocks {
            hasher.update(block.to_le_bytes());
        }
        
        let result = hasher.finalize();
        u64::from_le_bytes(result[0..8].try_into().unwrap())
    }
    
    /// Check if this fingerprint could be a legitimate evolution of the previous one
    pub fn is_valid_evolution(&self, previous: &BasicFingerprint) -> Result<bool, FlowError> {
        // Soradyne device ID should never change (most reliable identifier)
        if self.soradyne_device_id != previous.soradyne_device_id {
            return Ok(false);
        }
        
        // Hardware ID should never change
        if self.hardware_id != previous.hardware_id {
            return Ok(false);
        }
        
        // Filesystem UUID should only change with reformatting (suspicious)
        if self.filesystem_uuid != previous.filesystem_uuid {
            return Ok(false);
        }
        
        // Capacity should never change
        if self.capacity_bytes != previous.capacity_bytes {
            return Ok(false);
        }
        
        // Bad block signature can only increase (new bad blocks, never fewer)
        // For now, we just check they're different - more sophisticated logic later
        Ok(true)
    }
}

impl Default for BayesianDeviceIdentifier {
    fn default() -> Self {
        let mut evidence_models = HashMap::new();
        
        // Soradyne device ID match (most reliable)
        evidence_models.insert("soradyne_device_id".to_string(), LikelihoodModel {
            prob_same: 0.999,     // Extremely high confidence - we control this
            prob_different: 0.000001, // Virtually impossible collision
        });
        
        // Hardware ID match
        evidence_models.insert("hardware_id".to_string(), LikelihoodModel {
            prob_same: 0.95,      // High confidence when it matches
            prob_different: 0.0001, // Very rare collision
        });
        
        // Filesystem UUID match
        evidence_models.insert("filesystem_uuid".to_string(), LikelihoodModel {
            prob_same: 0.99,      // Very reliable
            prob_different: 0.00001, // Extremely rare collision
        });
        
        // Bad block signature match
        evidence_models.insert("bad_block_signature".to_string(), LikelihoodModel {
            prob_same: 0.90,      // Pretty reliable
            prob_different: 0.001,  // Rare collision
        });
        
        // Capacity match
        evidence_models.insert("capacity".to_string(), LikelihoodModel {
            prob_same: 0.80,      // Common among same model
            prob_different: 0.1,   // Many cards have same capacity
        });
        
        Self {
            prior_same: 0.5,      // No prior bias
            evidence_models,
            threshold: 0.95,      // 95% confidence required
        }
    }
}

impl BayesianDeviceIdentifier {
    pub fn identify_device(
        &self,
        current: &BasicFingerprint,
        previous: &BasicFingerprint,
    ) -> Result<DeviceIdentityResult, FlowError> {
        let mut evidence_summary = Vec::new();
        let mut log_odds = (self.prior_same / (1.0 - self.prior_same)).ln();
        
        // Soradyne device ID evidence (highest priority)
        if let (Some(curr_id), Some(prev_id)) = (&current.soradyne_device_id, &previous.soradyne_device_id) {
            let matches = curr_id == prev_id;
            let model = &self.evidence_models["soradyne_device_id"];
            
            if matches {
                log_odds += (model.prob_same / model.prob_different).ln();
                evidence_summary.push("Soradyne device ID matches".to_string());
            } else {
                log_odds += ((1.0 - model.prob_same) / (1.0 - model.prob_different)).ln();
                evidence_summary.push("Soradyne device ID differs".to_string());
            }
        } else {
            evidence_summary.push("Soradyne device ID unavailable".to_string());
        }
        
        // Hardware ID evidence
        if let (Some(curr_hw), Some(prev_hw)) = (&current.hardware_id, &previous.hardware_id) {
            // Skip default/placeholder hardware IDs
            if is_meaningful_hardware_id(curr_hw) && is_meaningful_hardware_id(prev_hw) {
                let matches = curr_hw == prev_hw;
                let model = &self.evidence_models["hardware_id"];
                
                if matches {
                    log_odds += (model.prob_same / model.prob_different).ln();
                    evidence_summary.push("Hardware ID matches".to_string());
                } else {
                    log_odds += ((1.0 - model.prob_same) / (1.0 - model.prob_different)).ln();
                    evidence_summary.push("Hardware ID differs".to_string());
                }
            } else {
                evidence_summary.push("Hardware ID is placeholder/default".to_string());
            }
        } else {
            evidence_summary.push("Hardware ID unavailable".to_string());
        }
        
        // Filesystem UUID evidence
        if let (Some(curr_fs), Some(prev_fs)) = (&current.filesystem_uuid, &previous.filesystem_uuid) {
            // Skip default/placeholder UUIDs
            if is_meaningful_uuid(curr_fs) && is_meaningful_uuid(prev_fs) {
                let matches = curr_fs == prev_fs;
                let model = &self.evidence_models["filesystem_uuid"];
                
                if matches {
                    log_odds += (model.prob_same / model.prob_different).ln();
                    evidence_summary.push("Filesystem UUID matches".to_string());
                } else {
                    log_odds += ((1.0 - model.prob_same) / (1.0 - model.prob_different)).ln();
                    evidence_summary.push("Filesystem UUID differs".to_string());
                }
            } else {
                evidence_summary.push("Filesystem UUID is placeholder/default".to_string());
            }
        } else {
            evidence_summary.push("Filesystem UUID unavailable".to_string());
        }
        
        // Bad block signature evidence
        // Only use if we have meaningful bad block data (not just empty/default)
        if current.bad_block_signature != 0 || previous.bad_block_signature != 0 {
            let matches = current.bad_block_signature == previous.bad_block_signature;
            let model = &self.evidence_models["bad_block_signature"];
            
            if matches {
                log_odds += (model.prob_same / model.prob_different).ln();
                evidence_summary.push("Bad block pattern matches".to_string());
            } else {
                log_odds += ((1.0 - model.prob_same) / (1.0 - model.prob_different)).ln();
                evidence_summary.push("Bad block pattern differs".to_string());
            }
        } else {
            evidence_summary.push("Bad block data unavailable/empty".to_string());
        }
        
        // Capacity evidence
        let matches = current.capacity_bytes == previous.capacity_bytes;
        let model = &self.evidence_models["capacity"];
        
        if matches {
            log_odds += (model.prob_same / model.prob_different).ln();
            evidence_summary.push("Capacity matches".to_string());
        } else {
            log_odds += ((1.0 - model.prob_same) / (1.0 - model.prob_different)).ln();
            evidence_summary.push("Capacity differs".to_string());
        }
        
        // Convert log odds back to probability
        let odds = log_odds.exp();
        let confidence = odds / (1.0 + odds);
        
        let is_same_device = confidence >= self.threshold;
        
        Ok(DeviceIdentityResult {
            is_same_device,
            confidence,
            evidence_summary,
        })
    }
}

/// Check if a UUID is meaningful (not a default/placeholder)
fn is_meaningful_uuid(uuid: &str) -> bool {
    // Common placeholder UUIDs
    let placeholders = [
        "00000000-0000-0000-0000-000000000000",
        "11111111-1111-1111-1111-111111111111",
        "ffffffff-ffff-ffff-ffff-ffffffffffff",
        "FFFFFFFF-FFFF-FFFF-FFFF-FFFFFFFFFFFF",
        "placeholder-uuid",
        "",
    ];
    
    !placeholders.contains(&uuid)
}

/// Check if a hardware ID is meaningful (not a default/placeholder)
fn is_meaningful_hardware_id(hw_id: &str) -> bool {
    // Common placeholder hardware IDs
    let placeholders = [
        "unknown",
        "default",
        "placeholder",
        "test-hw-id",
        "0000000000000000",
        "1111111111111111",
        "",
    ];
    
    !placeholders.contains(&hw_id) && hw_id.len() > 3
}

/// Extract device fingerprint from a rimsd directory path
pub async fn fingerprint_device(rimsd_path: &Path) -> Result<BasicFingerprint, FlowError> {
    let soradyne_device_id = extract_soradyne_device_id(rimsd_path).await?;
    let hardware_id = extract_hardware_id(rimsd_path).await?;
    let filesystem_uuid = extract_filesystem_uuid(rimsd_path).await?;
    let bad_blocks = extract_bad_blocks(rimsd_path).await?;
    let capacity = extract_capacity(rimsd_path).await?;
    
    Ok(BasicFingerprint::new(
        soradyne_device_id,
        hardware_id,
        filesystem_uuid,
        &bad_blocks,
        capacity,
    ))
}

/// Extract or create Soradyne device ID from .rimsd directory
async fn extract_soradyne_device_id(rimsd_path: &Path) -> Result<Option<String>, FlowError> {
    let device_id_file = rimsd_path.join("soradyne_device_id.txt");
    
    if device_id_file.exists() {
        // Read existing device ID
        let content = tokio::fs::read_to_string(&device_id_file).await.map_err(|e|
            FlowError::PersistenceError(format!("Failed to read device ID: {}", e))
        )?;
        Ok(Some(content.trim().to_string()))
    } else {
        // Create new device ID
        let new_device_id = uuid::Uuid::new_v4().to_string();
        
        // Ensure rimsd directory exists
        tokio::fs::create_dir_all(rimsd_path).await.map_err(|e|
            FlowError::PersistenceError(format!("Failed to create rimsd directory: {}", e))
        )?;
        
        // Write device ID to file
        tokio::fs::write(&device_id_file, &new_device_id).await.map_err(|e|
            FlowError::PersistenceError(format!("Failed to write device ID: {}", e))
        )?;
        
        Ok(Some(new_device_id))
    }
}

async fn extract_hardware_id(rimsd_path: &Path) -> Result<Option<String>, FlowError> {
    // Try to get hardware ID from the device containing the rimsd path
    let _device_path = get_device_path(rimsd_path).await?;
    
    #[cfg(target_os = "linux")]
    {
        extract_hardware_id_linux(&_device_path).await
    }
    
    #[cfg(target_os = "macos")]
    {
        extract_hardware_id_macos(&_device_path).await
    }
    
    #[cfg(target_os = "windows")]
    {
        extract_hardware_id_windows(&_device_path).await
    }
    
    #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
    {
        Ok(None)
    }
}

async fn extract_filesystem_uuid(rimsd_path: &Path) -> Result<Option<String>, FlowError> {
    let _device_path = get_device_path(rimsd_path).await?;
    
    #[cfg(target_os = "linux")]
    {
        extract_filesystem_uuid_linux(&_device_path).await
    }
    
    #[cfg(target_os = "macos")]
    {
        extract_filesystem_uuid_macos(&_device_path).await
    }
    
    #[cfg(target_os = "windows")]
    {
        extract_filesystem_uuid_windows(&_device_path).await
    }
    
    #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
    {
        Ok(None)
    }
}

async fn extract_bad_blocks(rimsd_path: &Path) -> Result<Vec<u64>, FlowError> {
    let _device_path = get_device_path(rimsd_path).await?;
    
    #[cfg(target_os = "linux")]
    {
        extract_bad_blocks_linux(&_device_path).await
    }
    
    #[cfg(any(target_os = "macos", target_os = "windows"))]
    {
        // Bad block detection is more complex on macOS/Windows
        // For now, use filesystem-based heuristics
        extract_bad_blocks_heuristic(rimsd_path).await
    }
    
    #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
    {
        Ok(vec![])
    }
}

async fn extract_capacity(rimsd_path: &Path) -> Result<u64, FlowError> {
    let _device_path = get_device_path(rimsd_path).await?;
    
    #[cfg(target_os = "linux")]
    {
        extract_capacity_linux(&_device_path).await
    }
    
    #[cfg(target_os = "macos")]
    {
        extract_capacity_macos(&_device_path).await
    }
    
    #[cfg(target_os = "windows")]
    {
        extract_capacity_windows(&_device_path).await
    }
    
    #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
    {
        // Fallback: use filesystem stats
        extract_capacity_fallback(rimsd_path).await
    }
}

// === Platform-agnostic helpers ===

async fn get_device_path(rimsd_path: &Path) -> Result<String, FlowError> {
    // Find the mount point and device for this path
    let mut current_path = rimsd_path;
    
    // Walk up the directory tree to find the mount point
    while let Some(parent) = current_path.parent() {
        if is_mount_point(current_path).await? {
            break;
        }
        current_path = parent;
    }
    
    get_device_for_mount_point(current_path).await
}

async fn is_mount_point(path: &Path) -> Result<bool, FlowError> {
    #[cfg(unix)]
    {
        use std::os::unix::fs::MetadataExt;
        
        let metadata = tokio::fs::metadata(path).await.map_err(|e|
            FlowError::PersistenceError(format!("Failed to get metadata: {}", e))
        )?;
        
        if let Some(parent) = path.parent() {
            let parent_metadata = tokio::fs::metadata(parent).await.map_err(|e|
                FlowError::PersistenceError(format!("Failed to get parent metadata: {}", e))
            )?;
            
            // Different device IDs indicate a mount point
            Ok(metadata.dev() != parent_metadata.dev())
        } else {
            Ok(true) // Root is always a mount point
        }
    }
    
    #[cfg(windows)]
    {
        // On Windows, check if this is a drive root
        let path_str = path.to_string_lossy();
        Ok(path_str.len() == 3 && path_str.ends_with(":\\"))
    }
}

async fn get_device_for_mount_point(mount_point: &Path) -> Result<String, FlowError> {
    #[cfg(target_os = "linux")]
    {
        // Read /proc/mounts to find the device
        let mounts = tokio::fs::read_to_string("/proc/mounts").await.map_err(|e|
            FlowError::PersistenceError(format!("Failed to read /proc/mounts: {}", e))
        )?;
        
        let mount_point_str = mount_point.to_string_lossy();
        for line in mounts.lines() {
            let parts: Vec<&str> = line.split_whitespace().collect();
            if parts.len() >= 2 && parts[1] == mount_point_str {
                return Ok(parts[0].to_string());
            }
        }
        
        Err(FlowError::PersistenceError("Device not found in /proc/mounts".to_string()))
    }
    
    #[cfg(target_os = "macos")]
    {
        // Use diskutil to find the device
        let output = tokio::process::Command::new("diskutil")
            .args(&["info", &mount_point.to_string_lossy()])
            .output()
            .await
            .map_err(|e| FlowError::PersistenceError(format!("Failed to run diskutil: {}", e)))?;
        
        let output_str = String::from_utf8_lossy(&output.stdout);
        for line in output_str.lines() {
            if line.trim().starts_with("Device Node:") {
                if let Some(device) = line.split(':').nth(1) {
                    return Ok(device.trim().to_string());
                }
            }
        }
        
        Err(FlowError::PersistenceError("Device not found in diskutil output".to_string()))
    }
    
    #[cfg(target_os = "windows")]
    {
        // On Windows, the mount point is typically the drive letter
        Ok(mount_point.to_string_lossy().to_string())
    }
    
    #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
    {
        Ok("unknown".to_string())
    }
}

// === Linux implementations ===

#[cfg(target_os = "linux")]
async fn extract_hardware_id_linux(device_path: &str) -> Result<Option<String>, FlowError> {
    // Try to get hardware info from udev
    let output = tokio::process::Command::new("udevadm")
        .args(&["info", "--query=all", "--name", device_path])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run udevadm: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    let mut serial = None;
    let mut vendor = None;
    let mut model = None;
    
    for line in output_str.lines() {
        if line.contains("ID_SERIAL_SHORT=") {
            serial = line.split('=').nth(1).map(|s| s.to_string());
        } else if line.contains("ID_VENDOR=") {
            vendor = line.split('=').nth(1).map(|s| s.to_string());
        } else if line.contains("ID_MODEL=") {
            model = line.split('=').nth(1).map(|s| s.to_string());
        }
    }
    
    match (vendor, model, serial) {
        (Some(v), Some(m), Some(s)) => Ok(Some(format!("{}:{}:{}", v, m, s))),
        (Some(v), Some(m), None) => Ok(Some(format!("{}:{}", v, m))),
        _ => Ok(None),
    }
}

#[cfg(target_os = "linux")]
async fn extract_filesystem_uuid_linux(device_path: &str) -> Result<Option<String>, FlowError> {
    let output = tokio::process::Command::new("blkid")
        .args(&["-s", "UUID", "-o", "value", device_path])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run blkid: {}", e)))?;
    
    let uuid = String::from_utf8_lossy(&output.stdout).trim().to_string();
    if uuid.is_empty() {
        Ok(None)
    } else {
        Ok(Some(uuid))
    }
}

#[cfg(target_os = "linux")]
async fn extract_bad_blocks_linux(device_path: &str) -> Result<Vec<u64>, FlowError> {
    // Use badblocks to scan for bad blocks (read-only scan)
    let output = tokio::process::Command::new("badblocks")
        .args(&["-v", device_path])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run badblocks: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    let mut bad_blocks = Vec::new();
    
    for line in output_str.lines() {
        if let Ok(block_num) = line.trim().parse::<u64>() {
            bad_blocks.push(block_num);
        }
    }
    
    Ok(bad_blocks)
}

#[cfg(target_os = "linux")]
async fn extract_capacity_linux(device_path: &str) -> Result<u64, FlowError> {
    // Read the size from /sys/block/*/size
    let device_name = device_path.trim_start_matches("/dev/");
    let size_path = format!("/sys/block/{}/size", device_name);
    
    let size_str = tokio::fs::read_to_string(&size_path).await.map_err(|e|
        FlowError::PersistenceError(format!("Failed to read device size: {}", e))
    )?;
    
    let sectors = size_str.trim().parse::<u64>().map_err(|e|
        FlowError::PersistenceError(format!("Failed to parse device size: {}", e))
    )?;
    
    // Convert sectors to bytes (assuming 512-byte sectors)
    Ok(sectors * 512)
}

// === macOS implementations ===

#[cfg(target_os = "macos")]
async fn extract_hardware_id_macos(device_path: &str) -> Result<Option<String>, FlowError> {
    let output = tokio::process::Command::new("diskutil")
        .args(&["info", device_path])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run diskutil: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    let vendor: Option<String> = None;
    let mut model: Option<String> = None;
    
    for line in output_str.lines() {
        let line = line.trim();
        if line.starts_with("Device / Media Name:") {
            if let Some(name) = line.split(':').nth(1) {
                model = Some(name.trim().to_string());
            }
        } else if line.starts_with("Disk / Partition UUID:") {
            if let Some(uuid) = line.split(':').nth(1) {
                return Ok(Some(uuid.trim().to_string()));
            }
        }
    }
    
    match (vendor, model) {
        (Some(v), Some(m)) => Ok(Some(format!("{}:{}", v, m))),
        (None, Some(m)) => Ok(Some(m)),
        _ => Ok(None),
    }
}

#[cfg(target_os = "macos")]
async fn extract_filesystem_uuid_macos(device_path: &str) -> Result<Option<String>, FlowError> {
    let output = tokio::process::Command::new("diskutil")
        .args(&["info", device_path])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run diskutil: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    for line in output_str.lines() {
        let line = line.trim();
        if line.starts_with("Volume UUID:") {
            if let Some(uuid) = line.split(':').nth(1) {
                return Ok(Some(uuid.trim().to_string()));
            }
        }
    }
    
    Ok(None)
}

#[cfg(target_os = "macos")]
async fn extract_capacity_macos(device_path: &str) -> Result<u64, FlowError> {
    let output = tokio::process::Command::new("diskutil")
        .args(&["info", device_path])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run diskutil: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    for line in output_str.lines() {
        let line = line.trim();
        if line.starts_with("Disk Size:") {
            // Parse something like "Disk Size: 32.0 GB (32017047552 Bytes) (exactly 62533296 512-Byte-Units)"
            if let Some(bytes_part) = line.split('(').nth(1) {
                if let Some(bytes_str) = bytes_part.split(' ').next() {
                    if let Ok(bytes) = bytes_str.parse::<u64>() {
                        return Ok(bytes);
                    }
                }
            }
        }
    }
    
    Err(FlowError::PersistenceError("Could not parse disk size from diskutil".to_string()))
}

// === Windows implementations ===

#[cfg(target_os = "windows")]
async fn extract_hardware_id_windows(device_path: &str) -> Result<Option<String>, FlowError> {
    // Use wmic to get disk information
    let drive_letter = device_path.chars().next().unwrap_or('C');
    let output = tokio::process::Command::new("wmic")
        .args(&["logicaldisk", "where", &format!("DeviceID='{}':", drive_letter), "get", "VolumeSerialNumber", "/value"])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run wmic: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    for line in output_str.lines() {
        if line.starts_with("VolumeSerialNumber=") {
            if let Some(serial) = line.split('=').nth(1) {
                let serial = serial.trim();
                if !serial.is_empty() {
                    return Ok(Some(serial.to_string()));
                }
            }
        }
    }
    
    Ok(None)
}

#[cfg(target_os = "windows")]
async fn extract_filesystem_uuid_windows(device_path: &str) -> Result<Option<String>, FlowError> {
    // Windows doesn't have UUIDs in the same way, use volume serial number
    extract_hardware_id_windows(device_path).await
}

#[cfg(target_os = "windows")]
async fn extract_capacity_windows(device_path: &str) -> Result<u64, FlowError> {
    let drive_letter = device_path.chars().next().unwrap_or('C');
    let output = tokio::process::Command::new("wmic")
        .args(&["logicaldisk", "where", &format!("DeviceID='{}':", drive_letter), "get", "Size", "/value"])
        .output()
        .await
        .map_err(|e| FlowError::PersistenceError(format!("Failed to run wmic: {}", e)))?;
    
    let output_str = String::from_utf8_lossy(&output.stdout);
    for line in output_str.lines() {
        if line.starts_with("Size=") {
            if let Some(size_str) = line.split('=').nth(1) {
                if let Ok(size) = size_str.trim().parse::<u64>() {
                    return Ok(size);
                }
            }
        }
    }
    
    Err(FlowError::PersistenceError("Could not parse disk size from wmic".to_string()))
}

// === Fallback implementations ===

async fn extract_bad_blocks_heuristic(rimsd_path: &Path) -> Result<Vec<u64>, FlowError> {
    // Heuristic: look for filesystem errors or read errors
    // This is a simplified approach - real bad block detection requires low-level access
    
    // Try to read some test files and see if we get I/O errors
    let test_dir = rimsd_path.join("soradyne_test");
    tokio::fs::create_dir_all(&test_dir).await.ok();
    
    let mut bad_blocks = Vec::new();
    
    // Write and read test patterns to detect bad areas
    for i in 0..10 {
        let test_file = test_dir.join(format!("test_{}.dat", i));
        let test_data = vec![0xAA; 4096]; // Test pattern
        
        if tokio::fs::write(&test_file, &test_data).await.is_err() {
            bad_blocks.push(i as u64);
        } else if let Ok(read_data) = tokio::fs::read(&test_file).await {
            if read_data != test_data {
                bad_blocks.push(i as u64);
            }
        }
        
        tokio::fs::remove_file(&test_file).await.ok();
    }
    
    tokio::fs::remove_dir(&test_dir).await.ok();
    Ok(bad_blocks)
}

async fn extract_capacity_fallback(rimsd_path: &Path) -> Result<u64, FlowError> {
    // Use filesystem stats as fallback
    let metadata = tokio::fs::metadata(rimsd_path).await.map_err(|e|
        FlowError::PersistenceError(format!("Failed to get filesystem metadata: {}", e))
    )?;
    
    // This is not the true device capacity, but filesystem available space
    // It's better than nothing for fingerprinting purposes
    Ok(metadata.len())
}

/// Discover all Soradyne-initialized volumes on the system
pub async fn discover_soradyne_volumes() -> Result<Vec<PathBuf>, FlowError> {
    let mut rimsd_dirs = Vec::new();
    
    // Get all mounted volumes/drives
    let mount_points = get_system_mount_points().await?;
    
    for mount_point in mount_points {
        let potential_rimsd = mount_point.join(".rimsd");
        
        if potential_rimsd.exists() && potential_rimsd.is_dir() {
            // Verify it's a valid Soradyne volume by checking for key files
            if is_valid_soradyne_volume(&potential_rimsd).await? {
                rimsd_dirs.push(potential_rimsd);
                println!("Found Soradyne volume: {:?}", mount_point);
            }
        }
    }
    
    Ok(rimsd_dirs)
}

async fn get_system_mount_points() -> Result<Vec<PathBuf>, FlowError> {
    let mut mount_points = Vec::new();
    
    #[cfg(target_os = "macos")]
    {
        // Check /Volumes for mounted drives
        if let Ok(entries) = fs::read_dir("/Volumes") {
            for entry in entries.flatten() {
                mount_points.push(entry.path());
            }
        }
    }
    
    #[cfg(target_os = "linux")]
    {
        // Check /media and /mnt for mounted drives
        for base_dir in &["/media", "/mnt"] {
            if let Ok(entries) = fs::read_dir(base_dir) {
                for entry in entries.flatten() {
                    mount_points.push(entry.path());
                }
            }
        }
        
        // Also check /run/media for user mounts
        if let Ok(entries) = fs::read_dir("/run/media") {
            for user_dir in entries.flatten() {
                if let Ok(user_entries) = fs::read_dir(user_dir.path()) {
                    for entry in user_entries.flatten() {
                        mount_points.push(entry.path());
                    }
                }
            }
        }
    }
    
    #[cfg(target_os = "windows")]
    {
        // Check all drive letters
        for letter in 'A'..='Z' {
            let drive_path = PathBuf::from(format!("{}:\\", letter));
            if drive_path.exists() {
                mount_points.push(drive_path);
            }
        }
    }
    
    Ok(mount_points)
}

async fn is_valid_soradyne_volume(rimsd_path: &PathBuf) -> Result<bool, FlowError> {
    // Check for essential Soradyne files
    let soradyne_device_id = rimsd_path.join("soradyne_device_id.txt");
    
    Ok(soradyne_device_id.exists())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::io::{self, Write};
    
    #[test]
    fn test_basic_fingerprint_creation() {
        let fingerprint = BasicFingerprint::new(
            Some("soradyne-device-123".to_string()),
            Some("test-hw-id".to_string()),
            Some("test-fs-uuid".to_string()),
            &[1047, 2891, 4203],
            32 * 1024 * 1024 * 1024,
        );
        
        assert_eq!(fingerprint.soradyne_device_id, Some("soradyne-device-123".to_string()));
        assert_eq!(fingerprint.hardware_id, Some("test-hw-id".to_string()));
        assert_eq!(fingerprint.capacity_bytes, 32 * 1024 * 1024 * 1024);
        assert_ne!(fingerprint.bad_block_signature, 0);
    }
    
    #[test]
    fn test_bayesian_identification() {
        let identifier = BayesianDeviceIdentifier::default();
        
        let fingerprint1 = BasicFingerprint::new(
            Some("soradyne-device-456".to_string()),
            Some("hw123".to_string()),
            Some("fs-uuid-456".to_string()),
            &[100, 200, 300],
            1000000,
        );
        
        let fingerprint2 = fingerprint1.clone();
        
        let result = identifier.identify_device(&fingerprint1, &fingerprint2).unwrap();
        
        assert!(result.is_same_device);
        assert!(result.confidence > 0.95);
    }
    
    #[test]
    fn test_different_devices() {
        let identifier = BayesianDeviceIdentifier::default();
        
        let fingerprint1 = BasicFingerprint::new(
            Some("soradyne-device-123".to_string()),
            Some("hw123".to_string()),
            Some("fs-uuid-456".to_string()),
            &[100, 200, 300],
            1000000,
        );
        
        let fingerprint2 = BasicFingerprint::new(
            Some("soradyne-device-999".to_string()),  // Different Soradyne ID
            Some("hw999".to_string()),  // Different hardware
            Some("fs-uuid-999".to_string()),  // Different filesystem
            &[400, 500, 600],  // Different bad blocks
            2000000,  // Different capacity
        );
        
        let result = identifier.identify_device(&fingerprint1, &fingerprint2).unwrap();
        
        assert!(!result.is_same_device);
        assert!(result.confidence < 0.05);
    }
    
    #[tokio::test]
    async fn test_device_fingerprinting() {
        use tempfile::TempDir;
        
        // Create a temporary directory to simulate an SD card
        let temp_dir = TempDir::new().unwrap();
        let rimsd_path = temp_dir.path().join(".rimsd");
        
        // Test fingerprinting
        let fingerprint = fingerprint_device(&rimsd_path).await.unwrap();
        
        // Should have a Soradyne device ID (created automatically)
        assert!(fingerprint.soradyne_device_id.is_some());
        
        // Should have some capacity (even if fallback)
        assert!(fingerprint.capacity_bytes > 0);
        
        // Test that fingerprinting the same path gives the same Soradyne ID
        let fingerprint2 = fingerprint_device(&rimsd_path).await.unwrap();
        assert_eq!(fingerprint.soradyne_device_id, fingerprint2.soradyne_device_id);
        
        // Test device identification
        let identifier = BayesianDeviceIdentifier::default();
        let result = identifier.identify_device(&fingerprint, &fingerprint2).unwrap();
        
        assert!(result.is_same_device);
        assert!(result.confidence > 0.95);
    }
    
    #[tokio::test]
    #[ignore] // Run manually with: cargo test test_interactive_sd_card_verification -- --ignored
    async fn test_interactive_sd_card_verification() {
        println!("\n🔍 Interactive SD Card Device Identity Test");
        println!("==========================================");
        println!("This test will help you verify that SD card fingerprinting works correctly.");
        println!("You'll need to insert SD cards when prompted.\n");
        
        let mut stored_fingerprints: std::collections::HashMap<String, BasicFingerprint> = std::collections::HashMap::new();
        let identifier = BayesianDeviceIdentifier::default();
        
        loop {
            println!("Options:");
            println!("1. Initialize new SD card");
            println!("2. Verify existing SD card");
            println!("3. List stored fingerprints");
            println!("4. Exit");
            print!("Choose an option (1-4): ");
            io::stdout().flush().unwrap();
            
            let mut input = String::new();
            io::stdin().read_line(&mut input).unwrap();
            let choice = input.trim();
            
            match choice {
                "1" => {
                    println!("\n📱 Insert an SD card and enter its mount path:");
                    print!("Path (e.g., /Volumes/SDCARD or /media/sdcard): ");
                    io::stdout().flush().unwrap();
                    
                    let mut path_input = String::new();
                    io::stdin().read_line(&mut path_input).unwrap();
                    let rimsd_path = std::path::Path::new(path_input.trim()).join(".rimsd");
                    
                    println!("🔍 Fingerprinting SD card...");
                    match fingerprint_device(&rimsd_path).await {
                        Ok(fingerprint) => {
                            println!("✅ Successfully fingerprinted SD card!");
                            println!("   Soradyne ID: {:?}", fingerprint.soradyne_device_id);
                            println!("   Hardware ID: {:?}", fingerprint.hardware_id);
                            println!("   Filesystem UUID: {:?}", fingerprint.filesystem_uuid);
                            println!("   Capacity: {} GB", fingerprint.capacity_bytes / (1024 * 1024 * 1024));
                            println!("   Bad blocks: {} detected", if fingerprint.bad_block_signature == 0 { 0 } else { 1 });
                            
                            if let Some(soradyne_id) = &fingerprint.soradyne_device_id {
                                let device_id = soradyne_id.clone();
                                stored_fingerprints.insert(device_id.clone(), fingerprint);
                                println!("💾 Stored fingerprint for device: {}", device_id);
                            } else {
                                println!("⚠️  Warning: No Soradyne device ID found");
                            }
                        }
                        Err(e) => {
                            println!("❌ Failed to fingerprint SD card: {}", e);
                        }
                    }
                }
                
                "2" => {
                    if stored_fingerprints.is_empty() {
                        println!("❌ No stored fingerprints. Initialize an SD card first.");
                        continue;
                    }
                    
                    println!("\n📱 Insert an SD card to verify and enter its mount path:");
                    print!("Path: ");
                    io::stdout().flush().unwrap();
                    
                    let mut path_input = String::new();
                    io::stdin().read_line(&mut path_input).unwrap();
                    let rimsd_path = std::path::Path::new(path_input.trim()).join(".rimsd");
                    
                    println!("🔍 Fingerprinting SD card...");
                    match fingerprint_device(&rimsd_path).await {
                        Ok(current_fingerprint) => {
                            if let Some(soradyne_id) = &current_fingerprint.soradyne_device_id {
                                if let Some(stored_fingerprint) = stored_fingerprints.get(soradyne_id) {
                                    println!("🔍 Comparing with stored fingerprint...");
                                    
                                    match identifier.identify_device(&current_fingerprint, stored_fingerprint) {
                                        Ok(result) => {
                                            if result.is_same_device {
                                                println!("✅ MATCH: This is the same SD card!");
                                                println!("   Confidence: {:.2}%", result.confidence * 100.0);
                                                println!("   Evidence: {:?}", result.evidence_summary);
                                            } else {
                                                println!("❌ NO MATCH: This appears to be a different SD card!");
                                                println!("   Confidence: {:.2}%", result.confidence * 100.0);
                                                println!("   Evidence: {:?}", result.evidence_summary);
                                            }
                                        }
                                        Err(e) => {
                                            println!("❌ Failed to compare fingerprints: {}", e);
                                        }
                                    }
                                } else {
                                    println!("❌ No stored fingerprint found for Soradyne ID: {}", soradyne_id);
                                    println!("   This appears to be a new SD card.");
                                }
                            } else {
                                println!("❌ No Soradyne device ID found on this SD card");
                            }
                        }
                        Err(e) => {
                            println!("❌ Failed to fingerprint SD card: {}", e);
                        }
                    }
                }
                
                "3" => {
                    println!("\n📋 Stored Fingerprints:");
                    if stored_fingerprints.is_empty() {
                        println!("   (none)");
                    } else {
                        for (id, fingerprint) in &stored_fingerprints {
                            println!("   🔑 {}", id);
                            println!("      Hardware: {:?}", fingerprint.hardware_id);
                            println!("      Filesystem: {:?}", fingerprint.filesystem_uuid);
                            println!("      Capacity: {} GB", fingerprint.capacity_bytes / (1024 * 1024 * 1024));
                        }
                    }
                }
                
                "4" => {
                    println!("👋 Goodbye!");
                    break;
                }
                
                _ => {
                    println!("❌ Invalid option. Please choose 1-4.");
                }
            }
            
            println!();
        }
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/block_manager.rs
// =====================================================================

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;
use sha2::{Sha256, Digest};
use chrono::Utc;
use serde::{Serialize, Deserialize};

use crate::storage::block::*;
use crate::storage::device_identity::{BasicFingerprint, BayesianDeviceIdentifier, fingerprint_device};

#[derive(Debug, Clone)]
pub struct StorageInfo {
    pub total_devices: usize,
    pub threshold: usize,
    pub total_shards: usize,
    pub rimsd_paths: Vec<PathBuf>,
}

#[derive(Debug, Clone)]
pub struct BlockDistribution {
    pub block_id: [u8; 32],
    pub total_shards: usize,
    pub available_shards: Vec<ShardInfo>,
    pub missing_shards: Vec<usize>,
    pub can_reconstruct: bool,
    pub original_size: usize,
}

#[derive(Debug, Clone)]
pub struct ShardInfo {
    pub index: usize,
    pub device_path: String,
    pub file_path: String,
    pub size: u64,
}

#[derive(Debug, Clone)]
pub struct DemonstrationResult {
    pub original_shards: usize,
    pub simulated_missing: Vec<usize>,
    pub available_shards: usize,
    pub threshold_required: usize,
    pub recovery_successful: bool,
    pub recovered_data_size: usize,
}

const BLOCK_SIZE: usize = 32 * 1024 * 1024; // 32MB
use crate::storage::erasure::ShamirErasureEncoder;
use crate::flow::FlowError;

#[derive(Debug)]
pub struct BlockManager {
    rimsd_directories: Vec<PathBuf>,
    metadata_store: Arc<RwLock<BlockMetadataStore>>,
    erasure_encoder: ShamirErasureEncoder,
    threshold: usize,
    total_shards: usize,
    device_identifier: BayesianDeviceIdentifier,
    device_fingerprints: Arc<RwLock<HashMap<PathBuf, BasicFingerprint>>>,
}

#[derive(Debug)]
pub struct BlockMetadataStore {
    blocks: HashMap<[u8; 32], BlockMetadata>,
    metadata_path: PathBuf,
}

#[derive(Serialize, Deserialize)]
struct SerializableBlockStore {
    blocks: HashMap<String, BlockMetadata>,
}

impl BlockMetadataStore {
    pub fn load_or_create(metadata_path: PathBuf) -> Result<Self, FlowError> {
        let blocks = if metadata_path.exists() {
            let data = std::fs::read(&metadata_path).map_err(|e| 
                FlowError::PersistenceError(format!("Failed to read metadata: {}", e))
            )?;
            
            let serializable: SerializableBlockStore = serde_json::from_slice(&data).map_err(|e|
                FlowError::PersistenceError(format!("Failed to parse metadata: {}", e))
            )?;
            
            // Convert hex string keys back to [u8; 32]
            let mut blocks = HashMap::new();
            for (hex_key, metadata) in serializable.blocks {
                let block_id = hex::decode(&hex_key).map_err(|e|
                    FlowError::PersistenceError(format!("Invalid block ID in metadata: {}", e))
                )?;
                if block_id.len() != 32 {
                    return Err(FlowError::PersistenceError("Invalid block ID length".to_string()));
                }
                let mut id = [0u8; 32];
                id.copy_from_slice(&block_id);
                blocks.insert(id, metadata);
            }
            blocks
        } else {
            HashMap::new()
        };
        
        Ok(Self {
            blocks,
            metadata_path,
        })
    }
    
    pub fn add_block(&mut self, metadata: BlockMetadata) -> Result<(), FlowError> {
        self.blocks.insert(metadata.id, metadata);
        self.save()
    }
    
    pub fn get_block(&self, id: &[u8; 32]) -> Result<BlockMetadata, FlowError> {
        self.blocks.get(id)
            .cloned()
            .ok_or_else(|| FlowError::PersistenceError(
                format!("Block not found: {}", hex::encode(id))
            ))
    }
    
    fn save(&self) -> Result<(), FlowError> {
        // Convert [u8; 32] keys to hex strings for JSON serialization
        let mut serializable_blocks = HashMap::new();
        for (block_id, metadata) in &self.blocks {
            let hex_key = hex::encode(block_id);
            serializable_blocks.insert(hex_key, metadata.clone());
        }
        
        let serializable = SerializableBlockStore {
            blocks: serializable_blocks,
        };
        
        let data = serde_json::to_vec_pretty(&serializable).map_err(|e|
            FlowError::PersistenceError(format!("Failed to serialize metadata: {}", e))
        )?;
        
        std::fs::write(&self.metadata_path, data).map_err(|e|
            FlowError::PersistenceError(format!("Failed to write metadata: {}", e))
        )
    }
}

impl BlockManager {
    pub fn new(
        rimsd_directories: Vec<PathBuf>, 
        metadata_path: PathBuf,
        threshold: usize,
        total_shards: usize,
    ) -> Result<Self, FlowError> {
        // Ensure all rimsd directories exist
        for dir in &rimsd_directories {
            std::fs::create_dir_all(dir).map_err(|e| 
                FlowError::PersistenceError(format!("Failed to create rimsd directory: {}", e))
            )?;
        }
        
        let metadata_store = BlockMetadataStore::load_or_create(metadata_path)?;
        
        let erasure_encoder = ShamirErasureEncoder::new(threshold, total_shards)?;
        
        Ok(Self {
            rimsd_directories,
            metadata_store: Arc::new(RwLock::new(metadata_store)),
            erasure_encoder,
            threshold,
            total_shards,
            device_identifier: BayesianDeviceIdentifier::default(),
            device_fingerprints: Arc::new(RwLock::new(HashMap::new())),
        })
    }
    
    /// Create a new BlockManager by discovering Soradyne volumes automatically
    pub async fn new_with_discovery(
        metadata_path: PathBuf,
        threshold: usize,
        total_shards: usize,
    ) -> Result<Self, FlowError> {
        let rimsd_dirs = crate::storage::device_identity::discover_soradyne_volumes().await?;
        
        if rimsd_dirs.is_empty() {
            return Err(FlowError::PersistenceError(
                "No Soradyne volumes found. Please initialize some SD cards first.".to_string()
            ));
        }
        
        println!("Found {} Soradyne volumes", rimsd_dirs.len());
        
        Self::new(rimsd_dirs, metadata_path, threshold, total_shards)
    }
    
    /// Get information about the current storage configuration
    pub fn get_storage_info(&self) -> StorageInfo {
        StorageInfo {
            total_devices: self.rimsd_directories.len(),
            threshold: self.threshold,
            total_shards: self.total_shards,
            rimsd_paths: self.rimsd_directories.clone(),
        }
    }
    
    /// Get detailed information about block distribution
    pub async fn get_block_distribution(&self, block_id: &[u8; 32]) -> Result<BlockDistribution, FlowError> {
        let metadata = self.metadata_store.read().await.get_block(block_id)?;
        
        let mut available_shards = Vec::new();
        let mut missing_shards = Vec::new();
        
        for location in &metadata.shard_locations {
            let shard_path = PathBuf::from(&location.rimsd_path)
                .join(&location.relative_path);
            
            if shard_path.exists() {
                available_shards.push(ShardInfo {
                    index: location.shard_index,
                    device_path: location.rimsd_path.clone(),
                    file_path: shard_path.to_string_lossy().to_string(),
                    size: tokio::fs::metadata(&shard_path).await
                        .map(|m| m.len())
                        .unwrap_or(0),
                });
            } else {
                missing_shards.push(location.shard_index);
            }
        }
        
        let can_reconstruct = available_shards.len() >= self.threshold;
        
        Ok(BlockDistribution {
            block_id: *block_id,
            total_shards: metadata.shard_locations.len(),
            available_shards,
            missing_shards,
            can_reconstruct,
            original_size: metadata.size,
        })
    }
    
    /// List all blocks in the metadata store
    pub async fn list_blocks(&self) -> Vec<([u8; 32], BlockMetadata)> {
        let store = self.metadata_store.read().await;
        store.blocks.iter().map(|(id, meta)| (*id, meta.clone())).collect()
    }
    
    /// Demonstrate erasure coding by intentionally "removing" some shards
    pub async fn demonstrate_erasure_recovery(&self, block_id: &[u8; 32], shards_to_simulate_missing: Vec<usize>) -> Result<DemonstrationResult, FlowError> {
        self.demonstrate_fault_tolerance(block_id, shards_to_simulate_missing).await
    }
    
    /// Demonstrate fault tolerance by intentionally "removing" some shards
    pub async fn demonstrate_fault_tolerance(&self, block_id: &[u8; 32], shards_to_simulate_missing: Vec<usize>) -> Result<DemonstrationResult, FlowError> {
        let metadata = self.metadata_store.read().await.get_block(block_id)?;
        
        // Collect available shards, excluding the ones we're simulating as missing
        let mut available_shards = HashMap::new();
        
        for location in &metadata.shard_locations {
            if shards_to_simulate_missing.contains(&location.shard_index) {
                continue; // Simulate this shard as missing
            }
            
            let shard_path = PathBuf::from(&location.rimsd_path)
                .join(&location.relative_path);
            
            if shard_path.exists() {
                let shard_data = tokio::fs::read(&shard_path).await.map_err(|e|
                    FlowError::PersistenceError(format!("Failed to read shard: {}", e))
                )?;
                available_shards.insert(location.shard_index, shard_data);
            }
        }
        
        let available_shards_count = available_shards.len();
        let can_recover = available_shards_count >= self.threshold;
        let recovery_result = if can_recover {
            match self.erasure_encoder.decode(available_shards, metadata.size) {
                Ok(data) => Some(data),
                Err(_) => None,
            }
        } else {
            None
        };
        
        Ok(DemonstrationResult {
            original_shards: metadata.shard_locations.len(),
            simulated_missing: shards_to_simulate_missing,
            available_shards: available_shards_count,
            threshold_required: self.threshold,
            recovery_successful: recovery_result.is_some(),
            recovered_data_size: recovery_result.as_ref().map(|d| d.len()).unwrap_or(0),
        })
    }
    
    pub async fn write_direct_block(&self, data: &[u8]) -> Result<[u8; 32], FlowError> {
        if data.len() > BLOCK_SIZE {
            return Err(FlowError::PersistenceError(
                format!("Data size {} exceeds block size {}", data.len(), BLOCK_SIZE)
            ));
        }
        
        let id = self.generate_block_id();
        let nonce = ShamirErasureEncoder::derive_nonce(&id);
        
        let metadata = BlockMetadata {
            id,
            directness: 0,
            size: data.len(),
            created_at: Utc::now(),
            modified_at: Utc::now(),
            shard_locations: Vec::new(),
            encryption_version: 1, // New Shamir+RS format
            nonce,
        };
        
        // Shamir + Reed-Solomon encode the data
        let shards_with_keys = self.erasure_encoder.encode(data, &id)?;
        
        // Distribute shards across rimsd directories
        println!("📦 Distributing {} encrypted shards across {} devices:", shards_with_keys.len(), self.rimsd_directories.len());
        let mut shard_locations = Vec::new();
        let mut successful_writes = 0;
        let mut failed_writes = 0;
        
        for (i, shard_with_key) in shards_with_keys.iter().enumerate() {
            let rimsd_dir = &self.rimsd_directories[i % self.rimsd_directories.len()].as_path();
            let shard_path = self.shard_path(rimsd_dir, &id, i);
            let key_share_path = self.key_share_path(rimsd_dir, &id, i);
            
            println!("   📝 Writing shard {} ({} bytes) + key share → {}", 
                i, shard_with_key.shard_data.len(), shard_path.display());
            
            // Try to write shard and key share, but don't fail the entire operation if one device fails
            let write_result = async {
                // Write shard data to disk
                if let Some(parent) = shard_path.parent() {
                    tokio::fs::create_dir_all(parent).await.map_err(|e|
                        format!("Failed to create shard directory: {}", e)
                    )?;
                }
                tokio::fs::write(&shard_path, &shard_with_key.shard_data).await.map_err(|e|
                    format!("Failed to write shard: {}", e)
                )?;
                
                // Write key share to disk
                if let Some(parent) = key_share_path.parent() {
                    tokio::fs::create_dir_all(parent).await.map_err(|e|
                        format!("Failed to create key share directory: {}", e)
                    )?;
                }
                let key_share_data = serde_json::to_vec(&shard_with_key.key_share).map_err(|e|
                    format!("Failed to serialize key share: {}", e)
                )?;
                tokio::fs::write(&key_share_path, &key_share_data).await.map_err(|e|
                    format!("Failed to write key share: {}", e)
                )?;
                
                Ok::<(), String>(())
            }.await;
            
            match write_result {
                Ok(()) => {
                    println!("   ✅ Shard {} and key share written successfully", i);
                    successful_writes += 1;
                    
                    shard_locations.push(ShardLocation {
                        shard_index: i,
                        device_id: self.get_device_id(),
                        rimsd_path: rimsd_dir.to_string_lossy().to_string(),
                        relative_path: shard_path.strip_prefix(rimsd_dir)
                            .unwrap()
                            .to_string_lossy()
                            .to_string(),
                        key_share_path: Some(key_share_path.strip_prefix(rimsd_dir)
                            .unwrap()
                            .to_string_lossy()
                            .to_string()),
                    });
                }
                Err(e) => {
                    println!("   ⚠️  Failed to write shard {} to {}: {}", i, rimsd_dir.display(), e);
                    println!("      Continuing with remaining devices...");
                    failed_writes += 1;
                }
            }
        }
        
        // Check if we have enough successful writes to meet the threshold
        if successful_writes < self.threshold {
            println!("❌ Insufficient successful writes: {} < {} required", successful_writes, self.threshold);
            println!("   Available devices: {}, Failed writes: {}", self.rimsd_directories.len(), failed_writes);
            return Err(FlowError::PersistenceError(
                format!("Failed to write enough shards: only {} of {} required shards written (need {} minimum)", 
                        successful_writes, self.threshold, self.threshold)
            ));
        }
        
        if failed_writes > 0 {
            println!("⚠️  {} shards distributed successfully with {} failures", successful_writes, failed_writes);
            println!("   System remains operational with {} available devices", successful_writes);
        } else {
            println!("🎯 All encrypted shards and key shares distributed successfully!");
        }
        
        // Update metadata
        let mut metadata = metadata;
        metadata.shard_locations = shard_locations;
        
        // Save metadata (non-critical - continue even if this fails)
        if let Err(e) = self.metadata_store.write().await.add_block(metadata) {
            println!("⚠️  Failed to save metadata: {}", e);
            println!("   Block storage operation completed successfully despite metadata write failure");
            // Don't return error - the block was successfully distributed
        } else {
            println!("📝 Metadata saved successfully");
        }
        
        Ok(id)
    }
    
    pub async fn read_block(&self, id: &[u8; 32]) -> Result<Vec<u8>, FlowError> {
        let metadata = self.metadata_store.read().await.get_block(id)?;
        
        if metadata.directness == 0 {
            // Direct block - reconstruct from shards
            self.read_direct_block(&metadata).await
        } else {
            // Indirect block - read addresses and recursively read blocks
            Box::pin(self.read_indirect_block(&metadata)).await
        }
    }
    
    async fn read_direct_block(&self, metadata: &BlockMetadata) -> Result<Vec<u8>, FlowError> {
        println!("📖 Reading block with {} total shards (need {} minimum):", 
            metadata.shard_locations.len(), self.threshold);
        
        // Check encryption version for backward compatibility
        if metadata.encryption_version == 0 {
            return self.read_legacy_block(metadata).await;
        }
        
        // Collect available shards and key shares
        let mut shards_with_keys = HashMap::new();
        let mut missing_shards = Vec::new();
        
        for location in &metadata.shard_locations {
            let shard_path = PathBuf::from(&location.rimsd_path)
                .join(&location.relative_path);
            
            let key_share_path = if let Some(key_path) = &location.key_share_path {
                PathBuf::from(&location.rimsd_path).join(key_path)
            } else {
                // Fallback to legacy format
                return self.read_legacy_block(metadata).await;
            };
            
            if shard_path.exists() && key_share_path.exists() {
                match (tokio::fs::read(&shard_path).await, tokio::fs::read(&key_share_path).await) {
                    (Ok(shard_data), Ok(key_share_data)) => {
                        match serde_json::from_slice(&key_share_data) {
                            Ok(key_share) => {
                                println!("   ✅ Read shard {} ({} bytes) + key share ← {}", 
                                    location.shard_index, shard_data.len(), shard_path.display());
                                
                                shards_with_keys.insert(location.shard_index, crate::storage::erasure::ShardWithKey {
                                    shard_data,
                                    key_share,
                                });
                            }
                            Err(e) => {
                                println!("   ❌ Failed to parse key share {}: {}", location.shard_index, e);
                                missing_shards.push(location.shard_index);
                            }
                        }
                    }
                    (Err(e), _) => {
                        println!("   ❌ Failed to read shard {} from {}: {}", 
                            location.shard_index, shard_path.display(), e);
                        missing_shards.push(location.shard_index);
                    }
                    (_, Err(e)) => {
                        println!("   ❌ Failed to read key share {} from {}: {}", 
                            location.shard_index, key_share_path.display(), e);
                        missing_shards.push(location.shard_index);
                    }
                }
            } else {
                println!("   ⚠️  Shard {} or key share missing: {} / {}", 
                    location.shard_index, shard_path.display(), key_share_path.display());
                missing_shards.push(location.shard_index);
            }
        }
        
        println!("📊 Shard status: {} available, {} missing", shards_with_keys.len(), missing_shards.len());
        
        // Check if we have enough shards
        if shards_with_keys.len() < self.threshold {
            return Err(FlowError::PersistenceError(
                format!("Not enough shards available: {} < {} (missing: {:?})", 
                    shards_with_keys.len(), self.threshold, missing_shards)
            ));
        }
        
        println!("🔍 Reconstruction Diagnostics:");
        println!("   Total shards: {}", shards_with_keys.len());
        println!("   Block ID: {}", hex::encode(&metadata.id));
        println!("   Expected size: {}", metadata.size);
        println!("   Encryption version: {}", metadata.encryption_version);
        
        // Log shard details
        for (i, (shard_index, shard_with_key)) in shards_with_keys.iter().enumerate() {
            println!("   Shard {}: {} bytes, Key Share: {} bytes", 
                shard_index, 
                shard_with_key.shard_data.len(), 
                serde_json::to_vec(&shard_with_key.key_share).map(|v| v.len()).unwrap_or(0)
            );
        }

        println!("🔧 Reconstructing data from {} shards using Shamir + Reed-Solomon...", shards_with_keys.len());
        
        // Validate shard sizes are consistent
        let shard_sizes: Vec<usize> = shards_with_keys.values().map(|shard| shard.shard_data.len()).collect();
        if let (Some(min_size), Some(max_size)) = (shard_sizes.iter().min(), shard_sizes.iter().max()) {
            if min_size != max_size {
                println!("❌ Inconsistent shard sizes: min={}, max={}, all={:?}", min_size, max_size, shard_sizes);
                return Err(FlowError::PersistenceError("Inconsistent shard sizes".to_string()));
            }
        }

        // Create streaming decoder and reconstruct
        let mut decoder = self.erasure_encoder
            .decode_with_streaming(shards_with_keys, &metadata.id, metadata.size)
            .map_err(|e| {
                println!("❌ Failed to create streaming decoder: {}", e);
                e
            })?;

        let result = decoder.reconstruct_all().await.map_err(|e| {
            println!("❌ Failed to reconstruct data: {}", e);
            e
        })?;
        
        println!("✅ Successfully reconstructed {} bytes of encrypted data", result.len());
        
        Ok(result)
    }
    
    /// Read legacy RS-only blocks for backward compatibility
    async fn read_legacy_block(&self, metadata: &BlockMetadata) -> Result<Vec<u8>, FlowError> {
        println!("📖 Reading legacy RS-only block...");
        
        let mut shards = HashMap::new();
        let mut missing_shards = Vec::new();
        
        for location in &metadata.shard_locations {
            let shard_path = PathBuf::from(&location.rimsd_path)
                .join(&location.relative_path);
            
            if shard_path.exists() {
                match tokio::fs::read(&shard_path).await {
                    Ok(shard_data) => {
                        println!("   ✅ Read legacy shard {} ({} bytes) ← {}", 
                            location.shard_index, shard_data.len(), shard_path.display());
                        shards.insert(location.shard_index, shard_data);
                    }
                    Err(e) => {
                        println!("   ❌ Failed to read legacy shard {} from {}: {}", 
                            location.shard_index, shard_path.display(), e);
                        missing_shards.push(location.shard_index);
                    }
                }
            } else {
                println!("   ⚠️  Legacy shard {} missing: {}", 
                    location.shard_index, shard_path.display());
                missing_shards.push(location.shard_index);
            }
        }
        
        if shards.len() < self.threshold {
            return Err(FlowError::PersistenceError(
                format!("Not enough legacy shards available: {} < {}", shards.len(), self.threshold)
            ));
        }
        
        // Use legacy decode method
        let result = self.erasure_encoder.decode(shards, metadata.size)?;
        println!("✅ Successfully reconstructed {} bytes from legacy block", result.len());
        
        Ok(result)
    }
    
    async fn read_indirect_block(&self, metadata: &BlockMetadata) -> Result<Vec<u8>, FlowError> {
        // First read the indirect block itself to get addresses
        let addresses_data = self.read_direct_block(metadata).await?;
        let addresses = self.parse_addresses(&addresses_data)?;
        
        // Read all referenced blocks
        let mut result = Vec::new();
        for address in addresses {
            let block_data = Box::pin(self.read_block(&address)).await?;
            result.extend_from_slice(&block_data);
        }
        
        Ok(result)
    }
    
    fn generate_block_id(&self) -> [u8; 32] {
        // Use a cryptographic hash of UUID + timestamp
        let mut hasher = Sha256::new();
        hasher.update(Uuid::new_v4().as_bytes());
        hasher.update(Utc::now().timestamp_nanos_opt().unwrap_or(0).to_le_bytes());
        let result = hasher.finalize();
        let mut id = [0u8; 32];
        id.copy_from_slice(&result);
        id
    }
    
    fn shard_path(&self, rimsd_dir: &Path, block_id: &[u8; 32], shard_index: usize) -> PathBuf {
        // Use first 4 bytes of block ID for directory structure
        let hex_id = hex::encode(block_id);
        rimsd_dir
            .join(&hex_id[..2])
            .join(&hex_id[2..4])
            .join(format!("{}.{}.shard", hex_id, shard_index))
    }
    
    fn key_share_path(&self, rimsd_dir: &Path, block_id: &[u8; 32], shard_index: usize) -> PathBuf {
        // Store key shares alongside shards
        let hex_id = hex::encode(block_id);
        rimsd_dir
            .join(&hex_id[..2])
            .join(&hex_id[2..4])
            .join(format!("{}.{}.keyshare", hex_id, shard_index))
    }
    
    fn get_device_id(&self) -> Uuid {
        // TODO: Get actual device ID from identity manager
        Uuid::new_v4()
    }
    
    fn parse_addresses(&self, data: &[u8]) -> Result<Vec<[u8; 32]>, FlowError> {
        if data.len() % 32 != 0 {
            return Err(FlowError::PersistenceError(
                "Invalid indirect block data".to_string()
            ));
        }
        
        let mut addresses = Vec::new();
        for chunk in data.chunks_exact(32) {
            let mut id = [0u8; 32];
            id.copy_from_slice(chunk);
            addresses.push(id);
        }
        
        Ok(addresses)
    }
    
    /// Verify device identity for all rimsd directories
    pub async fn verify_device_continuity(&self) -> Result<(), FlowError> {
        for rimsd_dir in &self.rimsd_directories {
            self.verify_single_device(rimsd_dir).await?;
        }
        Ok(())
    }
    
    /// Verify identity of a single device
    pub async fn verify_single_device(&self, rimsd_dir: &Path) -> Result<(), FlowError> {
        let current_fingerprint = fingerprint_device(rimsd_dir).await?;
        
        let fingerprints = self.device_fingerprints.read().await;
        if let Some(previous_fingerprint) = fingerprints.get(&rimsd_dir.to_path_buf()) {
            // Check if this could be a legitimate evolution
            if !current_fingerprint.is_valid_evolution(previous_fingerprint)? {
                return Err(FlowError::PersistenceError(
                    format!("Device identity validation failed for {}: incompatible evolution", 
                           rimsd_dir.display())
                ));
            }
            
            // Run Bayesian identification
            let result = self.device_identifier.identify_device(
                &current_fingerprint, 
                previous_fingerprint
            )?;
            
            if !result.is_same_device {
                return Err(FlowError::PersistenceError(
                    format!("Device identity mismatch for {}: confidence {:.2}%, evidence: {:?}", 
                           rimsd_dir.display(), 
                           result.confidence * 100.0,
                           result.evidence_summary)
                ));
            }
        }
        
        // Update stored fingerprint
        drop(fingerprints);
        let mut fingerprints = self.device_fingerprints.write().await;
        fingerprints.insert(rimsd_dir.to_path_buf(), current_fingerprint);
        
        Ok(())
    }
    
    /// Initialize device fingerprints for all rimsd directories
    pub async fn initialize_device_fingerprints(&self) -> Result<(), FlowError> {
        for rimsd_dir in &self.rimsd_directories {
            let fingerprint = fingerprint_device(rimsd_dir).await?;
            let mut fingerprints = self.device_fingerprints.write().await;
            fingerprints.insert(rimsd_dir.to_path_buf(), fingerprint);
        }
        Ok(())
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/mod.rs
// =====================================================================

//! Storage subsystem for Soradyne
//! 
//! This module provides dissolution storage capabilities with multiple backend
//! implementations including manual erasure coding and bcachefs.

pub mod block;
pub mod block_file;
pub mod block_manager;
pub mod device_identity;
pub mod erasure;
pub mod galois;
pub mod local_file;

// New abstraction layer
pub mod dissolution;
pub mod backends;
pub mod examples;

// Legacy exports
pub use local_file::LocalFileStorage;
pub use local_file::NoOpAuthenticator;
pub use block_manager::{BlockManager, StorageInfo, BlockDistribution, ShardInfo, DemonstrationResult};
pub use block_file::BlockFile;
pub use device_identity::{BasicFingerprint, BayesianDeviceIdentifier, fingerprint_device, discover_soradyne_volumes};

// New abstraction exports
pub use dissolution::{
    DissolutionStorage, DissolutionConfig, DissolutionFile, BlockId,
    BlockInfo, StorageStats, DissolutionDemo
};
pub use backends::{DissolutionStorageFactory, SdynErasureBackend};

#[cfg(target_os = "linux")]
pub use backends::BcacheFSBackend;


// =====================================================================
// FILE: packages/soradyne_core/src/storage/local_file.rs
// =====================================================================

use std::fs::{self, File};
use std::io::{Read, Write};
use std::path::{Path, PathBuf};
use uuid::Uuid;

use crate::flow::FlowError;
use crate::flow::traits::StorageBackend;

/// A storage backend that persists data to the local filesystem
pub struct LocalFileStorage {
    /// Base directory for storing flow data
    base_dir: PathBuf,
}

impl LocalFileStorage {
    /// Create a new local file storage with the specified base directory
    pub fn new<P: AsRef<Path>>(base_dir: P) -> Result<Self, FlowError> {
        let path = PathBuf::from(base_dir.as_ref());
        
        // Create the directory if it doesn't exist
        if !path.exists() {
            fs::create_dir_all(&path).map_err(|e| {
                FlowError::PersistenceError(format!("Failed to create directory: {}", e))
            })?;
        }
        
        Ok(Self { base_dir: path })
    }
    
    /// Get the file path for a specific flow ID
    fn get_file_path(&self, flow_id: Uuid) -> PathBuf {
        self.base_dir.join(format!("{}.json", flow_id))
    }
}

impl StorageBackend for LocalFileStorage {
    fn store(&self, flow_id: Uuid, data: &[u8]) -> Result<(), FlowError> {
        let file_path = self.get_file_path(flow_id);
        
        let mut file = File::create(&file_path).map_err(|e| {
            FlowError::PersistenceError(format!("Failed to create file: {}", e))
        })?;
        
        file.write_all(data).map_err(|e| {
            FlowError::PersistenceError(format!("Failed to write data: {}", e))
        })?;
        
        Ok(())
    }
    
    fn load(&self, flow_id: Uuid) -> Result<Vec<u8>, FlowError> {
        let file_path = self.get_file_path(flow_id);
        
        if !file_path.exists() {
            return Err(FlowError::PersistenceError(format!(
                "Flow data not found for ID: {}", flow_id
            )));
        }
        
        let mut file = File::open(&file_path).map_err(|e| {
            FlowError::PersistenceError(format!("Failed to open file: {}", e))
        })?;
        
        let mut buffer = Vec::new();
        file.read_to_end(&mut buffer).map_err(|e| {
            FlowError::PersistenceError(format!("Failed to read data: {}", e))
        })?;
        
        Ok(buffer)
    }
    
    fn exists(&self, flow_id: Uuid) -> bool {
        self.get_file_path(flow_id).exists()
    }
    
    fn delete(&self, flow_id: Uuid) -> Result<(), FlowError> {
        let file_path = self.get_file_path(flow_id);
        
        if file_path.exists() {
            fs::remove_file(&file_path).map_err(|e| {
                FlowError::PersistenceError(format!("Failed to delete file: {}", e))
            })?;
        }
        
        Ok(())
    }
}

/// A no-op implementation of FlowAuthenticator that doesn't actually perform any
/// cryptographic operations. Useful as a placeholder until real authentication is implemented.
pub struct NoOpAuthenticator;

impl<T> crate::flow::traits::FlowAuthenticator<T> for NoOpAuthenticator {
    fn sign(&self, _data: &T) -> Result<Vec<u8>, FlowError> {
        // Return a dummy signature
        Ok(vec![0, 1, 2, 3])
    }
    
    fn verify(&self, _data: &T, _signature: &[u8]) -> bool {
        // Always verify as true
        true
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/galois.rs
// =====================================================================

//! Galois Field GF(256) arithmetic operations
//! 
//! This module implements arithmetic in the Galois Field GF(2^8) using the
//! irreducible polynomial x^8 + x^4 + x^3 + x + 1 (0x11b).

/// Galois Field GF(256) implementation
#[derive(Debug, Clone)]
pub struct GF256 {
    /// Precomputed logarithm table for fast multiplication
    log_table: [u8; 256],
    /// Precomputed antilog table for fast multiplication (extended for wraparound)
    antilog_table: [u8; 512],
}

impl GF256 {
    /// Create a new GF(256) instance with precomputed tables
    pub fn new() -> Self {
        let mut gf = Self {
            log_table: [0; 256],
            antilog_table: [0; 512],
        };
        gf.build_tables();
        gf
    }
    
    /// Build logarithm and antilog tables for fast multiplication
    fn build_tables(&mut self) {
        const PRIMITIVE: u8 = 0x03;  // Correct primitive element
        const POLY: u16 = 0x11b;     // Full polynomial for multiplication
        
        let mut value: u16 = 1;
        for i in 0..255 {
            self.antilog_table[i] = value as u8;
            self.log_table[value as usize] = i as u8;
            value = Self::multiply_no_table(value as u8, PRIMITIVE, POLY) as u16;
        }
        
        // Copy for wraparound - this allows safe indexing beyond 255
        for i in 255..512 {
            self.antilog_table[i] = self.antilog_table[i - 255];
        }
        
        // Special case: log(0) is undefined, but we set it to 0 for convenience
        self.log_table[0] = 0;
    }
    
    /// Multiply two elements without using lookup tables (for table construction)
    fn multiply_no_table(a: u8, b: u8, poly: u16) -> u8 {
        let mut result: u16 = 0;
        let mut a = a as u16;
        let mut b = b as u16;
        
        for _ in 0..8 {
            if b & 1 != 0 {
                result ^= a;
            }
            let carry = a & 0x80;
            a <<= 1;
            if carry != 0 {
                a ^= poly;
            }
            b >>= 1;
        }
        (result & 0xFF) as u8
    }
    
    /// Addition in GF(256) - same as XOR
    pub fn add(&self, a: u8, b: u8) -> u8 {
        a ^ b
    }
    
    /// Subtraction in GF(256) - same as XOR (since -x = x in GF(2^n))
    pub fn subtract(&self, a: u8, b: u8) -> u8 {
        a ^ b
    }
    
    /// Multiplication in GF(256) using log/antilog tables
    pub fn multiply(&self, a: u8, b: u8) -> u8 {
        if a == 0 || b == 0 {
            return 0;
        }
        
        let log_a = self.log_table[a as usize] as usize;
        let log_b = self.log_table[b as usize] as usize;
        
        self.antilog_table[(log_a + log_b) % 255]
    }
    
    /// Division in GF(256) - multiplication by multiplicative inverse
    pub fn divide(&self, a: u8, b: u8) -> Result<u8, &'static str> {
        if b == 0 {
            return Err("Division by zero in GF(256)");
        }
        if a == 0 {
            return Ok(0);
        }
        
        let log_a = self.log_table[a as usize] as i16;
        let log_b = self.log_table[b as usize] as i16;
        let log_result = (255 + log_a - log_b) % 255;
        
        Ok(self.antilog_table[log_result as usize])
    }
    
    /// Multiplicative inverse in GF(256)
    pub fn inverse(&self, a: u8) -> Result<u8, &'static str> {
        if a == 0 {
            return Err("Zero has no multiplicative inverse");
        }
        
        let log_a = self.log_table[a as usize] as usize;
        let log_inverse = (255 - log_a) % 255;
        
        Ok(self.antilog_table[log_inverse])
    }
    
    /// Power operation in GF(256)
    pub fn power(&self, base: u8, exponent: u8) -> u8 {
        if base == 0 {
            return if exponent == 0 { 1 } else { 0 };
        }
        if exponent == 0 {
            return 1;
        }
        
        let log_base = self.log_table[base as usize] as usize;
        let log_result = (log_base * exponent as usize) % 255;
        
        self.antilog_table[log_result]
    }
    
    /// Evaluate polynomial at given point
    /// poly[0] + poly[1]*x + poly[2]*x^2 + ... + poly[n]*x^n
    pub fn eval_polynomial(&self, poly: &[u8], x: u8) -> u8 {
        if poly.is_empty() {
            return 0;
        }
        
        let mut result = poly[0];
        let mut x_power = x;
        
        for &coeff in &poly[1..] {
            result = self.add(result, self.multiply(coeff, x_power));
            x_power = self.multiply(x_power, x);
        }
        
        result
    }
    
    /// Lagrange interpolation to find polynomial value at x=0
    /// Given points (x_i, y_i), compute the polynomial value at 0
    pub fn lagrange_interpolate_at_zero(&self, points: &[(u8, u8)]) -> Result<u8, &'static str> {
        if points.is_empty() {
            return Ok(0);
        }
        
        let mut result = 0u8;
        
        for (i, &(xi, yi)) in points.iter().enumerate() {
            // Calculate Lagrange basis polynomial L_i(0)
            let mut numerator = 1u8;
            let mut denominator = 1u8;
            
            for (j, &(xj, _)) in points.iter().enumerate() {
                if i != j {
                    // For L_i(0): numerator *= (0 - xj) = xj (since -xj = xj in GF(2^n))
                    // denominator *= (xi - xj)
                    numerator = self.multiply(numerator, xj);
                    denominator = self.multiply(denominator, self.subtract(xi, xj));
                }
            }
            
            // Calculate yi * L_i(0) = yi * (numerator / denominator)
            let lagrange_coeff = self.divide(numerator, denominator)?;
            let term = self.multiply(yi, lagrange_coeff);
            result = self.add(result, term);
        }
        
        Ok(result)
    }
}

impl Default for GF256 {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_gf256_basic_operations() {
        let gf = GF256::new();
        
        // Test addition (XOR)
        assert_eq!(gf.add(0x53, 0xCA), 0x99);
        assert_eq!(gf.add(0xFF, 0xFF), 0x00);
        assert_eq!(gf.add(0x00, 0xFF), 0xFF);
        
        // Test subtraction (same as addition in GF(2^n))
        assert_eq!(gf.subtract(0x53, 0xCA), 0x99);
        assert_eq!(gf.subtract(0xFF, 0xFF), 0x00);
    }
    
    #[test]
    fn test_gf256_multiplication() {
        let gf = GF256::new();
        
        // Test basic multiplication
        assert_eq!(gf.multiply(0, 5), 0);
        assert_eq!(gf.multiply(5, 0), 0);
        assert_eq!(gf.multiply(1, 5), 5);
        assert_eq!(gf.multiply(5, 1), 5);
        
        // Test specific known values
        assert_eq!(gf.multiply(2, 2), 4);
        assert_eq!(gf.multiply(2, 3), 6);
        assert_eq!(gf.multiply(3, 3), 5); // 3*3 = 9 = x^3 + 1, reduced mod x^8+x^4+x^3+x+1
        
        // Test commutativity
        for a in 1..=10 {
            for b in 1..=10 {
                assert_eq!(gf.multiply(a, b), gf.multiply(b, a));
            }
        }
    }
    
    #[test]
    fn test_gf256_inverse() {
        let gf = GF256::new();
        
        // Test that a * a^(-1) = 1 for all non-zero elements
        for a in 1..=255u8 {
            let inv_a = gf.inverse(a).unwrap();
            assert_eq!(gf.multiply(a, inv_a), 1, "Failed for a={}", a);
        }
        
        // Test that 0 has no inverse
        assert!(gf.inverse(0).is_err());
    }
    
    #[test]
    fn test_gf256_division() {
        let gf = GF256::new();
        
        // Test basic division
        assert_eq!(gf.divide(0, 5).unwrap(), 0);
        assert_eq!(gf.divide(5, 1).unwrap(), 5);
        assert_eq!(gf.divide(6, 2).unwrap(), 3);
        
        // Test division by zero
        assert!(gf.divide(5, 0).is_err());
        
        // Test that (a * b) / b = a for all non-zero b
        for a in 1..=10 {
            for b in 1..=10 {
                let product = gf.multiply(a, b);
                let quotient = gf.divide(product, b).unwrap();
                assert_eq!(quotient, a, "Failed for a={}, b={}", a, b);
            }
        }
    }
    
    #[test]
    fn test_gf256_power() {
        let gf = GF256::new();
        
        // Test basic powers
        assert_eq!(gf.power(2, 0), 1);
        assert_eq!(gf.power(2, 1), 2);
        assert_eq!(gf.power(2, 2), 4);
        assert_eq!(gf.power(2, 3), 8);
        
        // Test that 0^0 = 1 and 0^n = 0 for n > 0
        assert_eq!(gf.power(0, 0), 1);
        assert_eq!(gf.power(0, 5), 0);
        
        // Test that a^1 = a
        for a in 1..=10 {
            assert_eq!(gf.power(a, 1), a);
        }
    }
    
    #[test]
    fn test_polynomial_evaluation() {
        let gf = GF256::new();
        
        // Test constant polynomial
        let poly = vec![5];
        assert_eq!(gf.eval_polynomial(&poly, 10), 5);
        
        // Test linear polynomial: 3 + 2x
        let poly = vec![3, 2];
        assert_eq!(gf.eval_polynomial(&poly, 0), 3);
        assert_eq!(gf.eval_polynomial(&poly, 1), gf.add(3, 2)); // 3 + 2*1
        assert_eq!(gf.eval_polynomial(&poly, 5), gf.add(3, gf.multiply(2, 5))); // 3 + 2*5
        
        // Test quadratic polynomial: 1 + 2x + 3x^2
        let poly = vec![1, 2, 3];
        let x = 4;
        let expected = gf.add(gf.add(1, gf.multiply(2, x)), gf.multiply(3, gf.multiply(x, x)));
        assert_eq!(gf.eval_polynomial(&poly, x), expected);
    }
    
    #[test]
    fn test_lagrange_interpolation() {
        let gf = GF256::new();
        
        // Test with simple points that should give a constant polynomial
        let points = vec![(1, 5), (2, 5), (3, 5)];
        let result = gf.lagrange_interpolate_at_zero(&points).unwrap();
        assert_eq!(result, 5);
        
        // Test with points from a linear polynomial f(x) = 3 + 2x
        // f(1) = 5, f(2) = 7^2 = 1 (in GF), f(3) = 3^2^3 = 9^3 = ...
        let points = vec![
            (1, gf.add(3, gf.multiply(2, 1))),
            (2, gf.add(3, gf.multiply(2, 2))),
            (3, gf.add(3, gf.multiply(2, 3))),
        ];
        let result = gf.lagrange_interpolate_at_zero(&points).unwrap();
        assert_eq!(result, 3); // Should recover the constant term
        
        // Test with known polynomial coefficients
        let secret = 42u8;
        let poly = vec![secret, 17, 23]; // f(x) = 42 + 17x + 23x^2
        
        // Generate points
        let points = vec![
            (1, gf.eval_polynomial(&poly, 1)),
            (2, gf.eval_polynomial(&poly, 2)),
            (3, gf.eval_polynomial(&poly, 3)),
        ];
        
        // Interpolate back to get f(0) = secret
        let recovered = gf.lagrange_interpolate_at_zero(&points).unwrap();
        assert_eq!(recovered, secret);
    }
    
    #[test]
    fn test_shamir_secret_sharing_simulation() {
        let gf = GF256::new();
        let secret = 123u8;
        let threshold = 3;
        let total_shares = 5;
        
        // Generate random polynomial coefficients
        let mut poly = vec![secret]; // f(0) = secret
        for _ in 1..threshold {
            poly.push(42); // Use fixed coefficients for reproducible test
        }
        
        // Generate shares
        let mut shares = Vec::new();
        for i in 1..=total_shares {
            let x = i as u8;
            let y = gf.eval_polynomial(&poly, x);
            shares.push((x, y));
        }
        
        // Test reconstruction with exactly threshold shares
        let points = &shares[..threshold];
        let recovered = gf.lagrange_interpolate_at_zero(points).unwrap();
        assert_eq!(recovered, secret);
        
        // Test reconstruction with different subset
        let points = vec![shares[0], shares[2], shares[4]];
        let recovered = gf.lagrange_interpolate_at_zero(&points).unwrap();
        assert_eq!(recovered, secret);
        
        // Test that we can't reconstruct with insufficient shares
        let points = &shares[..threshold-1];
        // This should still work mathematically, but won't give the right answer
        // in a real Shamir scheme due to insufficient constraints
        let _recovered = gf.lagrange_interpolate_at_zero(points).unwrap();
        // We don't assert equality here because it's expected to be wrong
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/block.rs
// =====================================================================

//! Block storage data structures

use serde::{Serialize, Deserialize};
use uuid::Uuid;
use chrono::{DateTime, Utc};

pub const BLOCK_SIZE: usize = 32 * 1024 * 1024; // 32MB
pub const CHUNK_SIZE: usize = 64 * 1024; // 64KB chunks for streaming
pub const BLOCK_ID_SIZE: usize = 32;

pub type BlockId = [u8; BLOCK_ID_SIZE];

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct BlockMetadata {
    pub id: [u8; 32],
    pub directness: u8,
    pub size: usize,
    pub created_at: DateTime<Utc>,
    pub modified_at: DateTime<Utc>,
    pub shard_locations: Vec<ShardLocation>,
    #[serde(default)]
    pub encryption_version: u8, // 0 = legacy RS-only, 1 = Shamir+RS
    #[serde(default)]
    pub nonce: [u8; 12], // AES-GCM nonce derived from block ID
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ShardLocation {
    pub shard_index: usize,
    pub device_id: Uuid,
    pub rimsd_path: String,
    pub relative_path: String,
    #[serde(default)]
    pub key_share_path: Option<String>, // Path to Shamir key share (for v1+ blocks)
}

#[derive(Debug)]
pub enum Block {
    Direct(DirectBlock),
    Indirect(IndirectBlock),
}

#[derive(Debug)]
pub struct DirectBlock {
    pub metadata: BlockMetadata,
    pub data: Vec<u8>,
}

#[derive(Debug)]
pub struct IndirectBlock {
    pub metadata: BlockMetadata,
    pub addresses: Vec<[u8; 32]>,
}

impl Block {
    pub fn directness(&self) -> u8 {
        match self {
            Block::Direct(b) => b.metadata.directness,
            Block::Indirect(b) => b.metadata.directness,
        }
    }
    
    pub fn id(&self) -> &[u8; 32] {
        match self {
            Block::Direct(b) => &b.metadata.id,
            Block::Indirect(b) => &b.metadata.id,
        }
    }
    
    pub fn size(&self) -> usize {
        match self {
            Block::Direct(b) => b.metadata.size,
            Block::Indirect(b) => b.metadata.size,
        }
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/erasure.rs
// =====================================================================

//! Shamir Secret Sharing + Reed-Solomon erasure coding for secure fault tolerance

use crate::flow::FlowError;
use crate::storage::block::{CHUNK_SIZE, BlockId};
use reed_solomon_erasure::galois_8::ReedSolomon;
use std::collections::HashMap;
use rand::Rng;
use aes_gcm::{Aes256Gcm, Key, Nonce, KeyInit, AeadInPlace};
use sha2::{Sha256, Digest};
use std::sync::Arc;
use tokio::sync::Mutex;
use serde::{Serialize, Deserialize};

// Re-export for compatibility
pub use ShamirErasureEncoder as ErasureEncoder;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct KeyShare {
    pub index: u8,
    pub value: Vec<u8>,
}

#[derive(Debug, Clone)]
pub struct ShardWithKey {
    pub shard_data: Vec<u8>,
    pub key_share: KeyShare,
}

#[derive(Debug)]
pub struct ShamirErasureEncoder {
    threshold: usize,
    total_shards: usize,
    reed_solomon: ReedSolomon,
}

impl ShamirErasureEncoder {
    pub fn new(threshold: usize, total_shards: usize) -> Result<Self, FlowError> {
        if threshold == 0 || total_shards == 0 {
            return Err(FlowError::PersistenceError(
                "Threshold and total_shards must be greater than 0".to_string()
            ));
        }
        
        if threshold > total_shards {
            return Err(FlowError::PersistenceError(
                "Threshold cannot be greater than total_shards".to_string()
            ));
        }
        
        if total_shards > 255 {
            return Err(FlowError::PersistenceError(
                "Total shards cannot exceed 255 (Shamir limitation)".to_string()
            ));
        }
        
        let parity_shards = total_shards - threshold;
        
        let reed_solomon = ReedSolomon::new(threshold, parity_shards)
            .map_err(|e| FlowError::PersistenceError(
                format!("Failed to create Reed-Solomon encoder: {}", e)
            ))?;
        
        Ok(Self {
            threshold,
            total_shards,
            reed_solomon,
        })
    }
    
    /// Generate a nonce from block ID for consistent encryption
    pub fn derive_nonce(block_id: &BlockId) -> [u8; 12] {
        let mut hasher = Sha256::new();
        hasher.update(b"SORADYNE_NONCE_V1");
        hasher.update(block_id);
        let hash = hasher.finalize();
        let mut nonce = [0u8; 12];
        nonce.copy_from_slice(&hash[..12]);
        nonce
    }
    
    /// Derive a chunk-specific key from master key and chunk index
    fn derive_chunk_key(master_key: &[u8; 32], chunk_index: usize, block_id: &BlockId) -> [u8; 32] {
        let mut hasher = Sha256::new();
        hasher.update(b"SORADYNE_CHUNK_KEY_V1");
        hasher.update(master_key);
        hasher.update(&chunk_index.to_le_bytes());
        hasher.update(block_id);
        let hash = hasher.finalize();
        let mut key = [0u8; 32];
        key.copy_from_slice(&hash[..32]);
        key
    }
    
    /// Derive a chunk-specific nonce from block ID and chunk index
    fn derive_chunk_nonce(block_id: &BlockId, chunk_index: usize) -> [u8; 12] {
        let mut hasher = Sha256::new();
        hasher.update(b"SORADYNE_CHUNK_NONCE_V1");
        hasher.update(block_id);
        hasher.update(&chunk_index.to_le_bytes());
        let hash = hasher.finalize();
        let mut nonce = [0u8; 12];
        nonce.copy_from_slice(&hash[..12]);
        nonce
    }
    
    /// Encode data with Shamir encryption scheme
    pub fn encode(&self, data: &[u8], block_id: &BlockId) -> Result<Vec<ShardWithKey>, FlowError> {
        if data.is_empty() {
            return Ok(vec![]);
        }
        
        // 1. Generate a new master secret key (32 bytes for AES-256)
        let mut rng = rand::thread_rng();
        let master_key: [u8; 32] = rng.gen();
        
        // 2. Encrypt the block data chunk by chunk
        let encrypted_data = self.encrypt_data_chunked(data, &master_key, block_id)?;
        
        // 3. Shamir secret share the master key
        let key_shares = self.split_secret(&master_key)?;
        
        // 4. Reed-Solomon encode the encrypted data (without length prefix here)
        let rs_shards = self.encode_rs_raw(&encrypted_data)?;
        
        // 5. Combine RS shards with key shares
        let mut result = Vec::new();
        for (i, rs_shard) in rs_shards.into_iter().enumerate() {
            result.push(ShardWithKey {
                shard_data: rs_shard,
                key_share: key_shares[i].clone(),
            });
        }
        
        Ok(result)
    }
    
    /// Decode data with streaming capability for early reads
    pub fn decode_with_streaming(&self, shards: HashMap<usize, ShardWithKey>, block_id: &BlockId, expected_size: usize) -> Result<StreamingDecoder, FlowError> {
        if shards.len() < self.threshold {
            return Err(FlowError::PersistenceError(
                format!("Not enough shards: {} < {}", shards.len(), self.threshold)
            ));
        }
        
        println!("🔧 Setting up decoding with {} shards", shards.len());
        
        // Extract key shares and reconstruct the master encryption key
        // IMPORTANT: Sort by Shamir index to ensure correct Lagrange interpolation
        let mut key_shares: Vec<KeyShare> = shards.values()
            .map(|s| s.key_share.clone())
            .collect();
        
        println!("🔧 Key shares before sorting:");
        for (i, share) in key_shares.iter().enumerate() {
            println!("   Share {}: index={}", i, share.index);
        }
        
        // Sort by Shamir index to ensure deterministic reconstruction
        key_shares.sort_by_key(|share| share.index);
        
        println!("🔧 Key shares after sorting:");
        for (i, share) in key_shares.iter().enumerate() {
            println!("   Share {}: index={}", i, share.index);
        }
        
        let master_key = self.reconstruct_secret(&key_shares[..self.threshold])?;
        
        // Prepare for streaming RS reconstruction
        let rs_shards: HashMap<usize, Vec<u8>> = shards.into_iter()
            .map(|(i, shard)| (i, shard.shard_data))
            .collect();
        
        Ok(StreamingDecoder::new(
            rs_shards,
            master_key,
            *block_id,
            expected_size,
            self.threshold,
            self.total_shards,
        ))
    }
    
    /// Legacy decode method for compatibility
    pub fn decode(&self, shards: HashMap<usize, Vec<u8>>, expected_size: usize) -> Result<Vec<u8>, FlowError> {
        // This is for backward compatibility with old RS-only blocks
        // We'll implement this as a fallback for migration
        self.decode_rs_only(shards, expected_size)
    }
    
    /// Reed-Solomon encoding without length prefix (for encrypted data)
    fn encode_rs_raw(&self, data: &[u8]) -> Result<Vec<Vec<u8>>, FlowError> {
        println!("🔧 Reed-Solomon encoding {} bytes with threshold={}, total_shards={}", 
                 data.len(), self.threshold, self.total_shards);
        
        let shard_size = (data.len() + self.threshold - 1) / self.threshold;
        let padded_size = shard_size * self.threshold;
        
        println!("🔧 Shard size: {}, padded size: {}", shard_size, padded_size);
        
        let mut padded_data = data.to_vec();
        padded_data.resize(padded_size, 0);
        
        let mut shards: Vec<Vec<u8>> = Vec::with_capacity(self.total_shards);
        
        // Create data shards
        for i in 0..self.threshold {
            let start = i * shard_size;
            let end = start + shard_size;
            shards.push(padded_data[start..end].to_vec());
        }
        
        // Create empty parity shards
        let parity_count = self.total_shards - self.threshold;
        for _ in 0..parity_count {
            shards.push(vec![0u8; shard_size]);
        }
        
        // Generate parity shards
        self.reed_solomon.encode(&mut shards)
            .map_err(|e| FlowError::PersistenceError(
                format!("Reed-Solomon encoding failed: {}", e)
            ))?;
        
        println!("🔧 Reed-Solomon encoding complete: {} shards of {} bytes each", 
                 shards.len(), shard_size);
        
        Ok(shards)
    }
    
    /// Traditional Reed-Solomon encoding with length prefix (for legacy compatibility)
    fn encode_rs(&self, data: &[u8]) -> Result<Vec<Vec<u8>>, FlowError> {
        // Store the original length at the beginning for proper truncation
        let mut length_prefixed_data = Vec::new();
        length_prefixed_data.extend_from_slice(&(data.len() as u32).to_le_bytes());
        length_prefixed_data.extend_from_slice(data);
        
        self.encode_rs_raw(&length_prefixed_data)
    }
    
    /// Legacy RS-only decode for backward compatibility
    fn decode_rs_only(&self, shards: HashMap<usize, Vec<u8>>, expected_size: usize) -> Result<Vec<u8>, FlowError> {
        if shards.len() < self.threshold {
            return Err(FlowError::PersistenceError(
                format!("Not enough shards: {} < {}", shards.len(), self.threshold)
            ));
        }
        
        let shard_size = shards.values().next()
            .ok_or_else(|| FlowError::PersistenceError("No shards available".to_string()))?
            .len();
        
        let mut reconstruction_shards: Vec<Option<Vec<u8>>> = vec![None; self.total_shards];
        
        for (index, shard_data) in shards {
            if index >= self.total_shards {
                return Err(FlowError::PersistenceError(
                    format!("Invalid shard index: {} >= {}", index, self.total_shards)
                ));
            }
            
            if shard_data.len() != shard_size {
                return Err(FlowError::PersistenceError(
                    format!("Shard size mismatch: expected {}, got {}", shard_size, shard_data.len())
                ));
            }
            
            reconstruction_shards[index] = Some(shard_data);
        }
        
        self.reed_solomon.reconstruct(&mut reconstruction_shards)
            .map_err(|e| FlowError::PersistenceError(
                format!("Reed-Solomon reconstruction failed: {}", e)
            ))?;
        
        // Extract the original length from the first 4 bytes
        let mut full_encrypted_data = Vec::new();
        for i in 0..self.threshold {
            if let Some(ref shard) = reconstruction_shards[i] {
                full_encrypted_data.extend_from_slice(shard);
            } else {
                return Err(FlowError::PersistenceError(
                    "Failed to reconstruct data shard".to_string()
                ));
            }
        }
        
        if full_encrypted_data.len() < 4 {
            return Err(FlowError::PersistenceError(
                "Reconstructed data too small to contain length prefix".to_string()
            ));
        }
        
        let original_length = u32::from_le_bytes([
            full_encrypted_data[0],
            full_encrypted_data[1], 
            full_encrypted_data[2],
            full_encrypted_data[3]
        ]) as usize;
        
        println!("🔧 Extracted original length: {} bytes", original_length);
        
        // Remove the length prefix
        let result = full_encrypted_data[4..].to_vec();
        
        // Truncate to original length
        let mut result = result;
        result.truncate(original_length);
        
        println!("🔧 After truncation: {} bytes", result.len());
        Ok(result)
    }
    
    fn encrypt_data_chunked(&self, data: &[u8], master_key: &[u8; 32], block_id: &BlockId) -> Result<Vec<u8>, FlowError> {
        println!("🔐 Encrypting {} bytes with master key[0..8]: {:02x?}", data.len(), &master_key[..8]);
        
        let mut result = Vec::new();
        
        for (chunk_index, chunk) in data.chunks(CHUNK_SIZE).enumerate() {
            let chunk_key = Self::derive_chunk_key(master_key, chunk_index, block_id);
            println!("🔐 Chunk {}: size={}, key[0..8]={:02x?}", chunk_index, chunk.len(), &chunk_key[..8]);
            
            let cipher = Aes256Gcm::new(Key::<Aes256Gcm>::from_slice(&chunk_key));
            
            // Derive unique nonce for each chunk
            let chunk_nonce = Self::derive_chunk_nonce(block_id, chunk_index);
            let nonce = Nonce::from_slice(&chunk_nonce);
            
            println!("🔐 Chunk {} nonce: {:02x?}", chunk_index, &chunk_nonce);
            
            let mut ciphertext = chunk.to_vec();
            let tag = cipher.encrypt_in_place_detached(nonce, b"", &mut ciphertext)
                .map_err(|e| FlowError::PersistenceError(
                    format!("Encryption failed for chunk {}: {}", chunk_index, e)
                ))?;
            
            println!("🔐 Chunk {} encrypted: tag={:02x?}, ciphertext[0..8]={:02x?}", 
                     chunk_index, &tag[..8], &ciphertext[..8.min(ciphertext.len())]);
            
            // Store tag + ciphertext for this chunk
            result.extend_from_slice(&tag);
            result.extend_from_slice(&ciphertext);
        }
        
        println!("🔐 Total encrypted size: {} bytes", result.len());
        Ok(result)
    }
    
    fn split_secret(&self, secret: &[u8; 32]) -> Result<Vec<KeyShare>, FlowError> {
        use crate::storage::galois::GF256;
        
        let gf = GF256::new();
        let mut shares = Vec::new();
        let mut rng = rand::thread_rng();
        
        // Generate random coefficients for the polynomial
        // f(x) = secret + a1*x + a2*x^2 + ... + a(k-1)*x^(k-1)
        let mut coefficients = vec![0u8; (self.threshold - 1) * 32];
        rng.fill(&mut coefficients[..]);
        
        for i in 1..=self.total_shards {
            let mut share_value = vec![0u8; 32];
            let x = i as u8;
            
            // Evaluate polynomial at point x for each byte
            for byte_idx in 0..32 {
                // Build polynomial for this byte: [secret_byte, coeff1, coeff2, ...]
                let mut poly = vec![secret[byte_idx]];
                for coeff_idx in 0..(self.threshold - 1) {
                    poly.push(coefficients[coeff_idx * 32 + byte_idx]);
                }
                
                share_value[byte_idx] = gf.eval_polynomial(&poly, x);
            }
            
            shares.push(KeyShare {
                index: x,
                value: share_value,
            });
        }
        
        Ok(shares)
    }
    
    fn reconstruct_secret(&self, shares: &[KeyShare]) -> Result<[u8; 32], FlowError> {
        use crate::storage::galois::GF256;
        
        if shares.len() < self.threshold {
            return Err(FlowError::PersistenceError(
                format!("Not enough key shares: {} < {}", shares.len(), self.threshold)
            ));
        }
        
        println!("🔑 Reconstructing secret from {} shares (threshold: {})", shares.len(), self.threshold);
        for (i, share) in shares.iter().enumerate() {
            println!("   Share {}: index={}, value[0..8]={:02x?}", i, share.index, &share.value[..8]);
        }
        
        let gf = GF256::new();
        let mut secret = [0u8; 32];
        
        // Use Lagrange interpolation to find f(0) = secret for each byte
        for byte_idx in 0..32 {
            // Collect points (x_i, y_i) for this byte
            let points: Vec<(u8, u8)> = shares[..self.threshold]
                .iter()
                .map(|share| (share.index, share.value[byte_idx]))
                .collect();
            
            if byte_idx < 8 { // Only debug first 8 bytes to avoid spam
                println!("   Byte {}: points={:?}", byte_idx, points);
            }
            
            secret[byte_idx] = gf.lagrange_interpolate_at_zero(&points)
                .map_err(|e| FlowError::PersistenceError(
                    format!("Lagrange interpolation failed for byte {}: {}", byte_idx, e)
                ))?;
        }
        
        println!("🔑 Reconstructed master key[0..8]: {:02x?}", &secret[..8]);
        Ok(secret)
    }
    
    
    /// Calculate storage overhead factor
    pub fn storage_overhead(&self) -> f64 {
        self.total_shards as f64 / self.threshold as f64
    }
    
    /// Calculate how many shards can be lost while still being able to reconstruct
    pub fn fault_tolerance(&self) -> usize {
        self.total_shards - self.threshold
    }
}

/// Streaming decoder for progressive reconstruction with read-ahead
pub struct StreamingDecoder {
    rs_shards: HashMap<usize, Vec<u8>>,
    master_key: [u8; 32],
    block_id: BlockId,
    expected_size: usize,
    threshold: usize,
    total_shards: usize,
    position: usize,
    chunk_cache: Arc<Mutex<HashMap<usize, Vec<u8>>>>,
    read_ahead_tasks: Vec<tokio::task::JoinHandle<()>>,
}

impl StreamingDecoder {
    fn new(
        rs_shards: HashMap<usize, Vec<u8>>,
        master_key: [u8; 32],
        block_id: BlockId,
        expected_size: usize,
        threshold: usize,
        total_shards: usize,
    ) -> Self {
        Self {
            rs_shards,
            master_key,
            block_id,
            expected_size,
            threshold,
            total_shards,
            position: 0,
            chunk_cache: Arc::new(Mutex::new(HashMap::new())),
            read_ahead_tasks: Vec::new(),
        }
    }
    
    /// Read next chunk and return decrypted data with read-ahead
    pub async fn read_chunk(&mut self) -> Result<Option<Vec<u8>>, FlowError> {
        let chunk_index = self.position / CHUNK_SIZE;
        let total_chunks = (self.expected_size + CHUNK_SIZE - 1) / CHUNK_SIZE;
        
        if chunk_index >= total_chunks {
            return Ok(None);
        }
        
        // Check cache first
        {
            let cache = self.chunk_cache.lock().await;
            if let Some(chunk_data) = cache.get(&chunk_index) {
                self.position += chunk_data.len();
                return Ok(Some(chunk_data.clone()));
            }
        }
        
        // Start read-ahead for next few chunks
        self.start_read_ahead(chunk_index, total_chunks).await;
        
        // Reconstruct current chunk
        let chunk_data = self.reconstruct_chunk(chunk_index).await?;
        self.position += chunk_data.len();
        
        Ok(Some(chunk_data))
    }
    
    async fn start_read_ahead(&mut self, current_chunk: usize, total_chunks: usize) {
        const READ_AHEAD_COUNT: usize = 4; // Read ahead 4 chunks
        
        for i in 1..=READ_AHEAD_COUNT {
            let chunk_index = current_chunk + i;
            if chunk_index >= total_chunks {
                break;
            }
            
            // Check if already cached or being processed
            {
                let cache = self.chunk_cache.lock().await;
                if cache.contains_key(&chunk_index) {
                    continue;
                }
            }
            
            // Start async reconstruction
            let rs_shards = self.rs_shards.clone();
            let master_key = self.master_key;
            let block_id = self.block_id;
            let threshold = self.threshold;
            let total_shards = self.total_shards;
            let expected_size = self.expected_size;
            let cache = Arc::clone(&self.chunk_cache);
            
            let task = tokio::spawn(async move {
                if let Ok(chunk_data) = Self::reconstruct_chunk_static(
                    &rs_shards, master_key, block_id, chunk_index, threshold, total_shards, expected_size
                ).await {
                    let mut cache = cache.lock().await;
                    cache.insert(chunk_index, chunk_data);
                }
            });
            
            self.read_ahead_tasks.push(task);
        }
    }
    
    async fn reconstruct_chunk(&self, chunk_index: usize) -> Result<Vec<u8>, FlowError> {
        Self::reconstruct_chunk_static(
            &self.rs_shards,
            self.master_key,
            self.block_id,
            chunk_index,
            self.threshold,
            self.total_shards,
            self.expected_size,
        ).await
    }
    
    async fn reconstruct_chunk_static(
        rs_shards: &HashMap<usize, Vec<u8>>,
        master_key: [u8; 32],
        block_id: BlockId,
        chunk_index: usize,
        threshold: usize,
        total_shards: usize,
        expected_size: usize,
    ) -> Result<Vec<u8>, FlowError> {
        let _shard_size = rs_shards.values().next()
            .ok_or_else(|| FlowError::PersistenceError("No shards available".to_string()))?
            .len();
        
        // Reconstruct all shards
        let mut reconstruction_shards: Vec<Option<Vec<u8>>> = vec![None; total_shards];
        
        for (index, shard_data) in rs_shards {
            if *index < total_shards {
                reconstruction_shards[*index] = Some(shard_data.clone());
            }
        }
        
        // Reconstruct using Reed-Solomon
        let reed_solomon = ReedSolomon::new(threshold, total_shards - threshold)
            .map_err(|e| FlowError::PersistenceError(
                format!("Failed to create Reed-Solomon decoder: {}", e)
            ))?;
        
        reed_solomon.reconstruct(&mut reconstruction_shards)
            .map_err(|e| FlowError::PersistenceError(
                format!("Reed-Solomon reconstruction failed: {}", e)
            ))?;
        
        // Concatenate the data shards to get the full encrypted data
        let mut full_encrypted_data = Vec::new();
        for i in 0..threshold {
            if let Some(ref shard) = reconstruction_shards[i] {
                full_encrypted_data.extend_from_slice(shard);
            }
        }
        
        // CRITICAL FIX: Calculate the actual encrypted size and remove Reed-Solomon padding
        let num_chunks = (expected_size + CHUNK_SIZE - 1) / CHUNK_SIZE;
        let mut total_encrypted_size = 0;
        
        for i in 0..num_chunks {
            let chunk_start = i * CHUNK_SIZE;
            let chunk_end = ((i + 1) * CHUNK_SIZE).min(expected_size);
            let chunk_data_size = chunk_end - chunk_start;
            total_encrypted_size += chunk_data_size + 16; // +16 for AES-GCM tag
        }
        
        println!("🔧 Original data: {} bytes, {} chunks", expected_size, num_chunks);
        println!("🔧 Expected encrypted size: {} bytes", total_encrypted_size);
        println!("🔧 Reed-Solomon reconstructed: {} bytes", full_encrypted_data.len());
        
        // Truncate to remove Reed-Solomon padding
        full_encrypted_data.truncate(total_encrypted_size);
        println!("🔧 After truncation: {} bytes", full_encrypted_data.len());
        
        // CRITICAL FIX: Calculate chunk boundaries correctly
        let mut current_pos = 0;
        let mut current_chunk_idx = 0;
        
        // Skip to our target chunk
        while current_chunk_idx < chunk_index && current_pos < full_encrypted_data.len() {
            let chunk_start_in_original = current_chunk_idx * CHUNK_SIZE;
            let chunk_end_in_original = ((current_chunk_idx + 1) * CHUNK_SIZE).min(expected_size);
            let chunk_data_size = chunk_end_in_original - chunk_start_in_original;
            let chunk_with_tag_size = chunk_data_size + 16;
            
            current_pos += chunk_with_tag_size;
            current_chunk_idx += 1;
        }
        
        if current_pos >= full_encrypted_data.len() {
            println!("🔧 Chunk {} not found: pos={}, total_size={}", chunk_index, current_pos, full_encrypted_data.len());
            return Ok(Vec::new());
        }
        
        // Calculate the size of our target chunk
        let chunk_start_in_original = chunk_index * CHUNK_SIZE;
        let chunk_end_in_original = ((chunk_index + 1) * CHUNK_SIZE).min(expected_size);
        let chunk_data_size = chunk_end_in_original - chunk_start_in_original;
        let chunk_with_tag_size = chunk_data_size + 16;
        
        let chunk_end_pos = (current_pos + chunk_with_tag_size).min(full_encrypted_data.len());
        let encrypted_chunk = &full_encrypted_data[current_pos..chunk_end_pos];
        
        println!("🔧 Extracting chunk {}: pos={}-{}, size={}, data_size={}", 
                 chunk_index, current_pos, chunk_end_pos, encrypted_chunk.len(), chunk_data_size);
        
        if encrypted_chunk.len() < 16 {
            println!("🔧 Chunk {} too small after extraction: {} bytes", chunk_index, encrypted_chunk.len());
            return Ok(Vec::new());
        }
        
        // Decrypt the chunk
        Self::decrypt_chunk(encrypted_chunk, &master_key, chunk_index, &block_id)
    }
    
    fn decrypt_chunk(encrypted_chunk: &[u8], master_key: &[u8; 32], chunk_index: usize, block_id: &BlockId) -> Result<Vec<u8>, FlowError> {
        println!("🔓 Decrypting chunk {}: {} bytes, master_key[0..8]={:02x?}", 
                 chunk_index, encrypted_chunk.len(), &master_key[..8]);
        
        if encrypted_chunk.len() < 16 {
            println!("🔓 Chunk {} too small: {} bytes", chunk_index, encrypted_chunk.len());
            return Ok(Vec::new()); // Empty chunk
        }
        
        let chunk_key = ShamirErasureEncoder::derive_chunk_key(master_key, chunk_index, block_id);
        println!("🔓 Chunk {} key[0..8]: {:02x?}", chunk_index, &chunk_key[..8]);
        
        let cipher = Aes256Gcm::new(Key::<Aes256Gcm>::from_slice(&chunk_key));
        
        // Use the same chunk-specific nonce that was used for encryption
        let chunk_nonce = ShamirErasureEncoder::derive_chunk_nonce(block_id, chunk_index);
        let nonce = Nonce::from_slice(&chunk_nonce);
        
        println!("🔓 Chunk {} nonce: {:02x?}", chunk_index, &chunk_nonce);
        
        // Extract tag and ciphertext
        let (tag, ciphertext) = encrypted_chunk.split_at(16);
        println!("🔓 Chunk {} tag: {:02x?}", chunk_index, &tag[..8]);
        println!("🔓 Chunk {} ciphertext: {} bytes, [0..8]={:02x?}", 
                 chunk_index, ciphertext.len(), &ciphertext[..8.min(ciphertext.len())]);
        
        let mut plaintext = ciphertext.to_vec();
        
        match cipher.decrypt_in_place_detached(nonce, b"", &mut plaintext, tag.into()) {
            Ok(()) => {
                println!("🔓 Chunk {} decrypted successfully: {} bytes", chunk_index, plaintext.len());
                Ok(plaintext)
            }
            Err(e) => {
                println!("🔓 Chunk {} decryption failed: {}", chunk_index, e);
                println!("🔓   Expected ciphertext size: {}", ciphertext.len());
                println!("🔓   Tag: {:02x?}", tag);
                println!("🔓   Nonce: {:02x?}", chunk_nonce);
                println!("🔓   Key: {:02x?}", &chunk_key[..8]);
                Err(FlowError::PersistenceError(
                    format!("Decryption failed for chunk {}: {}", chunk_index, e)
                ))
            }
        }
    }
    
    /// Convenience method to reconstruct all data at once
    pub async fn reconstruct_all(&mut self) -> Result<Vec<u8>, FlowError> {
        let mut result = Vec::new();
        
        while let Some(chunk) = self.read_chunk().await? {
            result.extend(chunk);
        }
        
        // Wait for any remaining read-ahead tasks
        for task in self.read_ahead_tasks.drain(..) {
            let _ = task.await;
        }
        
        // Truncate to expected size
        result.truncate(self.expected_size);
        Ok(result)
    }
    
    /// Extract pointers early for indirect blocks
    pub async fn peek_pointers(&mut self, expected_pointer_count: usize) -> Result<Vec<[u8; 32]>, FlowError> {
        let bytes_needed = expected_pointer_count * 32;
        let mut pointer_data = Vec::new();
        
        while pointer_data.len() < bytes_needed {
            if let Some(chunk) = self.read_chunk().await? {
                pointer_data.extend(chunk);
            } else {
                break;
            }
        }
        
        let mut pointers = Vec::new();
        for chunk in pointer_data.chunks_exact(32) {
            let mut pointer = [0u8; 32];
            pointer.copy_from_slice(chunk);
            pointers.push(pointer);
        }
        
        Ok(pointers)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_key_sharing_basic() {
        let encoder = ShamirErasureEncoder::new(3, 5).unwrap();
        let original_key = [42u8; 32]; // Test key
        
        // Split the key into shares
        let shares = encoder.split_secret(&original_key).unwrap();
        assert_eq!(shares.len(), 5);
        
        // Reconstruct with exactly threshold shares
        let reconstructed_key = encoder.reconstruct_secret(&shares[..3]).unwrap();
        assert_eq!(reconstructed_key, original_key);
        
        // Test with different subset of shares
        let subset_shares = [shares[0].clone(), shares[2].clone(), shares[4].clone()];
        let reconstructed_key2 = encoder.reconstruct_secret(&subset_shares).unwrap();
        assert_eq!(reconstructed_key2, original_key);
    }
    
    #[test]
    fn test_key_sharing_insufficient_shares() {
        let encoder = ShamirErasureEncoder::new(3, 5).unwrap();
        let original_key = [123u8; 32];
        
        let shares = encoder.split_secret(&original_key).unwrap();
        
        // Try to reconstruct with insufficient shares (only 2, need 3)
        let result = encoder.reconstruct_secret(&shares[..2]);
        assert!(result.is_err());
    }
    
    #[test]
    fn test_key_sharing_all_different_keys() {
        let encoder = ShamirErasureEncoder::new(3, 5).unwrap();
        
        // Test with several different keys
        let test_keys = [
            [0u8; 32],
            [255u8; 32],
            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
             17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],
        ];
        
        for (i, &original_key) in test_keys.iter().enumerate() {
            println!("Testing key {}: {:?}", i, &original_key[..8]);
            
            let shares = encoder.split_secret(&original_key).unwrap();
            let reconstructed = encoder.reconstruct_secret(&shares[..3]).unwrap();
            
            assert_eq!(reconstructed, original_key, "Failed for key {}", i);
        }
    }
    
    #[tokio::test]
    async fn test_encode_decode_basic() {
        let encoder = ShamirErasureEncoder::new(3, 5).unwrap();
        let original_data = b"Hello, Shamir + Reed-Solomon world! This is a test of the new secure erasure coding system.";
        let block_id = [1u8; 32];
        
        // Encode
        let shards = encoder.encode(original_data, &block_id).unwrap();
        assert_eq!(shards.len(), 5);
        
        // Decode with all shards
        let mut shard_map = HashMap::new();
        for (i, shard) in shards.into_iter().enumerate() {
            shard_map.insert(i, shard);
        }
        
        let mut decoder = encoder.decode_with_streaming(shard_map, &block_id, original_data.len()).unwrap();
        let decoded = decoder.reconstruct_all().await.unwrap();
        assert_eq!(decoded, original_data);
    }
    
    #[tokio::test]
    async fn test_fault_tolerance() {
        let encoder = ShamirErasureEncoder::new(3, 5).unwrap();
        let original_data = b"This is fault tolerance testing with Shamir secret sharing!";
        let block_id = [2u8; 32];
        
        let shards = encoder.encode(original_data, &block_id).unwrap();
        
        // Test reconstruction with exactly threshold shards (lose 2 shards)
        let mut shard_map = HashMap::new();
        shard_map.insert(0, shards[0].clone());
        shard_map.insert(2, shards[2].clone());
        shard_map.insert(4, shards[4].clone());
        
        let mut decoder = encoder.decode_with_streaming(shard_map, &block_id, original_data.len()).unwrap();
        let decoded = decoder.reconstruct_all().await.unwrap();
        assert_eq!(decoded, original_data);
    }
    
    #[tokio::test]
    async fn test_streaming_chunks() {
        let encoder = ShamirErasureEncoder::new(3, 5).unwrap();
        let original_data = vec![42u8; CHUNK_SIZE * 3 + 1000]; // Multiple chunks
        let block_id = [3u8; 32];
        
        let shards = encoder.encode(&original_data, &block_id).unwrap();
        
        let mut shard_map = HashMap::new();
        for (i, shard) in shards.into_iter().enumerate() {
            shard_map.insert(i, shard);
        }
        
        let mut decoder = encoder.decode_with_streaming(shard_map, &block_id, original_data.len()).unwrap();
        
        // Read chunk by chunk
        let mut reconstructed = Vec::new();
        while let Some(chunk) = decoder.read_chunk().await.unwrap() {
            reconstructed.extend(chunk);
        }
        
        assert_eq!(reconstructed, original_data);
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/examples.rs
// =====================================================================

//! Examples and demonstrations of the dissolution storage system

use std::path::PathBuf;
use crate::storage::{
    DissolutionStorageFactory, DissolutionFile,
    dissolution::{BackendConfig, DissolutionStorage}
};
use crate::flow::FlowError;

/// Example: Basic dissolution storage usage
pub async fn basic_dissolution_example() -> Result<(), FlowError> {
    // Create configuration for soradyne erasure backend
    let config = DissolutionStorageFactory::create_sdyn_erasure_config(
        3, // threshold: need 3 shards to reconstruct
        5, // total_shards: create 5 shards total
        PathBuf::from("/tmp/dissolution_metadata.json"),
    ).await?;
    
    // Create storage backend
    let storage = DissolutionStorageFactory::create(config).await?;
    
    // Store some data
    let test_data = b"Hello, dissolution storage!";
    let block_id = storage.store(test_data).await?;
    println!("Stored data with block ID: {}", hex::encode(block_id));
    
    // Retrieve the data
    let retrieved_data = storage.retrieve(&block_id).await?;
    assert_eq!(test_data, retrieved_data.as_slice());
    println!("Successfully retrieved data: {}", String::from_utf8_lossy(&retrieved_data));
    
    // Get block information
    let block_info = storage.block_info(&block_id).await?;
    println!("Block info: {} bytes, {} shards, can reconstruct: {}", 
             block_info.size, block_info.shard_count, block_info.can_reconstruct);
    
    Ok(())
}

/// Example: Demonstrate fault tolerance
pub async fn fault_tolerance_demo() -> Result<(), FlowError> {
    let config = DissolutionStorageFactory::create_sdyn_erasure_config(
        3, 5, PathBuf::from("/tmp/dissolution_metadata.json")
    ).await?;
    
    let storage = DissolutionStorageFactory::create(config).await?;
    
    // Store test data
    let test_data = b"This data will survive device failures!";
    let block_id = storage.store(test_data).await?;
    
    // Demonstrate that we can lose 2 shards and still reconstruct
    let demo = storage.demonstrate_dissolution(&block_id, vec![0, 1]).await?;
    
    println!("Fault Tolerance Demo Results:");
    println!("  Original shards: {}", demo.original_shards);
    println!("  Simulated missing: {:?}", demo.simulated_missing);
    println!("  Available shards: {}", demo.available_shards);
    println!("  Threshold required: {}", demo.threshold_required);
    println!("  Can reconstruct: {}", demo.can_reconstruct);
    println!("  Reconstruction successful: {}", demo.reconstruction_successful);
    println!("  Data integrity verified: {}", demo.data_integrity_verified);
    
    Ok(())
}

/// Example: High-level file interface
pub async fn file_interface_example() -> Result<(), FlowError> {
    let config = DissolutionStorageFactory::create_sdyn_erasure_config(
        2, 3, PathBuf::from("/tmp/dissolution_metadata.json")
    ).await?;
    
    let storage = DissolutionStorageFactory::create(config).await?;
    
    // Create a dissolution file
    let mut file = DissolutionFile::new(storage.clone());
    
    // Write data to the file
    let file_content = b"This is a file stored using dissolution!";
    file.write(file_content).await?;
    
    println!("File written, size: {} bytes", file.size());
    if let Some(root_block) = file.root_block() {
        println!("Root block ID: {}", hex::encode(root_block));
    }
    
    // Read the file back
    let read_content = file.read().await?;
    assert_eq!(file_content, read_content.as_slice());
    println!("File content: {}", String::from_utf8_lossy(&read_content));
    
    // Check file info
    if let Some(info) = file.info().await? {
        println!("File info: {} bytes, {} available shards", 
                 info.size, info.available_shards);
    }
    
    Ok(())
}

/// Example: Backend detection and selection
pub async fn backend_detection_example() -> Result<(), FlowError> {
    let available_backends = DissolutionStorageFactory::detect_available_backends().await;
    
    println!("Available dissolution storage backends:");
    for backend in &available_backends {
        println!("  - {}", backend);
    }
    
    // Create default configuration based on available backends
    let config = DissolutionStorageFactory::create_default_config(
        2, 4, PathBuf::from("/tmp/dissolution_metadata.json")
    ).await?;
    
    match &config.backend_config {
        BackendConfig::SdynErasure { rimsd_paths, .. } => {
            println!("Using soradyne erasure backend with {} devices", rimsd_paths.len());
        }
        BackendConfig::BcacheFS { device_paths, .. } => {
            println!("Using bcachefs backend with {} devices", device_paths.len());
        }
        _ => {
            println!("Using other backend type");
        }
    }
    
    Ok(())
}

/// Run all examples
pub async fn run_all_examples() -> Result<(), FlowError> {
    println!("=== Dissolution Storage Examples ===\n");
    
    println!("1. Backend Detection:");
    backend_detection_example().await?;
    println!();
    
    println!("2. Basic Dissolution Storage:");
    basic_dissolution_example().await?;
    println!();
    
    println!("3. Fault Tolerance Demo:");
    fault_tolerance_demo().await?;
    println!();
    
    println!("4. High-Level File Interface:");
    file_interface_example().await?;
    println!();
    
    println!("All examples completed successfully!");
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_backend_detection() {
        let backends = DissolutionStorageFactory::detect_available_backends().await;
        assert!(!backends.is_empty());
        assert!(backends.contains(&"sdyn_erasure".to_string()));
    }
    
    #[tokio::test]
    #[ignore] // Requires actual SD cards
    async fn test_basic_dissolution() {
        basic_dissolution_example().await.unwrap();
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/block_file.rs
// =====================================================================

//! Block-based file abstraction

use std::sync::Arc;
use tokio::sync::RwLock;
use crate::storage::block_manager::BlockManager;
use crate::flow::FlowError;

const BLOCK_SIZE: usize = 32 * 1024 * 1024; // 32MB

pub struct BlockFile {
    manager: Arc<BlockManager>,
    root_block: RwLock<Option<[u8; 32]>>,
    size: RwLock<usize>,
}

impl BlockFile {
    pub fn new(manager: Arc<BlockManager>) -> Self {
        Self {
            manager,
            root_block: RwLock::new(None),
            size: RwLock::new(0),
        }
    }
    
    pub fn from_existing(manager: Arc<BlockManager>, root_block: [u8; 32], size: usize) -> Self {
        Self {
            manager,
            root_block: RwLock::new(Some(root_block)),
            size: RwLock::new(size),
        }
    }
    
    pub async fn append(&self, data: &[u8]) -> Result<(), FlowError> {
        if data.len() <= BLOCK_SIZE {
            // Small file - use direct block
            let block_id = self.manager.write_direct_block(data).await?;
            *self.root_block.write().await = Some(block_id);
            *self.size.write().await = data.len();
        } else {
            // Large file - split into chunks and use indirect blocks
            let mut chunk_block_ids = Vec::new();
            
            // Split data into chunks
            for chunk in data.chunks(BLOCK_SIZE) {
                let chunk_block_id = self.manager.write_direct_block(chunk).await?;
                chunk_block_ids.push(chunk_block_id);
            }
            
            // Create indirect block containing the chunk addresses
            let mut addresses_data = Vec::new();
            for block_id in &chunk_block_ids {
                addresses_data.extend_from_slice(block_id);
            }
            
            let indirect_block_id = self.manager.write_direct_block(&addresses_data).await?;
            *self.root_block.write().await = Some(indirect_block_id);
            *self.size.write().await = data.len();
        }
        Ok(())
    }
    
    pub async fn read(&self) -> Result<Vec<u8>, FlowError> {
        if let Some(root) = *self.root_block.read().await {
            let size = *self.size.read().await;
            if size <= BLOCK_SIZE {
                // Direct block
                self.manager.read_block(&root).await
            } else {
                // Indirect block - read addresses and reconstruct
                let addresses_data = self.manager.read_block(&root).await?;
                let mut result = Vec::new();
                
                // Parse addresses (each is 32 bytes)
                for chunk in addresses_data.chunks_exact(32) {
                    let mut block_id = [0u8; 32];
                    block_id.copy_from_slice(chunk);
                    let chunk_data = self.manager.read_block(&block_id).await?;
                    result.extend_from_slice(&chunk_data);
                }
                
                // Truncate to actual file size (last chunk might be padded)
                result.truncate(size);
                Ok(result)
            }
        } else {
            Ok(Vec::new())
        }
    }
    
    pub async fn root_block(&self) -> Option<[u8; 32]> {
        *self.root_block.read().await
    }
    
    pub async fn size(&self) -> usize {
        *self.size.read().await
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/backends/sdyn_erasure.rs
// =====================================================================

//! Soradyne erasure coding backend implementation
//! 
//! This backend uses the existing BlockManager to provide dissolution storage
//! through soradyne erasure coding and shard distribution across rimsd directories.

use async_trait::async_trait;
use std::path::PathBuf;
use std::sync::Arc;

use crate::storage::dissolution::{
    DissolutionStorage, DissolutionConfig, BlockId, BlockInfo, StorageStats, 
    DissolutionDemo, DeviceHealth, ReconstructionStats
};
use crate::storage::block_manager::BlockManager;
use crate::flow::FlowError;

/// Implementation using soradyne erasure coding via BlockManager
#[derive(Clone)]
pub struct SdynErasureBackend {
    block_manager: Arc<BlockManager>,
    config: DissolutionConfig,
}

impl SdynErasureBackend {
    pub async fn new(
        rimsd_paths: Vec<PathBuf>,
        metadata_path: PathBuf,
        config: DissolutionConfig,
    ) -> Result<Self, FlowError> {
        let block_manager = Arc::new(BlockManager::new(
            rimsd_paths,
            metadata_path,
            config.threshold,
            config.total_shards,
        )?);
        
        // Initialize device fingerprints
        block_manager.initialize_device_fingerprints().await?;
        
        Ok(Self {
            block_manager,
            config,
        })
    }
    
    /// Create from auto-discovered Soradyne volumes
    pub async fn new_with_discovery(
        metadata_path: PathBuf,
        config: DissolutionConfig,
    ) -> Result<Self, FlowError> {
        let block_manager = Arc::new(BlockManager::new_with_discovery(
            metadata_path,
            config.threshold,
            config.total_shards,
        ).await?);
        
        // Initialize device fingerprints
        block_manager.initialize_device_fingerprints().await?;
        
        Ok(Self {
            block_manager,
            config,
        })
    }
}

#[async_trait]
impl DissolutionStorage for SdynErasureBackend {
    async fn store(&self, data: &[u8]) -> Result<BlockId, FlowError> {
        if data.len() > self.config.max_direct_block_size {
            return Err(FlowError::PersistenceError(
                format!("Data size {} exceeds max direct block size {}", 
                       data.len(), self.config.max_direct_block_size)
            ));
        }
        
        self.block_manager.write_direct_block(data).await
    }
    
    async fn retrieve(&self, block_id: &BlockId) -> Result<Vec<u8>, FlowError> {
        self.block_manager.read_block(block_id).await
    }
    
    async fn exists(&self, block_id: &BlockId) -> Result<bool, FlowError> {
        match self.block_manager.get_block_distribution(block_id).await {
            Ok(distribution) => Ok(distribution.can_reconstruct),
            Err(_) => Ok(false),
        }
    }
    
    async fn block_info(&self, block_id: &BlockId) -> Result<BlockInfo, FlowError> {
        let distribution = self.block_manager.get_block_distribution(block_id).await?;
        let blocks = self.block_manager.list_blocks().await;
        
        // Find the metadata for this block
        let metadata = blocks.iter()
            .find(|(id, _)| *id == *block_id)
            .map(|(_, meta)| meta)
            .ok_or_else(|| FlowError::PersistenceError("Block metadata not found".to_string()))?;
        
        Ok(BlockInfo {
            id: *block_id,
            size: distribution.original_size,
            created_at: metadata.created_at,
            is_indirect: metadata.directness > 0,
            shard_count: distribution.total_shards,
            available_shards: distribution.available_shards.len(),
            can_reconstruct: distribution.can_reconstruct,
        })
    }
    
    async fn delete(&self, _block_id: &BlockId) -> Result<(), FlowError> {
        // TODO: Implement block deletion in BlockManager
        Err(FlowError::PersistenceError("Block deletion not yet implemented".to_string()))
    }
    
    async fn list_blocks(&self) -> Result<Vec<BlockId>, FlowError> {
        let blocks = self.block_manager.list_blocks().await;
        Ok(blocks.into_iter().map(|(id, _)| id).collect())
    }
    
    async fn storage_stats(&self) -> Result<StorageStats, FlowError> {
        let storage_info = self.block_manager.get_storage_info();
        let blocks = self.block_manager.list_blocks().await;
        
        // Calculate reconstruction capability
        let mut blocks_at_risk = 0;
        let mut blocks_safe = 0;
        let mut blocks_lost = 0;
        let mut total_size = 0u64;
        
        for (block_id, metadata) in &blocks {
            total_size += metadata.size as u64;
            
            match self.block_manager.get_block_distribution(block_id).await {
                Ok(distribution) => {
                    if distribution.available_shards.len() < self.config.threshold {
                        blocks_lost += 1;
                    } else if distribution.available_shards.len() == self.config.threshold {
                        blocks_at_risk += 1;
                    } else {
                        blocks_safe += 1;
                    }
                }
                Err(_) => {
                    blocks_lost += 1;
                }
            }
        }
        
        // TODO: Implement device health checking
        let device_health: Vec<_> = storage_info.rimsd_paths.iter().map(|path| {
            DeviceHealth {
                device_id: path.to_string_lossy().to_string(),
                path: path.clone(),
                available: path.exists(),
                free_space_bytes: 0, // TODO: Get actual free space
                total_space_bytes: 0, // TODO: Get actual total space
                error_rate: 0.0,
            }
        }).collect();
        
        let health_score = if storage_info.total_devices > 0 {
            device_health.iter().filter(|d| d.available).count() as f64 / storage_info.total_devices as f64
        } else {
            0.0
        };
        
        Ok(StorageStats {
            total_blocks: blocks.len(),
            total_size_bytes: total_size,
            available_devices: device_health.iter().filter(|d| d.available).count(),
            total_devices: storage_info.total_devices,
            health_score,
            device_health,
            reconstruction_capability: ReconstructionStats {
                blocks_at_risk,
                blocks_safe,
                blocks_lost,
            },
        })
    }
    
    async fn demonstrate_dissolution(&self, block_id: &BlockId, simulate_missing: Vec<usize>) -> Result<DissolutionDemo, FlowError> {
        let demo_result = self.block_manager.demonstrate_erasure_recovery(block_id, simulate_missing.clone()).await?;
        
        // Verify data integrity by comparing with original
        let data_integrity_verified = if demo_result.recovery_successful {
            // Try to read the original data and compare
            match self.block_manager.read_block(block_id).await {
                Ok(original_data) => demo_result.recovered_data_size == original_data.len(),
                Err(_) => false,
            }
        } else {
            false
        };
        
        Ok(DissolutionDemo {
            block_id: *block_id,
            original_shards: demo_result.original_shards,
            simulated_missing: simulate_missing,
            available_shards: demo_result.available_shards,
            threshold_required: demo_result.threshold_required,
            can_reconstruct: demo_result.recovery_successful,
            reconstruction_successful: demo_result.recovery_successful,
            data_integrity_verified,
            recovered_data_size: demo_result.recovered_data_size,
        })
    }
    
    async fn maintenance(&self) -> Result<(), FlowError> {
        // Verify device continuity
        self.block_manager.verify_device_continuity().await?;
        
        // TODO: Add other maintenance tasks like:
        // - Checking for corrupted shards
        // - Rebalancing shards across devices
        // - Cleaning up orphaned metadata
        
        Ok(())
    }
    
    fn config(&self) -> &DissolutionConfig {
        &self.config
    }
    
    async fn update_config(&mut self, _config: DissolutionConfig) -> Result<(), FlowError> {
        // For now, don't allow config changes after creation
        // This would require rebuilding the BlockManager
        Err(FlowError::PersistenceError(
            "Configuration updates not supported for soradyne erasure backend".to_string()
        ))
    }
    
    async fn verify_device_continuity(&self) -> Result<(), FlowError> {
        self.block_manager.verify_device_continuity().await
    }
    
    async fn initialize_device_fingerprints(&self) -> Result<(), FlowError> {
        self.block_manager.initialize_device_fingerprints().await
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/backends/mod.rs
// =====================================================================

//! Storage backend implementations

pub mod sdyn_erasure;
pub mod bcachefs;

pub use sdyn_erasure::SdynErasureBackend;

// Only expose bcachefs on Linux
#[cfg(target_os = "linux")]
pub use bcachefs::BcacheFSBackend;

use std::path::PathBuf;
use crate::storage::dissolution::{DissolutionStorage, DissolutionConfig, BackendConfig, BlockId, BlockInfo, StorageStats, DissolutionDemo};
use crate::flow::FlowError;
use async_trait::async_trait;

/// Concrete enum for dissolution storage backends
#[derive(Clone)]
pub enum DissolutionBackend {
    SdynErasure(SdynErasureBackend),
    #[cfg(target_os = "linux")]
    BcacheFS(BcacheFSBackend),
}

#[async_trait]
impl DissolutionStorage for DissolutionBackend {
    async fn store(&self, data: &[u8]) -> Result<BlockId, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.store(data).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.store(data).await,
        }
    }
    
    async fn retrieve(&self, block_id: &BlockId) -> Result<Vec<u8>, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.retrieve(block_id).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.retrieve(block_id).await,
        }
    }
    
    async fn exists(&self, block_id: &BlockId) -> Result<bool, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.exists(block_id).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.exists(block_id).await,
        }
    }
    
    async fn block_info(&self, block_id: &BlockId) -> Result<BlockInfo, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.block_info(block_id).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.block_info(block_id).await,
        }
    }
    
    async fn delete(&self, block_id: &BlockId) -> Result<(), FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.delete(block_id).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.delete(block_id).await,
        }
    }
    
    async fn list_blocks(&self) -> Result<Vec<BlockId>, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.list_blocks().await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.list_blocks().await,
        }
    }
    
    async fn storage_stats(&self) -> Result<StorageStats, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.storage_stats().await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.storage_stats().await,
        }
    }
    
    async fn demonstrate_dissolution(&self, block_id: &BlockId, simulate_missing: Vec<usize>) -> Result<DissolutionDemo, FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.demonstrate_dissolution(block_id, simulate_missing).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.demonstrate_dissolution(block_id, simulate_missing).await,
        }
    }
    
    async fn maintenance(&self) -> Result<(), FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.maintenance().await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.maintenance().await,
        }
    }
    
    fn config(&self) -> &DissolutionConfig {
        match self {
            Self::SdynErasure(backend) => backend.config(),
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.config(),
        }
    }
    
    async fn update_config(&mut self, config: DissolutionConfig) -> Result<(), FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.update_config(config).await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.update_config(config).await,
        }
    }
    
    async fn verify_device_continuity(&self) -> Result<(), FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.verify_device_continuity().await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.verify_device_continuity().await,
        }
    }
    
    async fn initialize_device_fingerprints(&self) -> Result<(), FlowError> {
        match self {
            Self::SdynErasure(backend) => backend.initialize_device_fingerprints().await,
            #[cfg(target_os = "linux")]
            Self::BcacheFS(backend) => backend.initialize_device_fingerprints().await,
        }
    }
}

/// Factory for creating dissolution storage backends
pub struct DissolutionStorageFactory;

impl DissolutionStorageFactory {
    /// Create a storage backend from configuration
    pub async fn create(config: DissolutionConfig) -> Result<DissolutionBackend, FlowError> {
        match &config.backend_config {
            BackendConfig::SdynErasure { rimsd_paths, metadata_path } => {
                let backend = SdynErasureBackend::new(
                    rimsd_paths.clone(),
                    metadata_path.clone(),
                    config.clone(),
                ).await?;
                Ok(DissolutionBackend::SdynErasure(backend))
            },
            BackendConfig::BcacheFS { .. } => {
                #[cfg(target_os = "linux")]
                {
                    let backend = bcachefs::BcacheFSBackend::new(config.clone()).await?;
                    Ok(DissolutionBackend::BcacheFS(backend))
                }
                #[cfg(not(target_os = "linux"))]
                {
                    Err(FlowError::PersistenceError(
                        "bcachefs backend is only available on Linux".to_string()
                    ))
                }
            },
            BackendConfig::ZFS { pool_name, redundancy_level } => {
                Err(FlowError::PersistenceError(
                    format!("ZFS backend not yet implemented (pool: {}, redundancy: {})", 
                           pool_name, redundancy_level)
                ))
            },
            BackendConfig::Custom { implementation_name, .. } => {
                Err(FlowError::PersistenceError(
                    format!("Custom backend '{}' not implemented", implementation_name)
                ))
            },
        }
    }
    
    /// Detect available backends on the system
    pub async fn detect_available_backends() -> Vec<String> {
        let mut backends = vec!["sdyn_erasure".to_string()];
        
        // Check if bcachefs is available (Linux only)
        #[cfg(target_os = "linux")]
        {
            if tokio::process::Command::new("bcachefs")
                .arg("version")
                .output()
                .await
                .is_ok()
            {
                backends.push("bcachefs".to_string());
            }
        }
        
        // Check for ZFS
        if tokio::process::Command::new("zfs")
            .arg("version")
            .output()
            .await
            .is_ok()
        {
            backends.push("zfs".to_string());
        }
        
        backends
    }
    
    /// Create a configuration for soradyne erasure backend with auto-discovery
    pub async fn create_sdyn_erasure_config(
        threshold: usize,
        total_shards: usize,
        metadata_path: PathBuf,
    ) -> Result<DissolutionConfig, FlowError> {
        use crate::storage::device_identity::discover_soradyne_volumes;
        
        let rimsd_paths = discover_soradyne_volumes().await?;
        
        if rimsd_paths.is_empty() {
            return Err(FlowError::PersistenceError(
                "No Soradyne volumes found. Please initialize some SD cards first.".to_string()
            ));
        }
        
        Ok(DissolutionConfig {
            threshold,
            total_shards,
            max_direct_block_size: 32 * 1024 * 1024, // 32MB
            backend_config: BackendConfig::SdynErasure {
                rimsd_paths,
                metadata_path,
            },
        })
    }
    
    /// Create a default configuration based on available backends
    pub async fn create_default_config(
        threshold: usize,
        total_shards: usize,
        metadata_path: PathBuf,
    ) -> Result<DissolutionConfig, FlowError> {
        let available = Self::detect_available_backends().await;
        
        // Prefer bcachefs on Linux if available, otherwise use soradyne erasure
        if available.contains(&"bcachefs".to_string()) {
            // TODO: Implement bcachefs config creation
            Self::create_sdyn_erasure_config(threshold, total_shards, metadata_path).await
        } else {
            Self::create_sdyn_erasure_config(threshold, total_shards, metadata_path).await
        }
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/storage/backends/bcachefs.rs
// =====================================================================

//! bcachefs backend implementation (Linux only)
//! 
//! This backend provides dissolution storage using bcachefs with optimizations
//! for SD card longevity and distributed storage.

use async_trait::async_trait;
use std::path::PathBuf;

use crate::storage::dissolution::{
    DissolutionStorage, DissolutionConfig, BlockId, BlockInfo, StorageStats, 
    DissolutionDemo, DeviceHealth, ReconstructionStats
};
use crate::flow::FlowError;

/// bcachefs-based dissolution storage backend
#[derive(Clone)]
pub struct BcacheFSBackend {
    config: DissolutionConfig,
    mount_point: PathBuf,
}

impl BcacheFSBackend {
    pub async fn new(_config: DissolutionConfig) -> Result<Self, FlowError> {
        #[cfg(not(target_os = "linux"))]
        {
            return Err(FlowError::PersistenceError(
                "bcachefs backend is only available on Linux".to_string()
            ));
        }
        
        #[cfg(target_os = "linux")]
        {
            // TODO: Implement bcachefs initialization
            // This would involve:
            // 1. Checking if bcachefs tools are available
            // 2. Setting up the filesystem with appropriate options
            // 3. Mounting with SD card optimizations
            
            let mount_point = PathBuf::from("/tmp/bcachefs_dissolution"); // TODO: Make configurable
            
            Ok(Self {
                config: _config,
                mount_point,
            })
        }
    }
}

#[cfg(target_os = "linux")]
#[async_trait]
impl DissolutionStorage for BcacheFSBackend {
    async fn store(&self, _data: &[u8]) -> Result<BlockId, FlowError> {
        // TODO: Implement bcachefs storage
        Err(FlowError::PersistenceError("bcachefs backend not yet fully implemented".to_string()))
    }
    
    async fn retrieve(&self, _block_id: &BlockId) -> Result<Vec<u8>, FlowError> {
        // TODO: Implement bcachefs retrieval
        Err(FlowError::PersistenceError("bcachefs backend not yet fully implemented".to_string()))
    }
    
    async fn exists(&self, _block_id: &BlockId) -> Result<bool, FlowError> {
        // TODO: Implement bcachefs existence check
        Ok(false)
    }
    
    async fn block_info(&self, _block_id: &BlockId) -> Result<BlockInfo, FlowError> {
        // TODO: Implement bcachefs block info
        Err(FlowError::PersistenceError("bcachefs backend not yet fully implemented".to_string()))
    }
    
    async fn delete(&self, _block_id: &BlockId) -> Result<(), FlowError> {
        // TODO: Implement bcachefs deletion
        Err(FlowError::PersistenceError("bcachefs backend not yet fully implemented".to_string()))
    }
    
    async fn list_blocks(&self) -> Result<Vec<BlockId>, FlowError> {
        // TODO: Implement bcachefs block listing
        Ok(vec![])
    }
    
    async fn storage_stats(&self) -> Result<StorageStats, FlowError> {
        // TODO: Implement bcachefs stats
        Ok(StorageStats {
            total_blocks: 0,
            total_size_bytes: 0,
            available_devices: 0,
            total_devices: 0,
            health_score: 1.0,
            device_health: vec![],
            reconstruction_capability: ReconstructionStats {
                blocks_at_risk: 0,
                blocks_safe: 0,
                blocks_lost: 0,
            },
        })
    }
    
    async fn demonstrate_dissolution(&self, _block_id: &BlockId, _simulate_missing: Vec<usize>) -> Result<DissolutionDemo, FlowError> {
        // TODO: Implement bcachefs dissolution demo
        Err(FlowError::PersistenceError("bcachefs backend not yet fully implemented".to_string()))
    }
    
    async fn maintenance(&self) -> Result<(), FlowError> {
        // TODO: Implement bcachefs maintenance
        Ok(())
    }
    
    fn config(&self) -> &DissolutionConfig {
        &self.config
    }
    
    async fn update_config(&mut self, config: DissolutionConfig) -> Result<(), FlowError> {
        self.config = config;
        Ok(())
    }
    
    async fn verify_device_continuity(&self) -> Result<(), FlowError> {
        // TODO: Implement device verification for bcachefs
        Ok(())
    }
    
    async fn initialize_device_fingerprints(&self) -> Result<(), FlowError> {
        // TODO: Implement device fingerprinting for bcachefs
        Ok(())
    }
}

// Stub implementation for non-Linux platforms
#[cfg(not(target_os = "linux"))]
#[async_trait]
impl DissolutionStorage for BcacheFSBackend {
    async fn store(&self, _data: &[u8]) -> Result<BlockId, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn retrieve(&self, _block_id: &BlockId) -> Result<Vec<u8>, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn exists(&self, _block_id: &BlockId) -> Result<bool, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn block_info(&self, _block_id: &BlockId) -> Result<BlockInfo, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn delete(&self, _block_id: &BlockId) -> Result<(), FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn list_blocks(&self) -> Result<Vec<BlockId>, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn storage_stats(&self) -> Result<StorageStats, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn demonstrate_dissolution(&self, _block_id: &BlockId, _simulate_missing: Vec<usize>) -> Result<DissolutionDemo, FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn maintenance(&self) -> Result<(), FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    fn config(&self) -> &DissolutionConfig {
        &self.config
    }
    
    async fn update_config(&mut self, config: DissolutionConfig) -> Result<(), FlowError> {
        self.config = config;
        Ok(())
    }
    
    async fn verify_device_continuity(&self) -> Result<(), FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
    
    async fn initialize_device_fingerprints(&self) -> Result<(), FlowError> {
        Err(FlowError::PersistenceError("bcachefs not available on this platform".to_string()))
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/ffi/mod.rs
// =====================================================================

use std::ffi::{CStr, CString};
use std::os::raw::c_char;
use std::sync::{Arc, Mutex};
use std::collections::HashMap;
use std::path::PathBuf;
use tokio::runtime::Runtime;
use serde_json;

use crate::album::album::*;
use crate::album::operations::*;
use crate::album::crdt::{Crdt, CrdtCollection};
use crate::storage::block_manager::BlockManager;
use crate::video::{generate_video_at_size, generate_image_at_size, create_audio_placeholder_at_size, create_video_placeholder_at_size, is_video_file, is_audio_file};

// Create a simple placeholder image for videos when thumbnail generation fails
fn create_video_placeholder() -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    create_video_placeholder_at_size(150)
}

// Legacy function - kept for compatibility
fn _create_video_placeholder_legacy() -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    use image::{RgbaImage, Rgba};
    
    // Create a 150x150 gray image with a play icon
    let mut img = RgbaImage::new(150, 150);
    let gray = Rgba([128, 128, 128, 255]);
    let white = Rgba([255, 255, 255, 255]);
    
    // Fill with gray
    for pixel in img.pixels_mut() {
        *pixel = gray;
    }
    
    // Draw a simple play triangle in the center
    let center_x = 75;
    let center_y = 75;
    let size = 20;
    
    // Simple triangle points
    for y in (center_y - size as i32)..(center_y + size as i32) {
        for x in (center_x - size as i32/2)..(center_x + size as i32) {
            if x >= 0 && x < 150 && y >= 0 && y < 150 {
                // Simple triangle shape
                let dx: i32 = x - center_x;
                let dy: i32 = y - center_y;
                if dx > -(size as i32)/2 && dx < size as i32 && dy.abs() < size as i32 - dx.abs()/2 {
                    img.put_pixel(x as u32, y as u32, white);
                }
            }
        }
    }
    
    // Encode as PNG
    let mut buffer = Vec::new();
    img.write_to(&mut std::io::Cursor::new(&mut buffer), image::ImageOutputFormat::Png)?;
    
    Ok(buffer)
}

// Global state for the album system
static mut ALBUM_SYSTEM: Option<Arc<Mutex<AlbumSystem>>> = None;

pub struct AlbumSystem {
    albums: HashMap<String, MediaAlbum>,
    block_manager: Arc<BlockManager>,
    data_dir: PathBuf,
    albums_index_block_id: Option<[u8; 32]>,
}

impl AlbumSystem {
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        println!("Creating AlbumSystem...");
        
        // Use a writable location for metadata - app's container directory
        let metadata_path = if cfg!(target_os = "macos") {
            PathBuf::from(std::env::var("HOME").unwrap_or_else(|_| "/tmp".to_string()))
                .join("Library/Containers/com.example.soradyneApp/Data/.soradyne_metadata.json")
        } else {
            PathBuf::from("/tmp/soradyne_metadata.json")
        };
        println!("Metadata path: {:?}", metadata_path);
        
        // Ensure the parent directory exists
        if let Some(parent) = metadata_path.parent() {
            std::fs::create_dir_all(parent).map_err(|e| {
                println!("Failed to create metadata directory: {}", e);
                e
            })?;
        }
        
        println!("🔍 Discovering SD cards...");
        let rimsd_dirs = crate::storage::device_identity::discover_soradyne_volumes().await
            .map_err(|e| format!("SD card discovery failed: {}", e))?;

        if rimsd_dirs.is_empty() {
            return Err("No Soradyne SD cards found! Please insert SD cards with .rimsd directories".into());
        }

        println!("✅ Found {} SD cards", rimsd_dirs.len());
        let threshold = std::cmp::min(3, rimsd_dirs.len()); // Adaptive threshold, prefer 3 but adapt to available
        let total_shards = rimsd_dirs.len();

        println!("Creating BlockManager with {} shards (threshold: {})...", total_shards, threshold);
        let block_manager = Arc::new(BlockManager::new(
            rimsd_dirs,
            metadata_path,
            threshold,
            total_shards,
        ).map_err(|e| {
            println!("Failed to create BlockManager: {}", e);
            e
        })?);
        println!("BlockManager created successfully");
        
        let mut system = Self {
            albums: HashMap::new(),
            block_manager,
            data_dir: PathBuf::from("/tmp"), // Temporary directory since we're using SD cards
            albums_index_block_id: None,
        };
        
        println!("Loading existing albums from block storage...");
        // Load existing albums from block storage
        system.load_albums_from_blocks().await.map_err(|e| {
            println!("Failed to load albums from blocks: {}", e);
            e
        })?;
        println!("Albums loaded successfully");
        
        println!("AlbumSystem initialization complete");
        Ok(system)
    }
    
    async fn load_albums_from_blocks(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // Try to load from local storage first (for existing albums)
        let local_data_dir = if cfg!(target_os = "macos") {
            PathBuf::from(std::env::var("HOME").unwrap_or_else(|_| ".".to_string()))
                .join("Library/Containers/com.example.soradyneApp/Data/.soradyne_albums")
        } else {
            PathBuf::from(std::env::var("HOME").unwrap_or_else(|_| ".".to_string()))
                .join(".soradyne_albums")
        };
        
        let index_file = local_data_dir.join("albums_index.txt");
        println!("Looking for albums index at: {:?}", index_file);
        
        if index_file.exists() {
            println!("Albums index file exists, reading...");
            let index_content = std::fs::read_to_string(&index_file)?;
            println!("Index content: {}", index_content.trim());
            
            if let Ok(block_id_bytes) = hex::decode(index_content.trim()) {
                if block_id_bytes.len() == 32 {
                    let mut block_id = [0u8; 32];
                    block_id.copy_from_slice(&block_id_bytes);
                    self.albums_index_block_id = Some(block_id);
                    
                    println!("Loading albums index from block: {}", hex::encode(block_id));
                    
                    // Load the albums index
                    if let Ok(index_data) = self.block_manager.read_block(&block_id).await {
                        println!("Successfully read index data: {} bytes", index_data.len());
                        
                        if let Ok(index_json) = String::from_utf8(index_data) {
                            println!("Index JSON: {}", index_json);
                            
                            if let Ok(album_index) = serde_json::from_str::<HashMap<String, [u8; 32]>>(&index_json) {
                                println!("Found {} albums in index", album_index.len());
                                
                                // Load each album from its block
                                for (album_id, album_block_id) in album_index {
                                    println!("Loading album {} from block {}", album_id, hex::encode(album_block_id));
                                    
                                    if let Ok(album_data) = self.block_manager.read_block(&album_block_id).await {
                                        if let Ok(album_json) = String::from_utf8(album_data) {
                                            if let Ok(mut album) = serde_json::from_str::<MediaAlbum>(&album_json) {
                                                // Restore the block manager reference
                                                album.block_manager = Some(Arc::clone(&self.block_manager));
                                                self.albums.insert(album_id.clone(), album);
                                                println!("Successfully loaded album: {}", album_id);
                                            } else {
                                                println!("Failed to parse album JSON for {}", album_id);
                                            }
                                        } else {
                                            println!("Failed to decode album data as UTF-8 for {}", album_id);
                                        }
                                    } else {
                                        println!("Failed to read album block for {}", album_id);
                                    }
                                }
                                
                                println!("Loaded {} albums from block storage", self.albums.len());
                            } else {
                                println!("Failed to parse albums index JSON");
                            }
                        } else {
                            println!("Failed to decode index data as UTF-8");
                        }
                    } else {
                        println!("Failed to read albums index block");
                    }
                } else {
                    println!("Invalid block ID length: {}", block_id_bytes.len());
                }
            } else {
                println!("Failed to decode hex block ID");
            }
        } else {
            println!("Albums index file does not exist");
            // TODO: In the future, scan SD cards for album metadata blocks
            println!("🔍 Future: Will scan SD cards for existing album metadata");
        }
        
        Ok(())
    }
    
    async fn save_albums_to_blocks(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // Create an index mapping album IDs to their block IDs
        let mut album_index = HashMap::new();
        
        // Save each album to its own block
        for (album_id, album) in &self.albums {
            // Create a serializable version without the block manager
            let mut serializable_album = album.clone();
            serializable_album.block_manager = None;
            
            let album_json = serde_json::to_string_pretty(&serializable_album)?;
            let album_data = album_json.as_bytes();
            
            // Store the album in a block
            let album_block_id = self.block_manager.write_direct_block(album_data).await?;
            album_index.insert(album_id.clone(), album_block_id);
        }
        
        // Save the index to a block
        let index_json = serde_json::to_string_pretty(&album_index)?;
        let index_data = index_json.as_bytes();
        let index_block_id = self.block_manager.write_direct_block(index_data).await?;
        
        // Store the index block ID in the local directory for bootstrapping
        let local_data_dir = if cfg!(target_os = "macos") {
            PathBuf::from(std::env::var("HOME").unwrap_or_else(|_| ".".to_string()))
                .join("Library/Containers/com.example.soradyneApp/Data/.soradyne_albums")
        } else {
            PathBuf::from(std::env::var("HOME").unwrap_or_else(|_| ".".to_string()))
                .join(".soradyne_albums")
        };
        
        // Ensure the directory exists
        std::fs::create_dir_all(&local_data_dir)?;
        
        let index_file = local_data_dir.join("albums_index.txt");
        std::fs::write(&index_file, hex::encode(index_block_id))?;
        
        self.albums_index_block_id = Some(index_block_id);
        
        println!("Saved {} albums to block storage", self.albums.len());
        
        Ok(())
    }
}

// FFI function to initialize the album system
#[no_mangle]
pub extern "C" fn soradyne_init() -> i32 {
    println!("Starting Soradyne initialization...");
    
    // Create a runtime to handle the async initialization
    let rt = match Runtime::new() {
        Ok(rt) => rt,
        Err(e) => {
            println!("Failed to create Tokio runtime: {}", e);
            return -1;
        }
    };
    
    match rt.block_on(AlbumSystem::new()) {
        Ok(system) => {
            println!("AlbumSystem created successfully");
            unsafe {
                ALBUM_SYSTEM = Some(Arc::new(Mutex::new(system)));
            }
            println!("Soradyne initialization completed successfully");
            0 // Success
        }
        Err(e) => {
            println!("Soradyne initialization failed: {}", e);
            eprintln!("Soradyne initialization failed: {}", e);
            -1 // Error
        }
    }
}

// FFI function to get all albums as JSON
#[no_mangle]
pub extern "C" fn soradyne_get_albums() -> *mut c_char {
    unsafe {
        if let Some(system) = &ALBUM_SYSTEM {
            if let Ok(system) = system.lock() {
                let albums: Vec<serde_json::Value> = system.albums.iter().map(|(id, album)| {
                    serde_json::json!({
                        "id": id,
                        "name": album.metadata.title,
                        "item_count": album.items.len()
                    })
                }).collect();
                
                if let Ok(json) = serde_json::to_string(&albums) {
                    if let Ok(c_string) = CString::new(json) {
                        return c_string.into_raw();
                    }
                }
            }
        }
    }
    
    // Return empty array on error
    let empty = CString::new("[]").unwrap();
    empty.into_raw()
}

// FFI function to create a new album
#[no_mangle]
pub extern "C" fn soradyne_create_album(name_ptr: *const c_char) -> *mut c_char {
    unsafe {
        if let Some(system) = &ALBUM_SYSTEM {
            if let Ok(mut system) = system.lock() {
                let name = CStr::from_ptr(name_ptr).to_string_lossy().to_string();
                let album_id = uuid::Uuid::new_v4().to_string();
                
                let album = MediaAlbum {
                    album_id: album_id.clone(),
                    items: HashMap::new(),
                    metadata: AlbumMetadata {
                        title: name.clone(),
                        created_by: "flutter_user".to_string(),
                        created_at: chrono::Utc::now().timestamp() as u64,
                        shared_with: HashMap::new(),
                    },
                    block_manager: Some(Arc::clone(&system.block_manager)),
                };
                
                system.albums.insert(album_id.clone(), album);
                
                // Save to persistent storage immediately
                let rt = Runtime::new().unwrap();
                if let Err(e) = rt.block_on(async {
                    system.save_albums_to_blocks().await
                }) {
                    println!("Failed to save albums to blocks: {}", e);
                } else {
                    println!("Album created and saved successfully");
                }
                
                let response = serde_json::json!({
                    "id": album_id,
                    "name": name,
                    "item_count": 0
                });
                
                if let Ok(json) = serde_json::to_string(&response) {
                    if let Ok(c_string) = CString::new(json) {
                        return c_string.into_raw();
                    }
                }
            }
        }
    }
    
    let error = CString::new(r#"{"error": "Failed to create album"}"#).unwrap();
    error.into_raw()
}

// FFI function to get album items
#[no_mangle]
pub extern "C" fn soradyne_get_album_items(album_id_ptr: *const c_char) -> *mut c_char {
    unsafe {
        if let Some(system) = &ALBUM_SYSTEM {
            if let Ok(system) = system.lock() {
                let album_id = CStr::from_ptr(album_id_ptr).to_string_lossy().to_string();
                
                if let Some(album) = system.albums.get(&album_id) {
                    let items: Vec<serde_json::Value> = album.items.iter().map(|(media_id, crdt)| {
                        let _state = crdt.reduce();
                        
                        // Extract metadata from operations
                        let mut filename = format!("media_{}", media_id);
                        let mut media_type = "image/jpeg";
                        let mut size = 0u64;
                        
                        for op in crdt.ops() {
                            if op.op_type == "add_media" {
                                if let Some(f) = op.payload.get("filename").and_then(|v| v.as_str()) {
                                    filename = f.to_string();
                                }
                                if let Some(t) = op.payload.get("media_type").and_then(|v| v.as_str()) {
                                    media_type = match t {
                                        "video" => "video/mp4",
                                        "audio" => "audio/mpeg",
                                        _ => "image/jpeg"
                                    };
                                }
                                if let Some(s) = op.payload.get("size").and_then(|v| v.as_u64()) {
                                    size = s;
                                }
                            }
                        }
                        
                        serde_json::json!({
                            "id": media_id,
                            "filename": filename,
                            "media_type": media_type,
                            "size": size,
                            "rotation": _state.rotation,
                            "has_crop": _state.crop.is_some(),
                            "markup_count": _state.markup.len(),
                            "comments": []
                        })
                    }).collect();
                    
                    if let Ok(json) = serde_json::to_string(&items) {
                        if let Ok(c_string) = CString::new(json) {
                            return c_string.into_raw();
                        }
                    }
                }
            }
        }
    }
    
    let empty = CString::new("[]").unwrap();
    empty.into_raw()
}

// FFI function to upload media (takes file path)
#[no_mangle]
pub extern "C" fn soradyne_upload_media(album_id_ptr: *const c_char, file_path_ptr: *const c_char) -> i32 {
    unsafe {
        if let Some(system) = &ALBUM_SYSTEM {
            if let Ok(mut system) = system.lock() {
                let album_id = CStr::from_ptr(album_id_ptr).to_string_lossy().to_string();
                let file_path = CStr::from_ptr(file_path_ptr).to_string_lossy().to_string();
                
                // Read file data
                if let Ok(file_data) = std::fs::read(&file_path) {
                    println!("Read file data: {} bytes", file_data.len());
                    
                    let media_id = uuid::Uuid::new_v4().to_string();
                    let filename = PathBuf::from(&file_path)
                        .file_name()
                        .unwrap_or_default()
                        .to_string_lossy()
                        .to_string();
                    
                    // Store in block storage
                    let block_manager = Arc::clone(&system.block_manager);
                    
                    println!("Attempting to write {} bytes to block storage", file_data.len());
                    
                    // Create temporary runtime for FFI
                    let rt = Runtime::new().unwrap();
                    let result = rt.block_on(async {
                        block_manager.write_direct_block(&file_data).await
                    });
                    
                    match result {
                        Ok(block_id) => {
                            println!("Successfully wrote block: {}", hex::encode(block_id));
                            // Detect media type from file extension
                            let media_type = if filename.to_lowercase().ends_with(".mov") || 
                                               filename.to_lowercase().ends_with(".mp4") ||
                                               filename.to_lowercase().ends_with(".avi") {
                                "video"
                            } else if filename.to_lowercase().ends_with(".mp3") ||
                                     filename.to_lowercase().ends_with(".wav") ||
                                     filename.to_lowercase().ends_with(".flac") {
                                "audio"
                            } else {
                                "image"
                            };
                            
                            // Create operation
                            let op = EditOp {
                                op_id: uuid::Uuid::new_v4(),
                                timestamp: chrono::Utc::now().timestamp() as u64,
                                author: "flutter_user".to_string(),
                                op_type: "add_media".to_string(),
                                payload: serde_json::json!({
                                    "filename": filename,
                                    "block_id": hex::encode(block_id),
                                    "size": file_data.len(),
                                    "media_type": media_type
                                }),
                            };
                            
                            // Add to album
                            if let Some(album) = system.albums.get_mut(&album_id) {
                                let crdt = album.get_or_create(&media_id);
                                if crdt.apply_local(op).is_ok() {
                                    println!("Successfully uploaded media: {}", media_id);
                                    
                                    // Save albums to persistent storage immediately after adding media
                                    let rt = Runtime::new().unwrap();
                                    if let Err(e) = rt.block_on(async {
                                        system.save_albums_to_blocks().await
                                    }) {
                                        println!("Failed to save albums after media upload: {}", e);
                                    } else {
                                        println!("Albums saved successfully after media upload");
                                    }
                                    
                                    return 0; // Success
                                } else {
                                    println!("Failed to apply CRDT operation");
                                }
                            } else {
                                println!("Album not found: {}", album_id);
                            }
                        }
                        Err(e) => {
                            println!("Failed to write block: {}", e);
                        }
                    }
                } else {
                    println!("Failed to read file: {}", file_path);
                }
            }
        }
    }
    
    -1 // Error
}

// FFI function to free strings allocated by Rust
#[no_mangle]
pub extern "C" fn soradyne_free_string(ptr: *mut c_char) {
    unsafe {
        if !ptr.is_null() {
            let _ = CString::from_raw(ptr);
        }
    }
}

// FFI function to get media thumbnail (150px)
#[no_mangle]
pub extern "C" fn soradyne_get_media_thumbnail(album_id_ptr: *const c_char, media_id_ptr: *const c_char, data_ptr: *mut *mut u8, size_ptr: *mut usize) -> i32 {
    get_media_at_resolution(album_id_ptr, media_id_ptr, data_ptr, size_ptr, 150)
}

// FFI function to get media medium resolution (600px)
#[no_mangle]
pub extern "C" fn soradyne_get_media_medium(album_id_ptr: *const c_char, media_id_ptr: *const c_char, data_ptr: *mut *mut u8, size_ptr: *mut usize) -> i32 {
    get_media_at_resolution(album_id_ptr, media_id_ptr, data_ptr, size_ptr, 600)
}

// FFI function to get media high resolution (1200px)
#[no_mangle]
pub extern "C" fn soradyne_get_media_high(album_id_ptr: *const c_char, media_id_ptr: *const c_char, data_ptr: *mut *mut u8, size_ptr: *mut usize) -> i32 {
    get_media_at_resolution(album_id_ptr, media_id_ptr, data_ptr, size_ptr, 1200)
}

// FFI function to get media data (returns raw bytes for images, thumbnails for videos) - kept for compatibility
#[no_mangle]
pub extern "C" fn soradyne_get_media_data(album_id_ptr: *const c_char, media_id_ptr: *const c_char, data_ptr: *mut *mut u8, size_ptr: *mut usize) -> i32 {
    // Default to medium resolution for backward compatibility
    get_media_at_resolution(album_id_ptr, media_id_ptr, data_ptr, size_ptr, 600)
}

// Internal function to get media at specific resolution
fn get_media_at_resolution(album_id_ptr: *const c_char, media_id_ptr: *const c_char, data_ptr: *mut *mut u8, size_ptr: *mut usize, max_size: u32) -> i32 {
    unsafe {
        if let Some(system) = &ALBUM_SYSTEM {
            if let Ok(system) = system.lock() {
                let album_id = CStr::from_ptr(album_id_ptr).to_string_lossy().to_string();
                let media_id = CStr::from_ptr(media_id_ptr).to_string_lossy().to_string();
                
                if let Some(album) = system.albums.get(&album_id) {
                    if let Some(crdt) = album.items.get(&media_id) {
                        let _state = crdt.reduce();
                        
                        // Get the block_id and media_type from the first operation's payload
                        if let Some(op) = crdt.ops().first() {
                            if let Some(block_id_hex) = op.payload.get("block_id").and_then(|v| v.as_str()) {
                                if let Ok(block_id_bytes) = hex::decode(block_id_hex) {
                                    if block_id_bytes.len() == 32 {
                                        let mut block_id = [0u8; 32];
                                        block_id.copy_from_slice(&block_id_bytes);
                                        
                                        let block_manager = Arc::clone(&system.block_manager);
                                        
                                        // Read the media data from block storage
                                        let rt = Runtime::new().unwrap();
                                        if let Ok(media_data) = rt.block_on(async {
                                            block_manager.read_block(&block_id).await
                                        }) {
                                            println!("Successfully read {} bytes from block storage for media {}", media_data.len(), media_id);
                                            
                                            // Get the filename from the operation payload for better type detection
                                            let filename = op.payload.get("filename")
                                                .and_then(|v| v.as_str())
                                                .unwrap_or("");
                                            
                                            println!("Processing media file: {}", filename);
                                            
                                            // Use filename extension for primary detection, fallback to content detection
                                            // Check images FIRST to avoid false positives from audio detection
                                            let resized_data = if filename.to_lowercase().ends_with(".mp4") ||
                                                                 filename.to_lowercase().ends_with(".mov") ||
                                                                 filename.to_lowercase().ends_with(".avi") ||
                                                                 filename.to_lowercase().ends_with(".mkv") ||
                                                                 is_video_file(&media_data) {
                                                println!("Detected video file, generating video thumbnail at size {}", max_size);
                                                generate_video_at_size(&media_data, max_size)
                                            } else if filename.to_lowercase().ends_with(".jpg") ||
                                                     filename.to_lowercase().ends_with(".jpeg") ||
                                                     filename.to_lowercase().ends_with(".png") ||
                                                     filename.to_lowercase().ends_with(".gif") ||
                                                     filename.to_lowercase().ends_with(".bmp") ||
                                                     filename.to_lowercase().ends_with(".webp") ||
                                                     filename.to_lowercase().ends_with(".tiff") ||
                                                     filename.to_lowercase().ends_with(".tif") {
                                                println!("Detected image file (by extension), generating resized image at size {}", max_size);
                                                generate_image_at_size(&media_data, max_size)
                                            } else if filename.to_lowercase().ends_with(".mp3") ||
                                                     filename.to_lowercase().ends_with(".wav") ||
                                                     filename.to_lowercase().ends_with(".flac") ||
                                                     filename.to_lowercase().ends_with(".aac") ||
                                                     filename.to_lowercase().ends_with(".ogg") ||
                                                     is_audio_file(&media_data) {
                                                println!("Detected audio file, generating audio placeholder at size {}", max_size);
                                                create_audio_placeholder_at_size(max_size)
                                            } else {
                                                println!("Detected image file (fallback), generating resized image at size {}", max_size);
                                                generate_image_at_size(&media_data, max_size)
                                            };
                                            
                                            match resized_data {
                                                Ok(data) => {
                                                    let boxed_data = data.into_boxed_slice();
                                                    let len = boxed_data.len();
                                                    let ptr = Box::into_raw(boxed_data) as *mut u8;
                                                
                                                    *data_ptr = ptr;
                                                    *size_ptr = len;
                                                
                                                    return 0; // Success
                                                }
                                                Err(e) => {
                                                    println!("Failed to generate resized media: {}", e);
                                                    
                                                    // Fall back to original data for images only
                                                    if !is_video_file(&media_data) && !is_audio_file(&media_data) {
                                                        let boxed_data = media_data.into_boxed_slice();
                                                        let len = boxed_data.len();
                                                        let ptr = Box::into_raw(boxed_data) as *mut u8;
                                                    
                                                        *data_ptr = ptr;
                                                        *size_ptr = len;
                                                    
                                                        return 0; // Success with original data
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    
    -1 // Error
}

// FFI function to free media data allocated by Rust
#[no_mangle]
pub extern "C" fn soradyne_free_media_data(data_ptr: *mut u8, size: usize) {
    unsafe {
        if !data_ptr.is_null() {
            let _ = Box::from_raw(std::slice::from_raw_parts_mut(data_ptr, size));
        }
    }
}

// FFI function to get storage status
#[no_mangle]
pub extern "C" fn soradyne_get_storage_status() -> *mut c_char {
    // Create a runtime to handle the async discovery
    let rt = match Runtime::new() {
        Ok(rt) => rt,
        Err(e) => {
            println!("Failed to create runtime for storage status: {}", e);
            let error_status = serde_json::json!({
                "available_devices": 0,
                "required_threshold": 3,
                "can_read_data": false,
                "missing_devices": 3,
                "device_paths": [],
                "error": "Failed to create runtime"
            });
            return CString::new(error_status.to_string()).unwrap().into_raw();
        }
    };
    
    // Discover SD cards
    let discovery_result = rt.block_on(async {
        crate::storage::device_identity::discover_soradyne_volumes().await
    });
    
    let status_json = match discovery_result {
        Ok(volumes) => {
            let available_devices = volumes.len();
            let required_threshold = 3;
            let can_read_data = available_devices >= required_threshold;
            let missing_devices = if available_devices < required_threshold {
                required_threshold - available_devices
            } else {
                0
            };
            
            let device_paths: Vec<String> = volumes
                .iter()
                .map(|p| p.to_string_lossy().to_string())
                .collect();
            
            serde_json::json!({
                "available_devices": available_devices,
                "required_threshold": required_threshold,
                "can_read_data": can_read_data,
                "missing_devices": missing_devices,
                "device_paths": device_paths,
            })
        }
        Err(e) => {
            println!("SD card discovery failed: {}", e);
            serde_json::json!({
                "available_devices": 0,
                "required_threshold": 3,
                "can_read_data": false,
                "missing_devices": 3,
                "device_paths": [],
                "error": format!("Discovery failed: {}", e)
            })
        }
    };
    
    let status_str = status_json.to_string();
    CString::new(status_str).unwrap().into_raw()
}

// FFI function to refresh storage
#[no_mangle]
pub extern "C" fn soradyne_refresh_storage() -> i32 {
    // Create a runtime to handle the async discovery
    let rt = match Runtime::new() {
        Ok(rt) => rt,
        Err(e) => {
            println!("Failed to create runtime for storage refresh: {}", e);
            return -1;
        }
    };
    
    // Discover SD cards
    let discovery_result = rt.block_on(async {
        crate::storage::device_identity::discover_soradyne_volumes().await
    });
    
    match discovery_result {
        Ok(volumes) => {
            let available_devices = volumes.len();
            let required_threshold = 3;
            let can_read_data = available_devices >= required_threshold;
            
            println!("Storage refreshed: {} devices found (need {} for operation)", 
                     available_devices, required_threshold);
            
            if can_read_data {
                1 // Ready
            } else {
                0 // Not ready
            }
        }
        Err(e) => {
            println!("Storage refresh failed: {}", e);
            -1 // Error
        }
    }
}

// FFI function to cleanup
#[no_mangle]
pub extern "C" fn soradyne_cleanup() {
    unsafe {
        ALBUM_SYSTEM = None;
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/album/crdt.rs
// =====================================================================

//! Core CRDT traits and types for the album system

use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use uuid::Uuid;
use crate::flow::FlowError;

// === Core Types ===

pub type OpId = Uuid;
pub type ReplicaId = String;
pub type LogicalTime = u64;

// === Operation Trait ===

/// A single CRDT operation that can be applied and merged
pub trait CrdtOp: Clone + Serialize + for<'de> Deserialize<'de> + Send + Sync {
    /// Globally unique ID for this operation
    fn id(&self) -> OpId;
    
    /// Logical timestamp for ordering
    fn timestamp(&self) -> LogicalTime;
    
    /// Which replica created this operation
    fn author(&self) -> ReplicaId;
    
    /// Operation type for interpretation
    fn op_type(&self) -> &str;
}

// === Core CRDT Trait ===

/// A conflict-free replicated data type
pub trait Crdt<Op: CrdtOp>: Clone + Send + Sync {
    type State;
    type Error;

    /// Apply a local operation
    fn apply_local(&mut self, op: Op) -> Result<(), Self::Error>;
    
    /// Merge operations from another replica
    fn merge(&mut self, other: &Self) -> Result<(), Self::Error>;
    
    /// Get all operations (for syncing)
    fn ops(&self) -> &[Op];
    
    /// Reduce operations to current state
    fn reduce(&self) -> Self::State;
    
    /// Check if we have a specific operation
    fn has_op(&self, op_id: &OpId) -> bool;
    
    /// Get operations since a given timestamp (for incremental sync)
    fn ops_since(&self, timestamp: LogicalTime) -> Vec<Op>;
}

// === Reducer Trait ===

/// Interprets operations into displayable state
pub trait Reducer<Op: CrdtOp> {
    type State;
    type Error;
    
    /// Build state from a sequence of operations
    fn reduce(ops: &[Op]) -> Result<Self::State, Self::Error>;
    
    /// Incrementally apply a single operation to existing state
    fn apply_to_state(state: &mut Self::State, op: &Op) -> Result<(), Self::Error>;
}

// === Collection Management ===

/// Manages a collection of CRDTs (like an album of media items)
pub trait CrdtCollection<Key, Op: CrdtOp>: Send + Sync 
where 
    Key: Clone + Eq + std::hash::Hash + Send + Sync,
{
    type ItemCrdt: Crdt<Op>;
    type Error;
    
    /// Get or create a CRDT for a specific item
    fn get_or_create(&mut self, key: &Key) -> &mut Self::ItemCrdt;
    
    /// Get an existing CRDT (read-only)
    fn get(&self, key: &Key) -> Option<&Self::ItemCrdt>;
    
    /// Apply an operation to a specific item
    fn apply_to_item(&mut self, key: &Key, op: Op) -> Result<(), Self::Error>;
    
    /// Merge another collection
    fn merge_collection(&mut self, other: &Self) -> Result<(), Self::Error>;
    
    /// List all keys
    fn keys(&self) -> Vec<Key>;
    
    /// Get the state of all items
    fn reduce_all(&self) -> HashMap<Key, <Self::ItemCrdt as Crdt<Op>>::State>;
}

// === Error Types ===

#[derive(Debug, thiserror::Error)]
pub enum CrdtError {
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Flow error: {0}")]
    Flow(#[from] FlowError),
    
    #[error("Invalid operation: {0}")]
    InvalidOperation(String),
    
    #[error("Operation not found: {0}")]
    OperationNotFound(OpId),
}



// =====================================================================
// FILE: packages/soradyne_core/src/album/renderer.rs
// =====================================================================

//! Media rendering system for generating thumbnails and multi-resolution outputs
//! 
//! This module handles rendering media with applied edits (crop, rotation, markup)
//! at different resolutions, with proper composition of multiple edits.

use super::album::{MediaState, CropData, MarkupElement};
use super::operations::MarkupType;
use serde::{Serialize, Deserialize};
use image::{DynamicImage, Rgba, RgbaImage, imageops::FilterType};
use std::process::Command;
use std::path::Path;
use imageproc::drawing::{draw_filled_circle_mut, draw_hollow_circle_mut, draw_filled_rect_mut, draw_hollow_rect_mut, draw_line_segment_mut};
use imageproc::rect::Rect;

// === Rendering Configuration ===

#[derive(Debug, Clone)]
pub struct RenderConfig {
    pub thumbnail_size: u32,      // e.g., 150px
    pub preview_size: u32,        // e.g., 800px  
    pub max_full_size: u32,       // e.g., 2048px
}

impl Default for RenderConfig {
    fn default() -> Self {
        Self {
            thumbnail_size: 150,
            preview_size: 800,
            max_full_size: 2048,
        }
    }
}

#[derive(Debug, Clone)]
pub enum RenderResolution {
    Thumbnail,
    Preview,
    Full,
    Custom(u32, u32),
}

// === Markup Data Structures ===

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct CircleMarkup {
    pub center_x: f32,      // Normalized coordinates (0.0-1.0)
    pub center_y: f32,
    pub radius: f32,        // Normalized radius
    pub color: [u8; 4],     // RGBA
    pub filled: bool,
    pub stroke_width: f32,  // Normalized stroke width
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct RectangleMarkup {
    pub x: f32,             // Normalized coordinates
    pub y: f32,
    pub width: f32,
    pub height: f32,
    pub color: [u8; 4],
    pub filled: bool,
    pub stroke_width: f32,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct ArrowMarkup {
    pub start_x: f32,       // Normalized coordinates
    pub start_y: f32,
    pub end_x: f32,
    pub end_y: f32,
    pub color: [u8; 4],
    pub stroke_width: f32,
    pub arrow_head_size: f32,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct TextMarkup {
    pub x: f32,             // Normalized coordinates
    pub y: f32,
    pub text: String,
    pub font_size: f32,     // Normalized font size
    pub color: [u8; 4],
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct FreehandMarkup {
    pub points: Vec<(f32, f32)>,  // Normalized coordinates
    pub color: [u8; 4],
    pub stroke_width: f32,
}

// === Main Renderer ===

pub struct MediaRenderer {
    base_image: DynamicImage,
    config: RenderConfig,
}

#[derive(Debug, thiserror::Error)]
pub enum RenderError {
    #[error("Image processing error: {0}")]
    ImageError(#[from] image::ImageError),
    
    #[error("Invalid markup data: {0}")]
    InvalidMarkup(String),
    
    #[error("Rendering failed: {0}")]
    RenderFailed(String),
}

impl MediaRenderer {
    pub fn new(base_image_data: &[u8], config: Option<RenderConfig>) -> Result<Self, RenderError> {
        let base_image = image::load_from_memory(base_image_data)?;
        let config = config.unwrap_or_default();
        
        Ok(Self {
            base_image,
            config,
        })
    }
    
    /// Render media at specified resolution with all edits applied
    pub fn render(&self, media_state: &MediaState, resolution: RenderResolution) -> Result<Vec<u8>, RenderError> {
        // Step 1: Determine target dimensions
        let (target_width, target_height) = self.calculate_dimensions(resolution);
        
        // Step 2: Apply transforms (crop + rotation) - LWW semantics
        let mut canvas = self.apply_transforms(media_state, target_width, target_height)?;
        
        // Step 3: Apply all markup layers in chronological order
        self.apply_markup_layers(&mut canvas, media_state)?;
        
        // Step 4: Encode to bytes
        let mut output = Vec::new();
        canvas.write_to(&mut std::io::Cursor::new(&mut output), image::ImageOutputFormat::Png)?;
        
        Ok(output)
    }
    
    /// Convenience method for thumbnail generation
    pub fn render_thumbnail(&self, media_state: &MediaState) -> Result<Vec<u8>, RenderError> {
        self.render(media_state, RenderResolution::Thumbnail)
    }
    
    /// Convenience method for preview generation
    pub fn render_preview(&self, media_state: &MediaState) -> Result<Vec<u8>, RenderError> {
        self.render(media_state, RenderResolution::Preview)
    }
    
    /// Convenience method for full resolution
    pub fn render_full(&self, media_state: &MediaState) -> Result<Vec<u8>, RenderError> {
        self.render(media_state, RenderResolution::Full)
    }
    
    /// Generate a thumbnail from video file using ffmpeg
    pub fn generate_video_thumbnail(video_path: &Path, timestamp_seconds: f64) -> Result<Vec<u8>, RenderError> {
        let output_path = std::env::temp_dir().join(format!("thumb_{}.jpg", uuid::Uuid::new_v4()));
        
        println!("Running ffmpeg to generate thumbnail...");
        println!("Input: {:?}", video_path);
        println!("Output: {:?}", output_path);
        
        // Check if ffmpeg is available
        let ffmpeg_check = Command::new("ffmpeg")
            .arg("-version")
            .output();
            
        match ffmpeg_check {
            Ok(output) if output.status.success() => {
                println!("ffmpeg is available");
            }
            Ok(output) => {
                println!("ffmpeg version check failed: {}", String::from_utf8_lossy(&output.stderr));
            }
            Err(e) => {
                return Err(RenderError::RenderFailed(format!("ffmpeg not found: {}", e)));
            }
        }
        
        let output = Command::new("ffmpeg")
            .args(&[
                "-i", video_path.to_str().unwrap(),
                "-ss", &timestamp_seconds.to_string(),
                "-vframes", "1",
                "-q:v", "2",
                "-y",
                output_path.to_str().unwrap(),
            ])
            .output()
            .map_err(|e| RenderError::RenderFailed(format!("Failed to run ffmpeg: {}", e)))?;
        
        println!("ffmpeg exit code: {:?}", output.status.code());
        println!("ffmpeg stdout: {}", String::from_utf8_lossy(&output.stdout));
        println!("ffmpeg stderr: {}", String::from_utf8_lossy(&output.stderr));
        
        if !output.status.success() {
            return Err(RenderError::RenderFailed(format!(
                "ffmpeg failed with exit code {:?}: {}", 
                output.status.code(),
                String::from_utf8_lossy(&output.stderr)
            )));
        }
        
        if !output_path.exists() {
            return Err(RenderError::RenderFailed(
                "ffmpeg completed but output file was not created".to_string()
            ));
        }
        
        let thumbnail_data = std::fs::read(&output_path)
            .map_err(|e| RenderError::RenderFailed(format!("Failed to read thumbnail: {}", e)))?;
        
        println!("Successfully read thumbnail: {} bytes", thumbnail_data.len());
        
        // Clean up temp file
        let _ = std::fs::remove_file(&output_path);
        
        Ok(thumbnail_data)
    }
    
    fn calculate_dimensions(&self, resolution: RenderResolution) -> (u32, u32) {
        let (base_width, base_height) = (self.base_image.width(), self.base_image.height());
        
        match resolution {
            RenderResolution::Thumbnail => {
                let size = self.config.thumbnail_size;
                self.fit_to_square(base_width, base_height, size)
            }
            RenderResolution::Preview => {
                let max_size = self.config.preview_size;
                self.fit_to_max(base_width, base_height, max_size)
            }
            RenderResolution::Full => {
                let max_size = self.config.max_full_size;
                if base_width.max(base_height) <= max_size {
                    (base_width, base_height)
                } else {
                    self.fit_to_max(base_width, base_height, max_size)
                }
            }
            RenderResolution::Custom(width, height) => (width, height),
        }
    }
    
    fn fit_to_square(&self, width: u32, height: u32, size: u32) -> (u32, u32) {
        let aspect_ratio = width as f32 / height as f32;
        if aspect_ratio > 1.0 {
            (size, (size as f32 / aspect_ratio) as u32)
        } else {
            ((size as f32 * aspect_ratio) as u32, size)
        }
    }
    
    fn fit_to_max(&self, width: u32, height: u32, max_size: u32) -> (u32, u32) {
        if width.max(height) <= max_size {
            return (width, height);
        }
        
        let scale = max_size as f32 / width.max(height) as f32;
        ((width as f32 * scale) as u32, (height as f32 * scale) as u32)
    }
    
    fn apply_transforms(&self, media_state: &MediaState, target_width: u32, target_height: u32) -> Result<DynamicImage, RenderError> {
        let mut image = self.base_image.clone();
        
        // Apply crop first (LWW - use the most recent crop)
        if let Some(crop) = &media_state.crop {
            image = self.apply_crop(&image, crop)?;
        }
        
        // Apply rotation (LWW - use the current rotation value)
        if media_state.rotation != 0.0 {
            image = self.apply_rotation(&image, media_state.rotation)?;
        }
        
        // Resize to target dimensions
        Ok(image.resize_exact(target_width, target_height, FilterType::Lanczos3))
    }
    
    fn apply_crop(&self, image: &DynamicImage, crop: &CropData) -> Result<DynamicImage, RenderError> {
        let (width, height) = (image.width(), image.height());
        
        let x = (crop.left * width as f32) as u32;
        let y = (crop.top * height as f32) as u32;
        let crop_width = ((crop.right - crop.left) * width as f32) as u32;
        let crop_height = ((crop.bottom - crop.top) * height as f32) as u32;
        
        // Ensure crop bounds are valid
        let x = x.min(width.saturating_sub(1));
        let y = y.min(height.saturating_sub(1));
        let crop_width = crop_width.min(width - x);
        let crop_height = crop_height.min(height - y);
        
        Ok(image.crop_imm(x, y, crop_width, crop_height))
    }
    
    fn apply_rotation(&self, image: &DynamicImage, angle: f32) -> Result<DynamicImage, RenderError> {
        // Normalize angle to 0-360 range
        let normalized_angle = angle % 360.0;
        let normalized_angle = if normalized_angle < 0.0 { normalized_angle + 360.0 } else { normalized_angle };
        
        // Apply rotation in 90-degree increments for now (can be enhanced later)
        match normalized_angle as i32 {
            0..=44 | 316..=360 => Ok(image.clone()),
            45..=134 => Ok(image.rotate90()),
            135..=224 => Ok(image.rotate180()),
            225..=315 => Ok(image.rotate270()),
            _ => Ok(image.clone()),
        }
    }
    
    fn apply_markup_layers(&self, canvas: &mut DynamicImage, media_state: &MediaState) -> Result<(), RenderError> {
        // Convert to RGBA for drawing operations
        let mut rgba_image = canvas.to_rgba8();
        let (width, height) = (rgba_image.width(), rgba_image.height());
        
        // Sort markup by timestamp to apply in chronological order
        let mut sorted_markup = media_state.markup.clone();
        sorted_markup.sort_by_key(|m| m.timestamp);
        
        // Apply each markup element
        for markup in &sorted_markup {
            // Skip deleted markup
            if media_state.deleted_items.contains(&markup.id) {
                continue;
            }
            
            self.apply_single_markup(&mut rgba_image, markup, width, height)?;
        }
        
        *canvas = DynamicImage::ImageRgba8(rgba_image);
        Ok(())
    }
    
    fn apply_single_markup(&self, canvas: &mut RgbaImage, markup: &MarkupElement, width: u32, height: u32) -> Result<(), RenderError> {
        match markup.markup_type {
            MarkupType::Circle => self.draw_circle(canvas, &markup.data, width, height)?,
            MarkupType::Rectangle => self.draw_rectangle(canvas, &markup.data, width, height)?,
            MarkupType::Arrow => self.draw_arrow(canvas, &markup.data, width, height)?,
            MarkupType::Text => self.draw_text(canvas, &markup.data, width, height)?,
            MarkupType::Freehand => self.draw_freehand(canvas, &markup.data, width, height)?,
        }
        Ok(())
    }
    
    fn draw_circle(&self, canvas: &mut RgbaImage, data: &serde_json::Value, width: u32, height: u32) -> Result<(), RenderError> {
        let circle: CircleMarkup = serde_json::from_value(data.clone())
            .map_err(|e| RenderError::InvalidMarkup(format!("Circle markup: {}", e)))?;
        
        let center_x = (circle.center_x * width as f32) as i32;
        let center_y = (circle.center_y * height as f32) as i32;
        let radius = (circle.radius * width.min(height) as f32) as i32;
        let color = Rgba(circle.color);
        
        if circle.filled {
            draw_filled_circle_mut(canvas, (center_x, center_y), radius, color);
        } else {
            draw_hollow_circle_mut(canvas, (center_x, center_y), radius, color);
        }
        
        Ok(())
    }
    
    fn draw_rectangle(&self, canvas: &mut RgbaImage, data: &serde_json::Value, width: u32, height: u32) -> Result<(), RenderError> {
        let rect: RectangleMarkup = serde_json::from_value(data.clone())
            .map_err(|e| RenderError::InvalidMarkup(format!("Rectangle markup: {}", e)))?;
        
        let x = (rect.x * width as f32) as i32;
        let y = (rect.y * height as f32) as i32;
        let rect_width = (rect.width * width as f32) as u32;
        let rect_height = (rect.height * height as f32) as u32;
        let color = Rgba(rect.color);
        
        let rectangle = Rect::at(x, y).of_size(rect_width, rect_height);
        
        if rect.filled {
            draw_filled_rect_mut(canvas, rectangle, color);
        } else {
            draw_hollow_rect_mut(canvas, rectangle, color);
        }
        
        Ok(())
    }
    
    fn draw_arrow(&self, canvas: &mut RgbaImage, data: &serde_json::Value, width: u32, height: u32) -> Result<(), RenderError> {
        let arrow: ArrowMarkup = serde_json::from_value(data.clone())
            .map_err(|e| RenderError::InvalidMarkup(format!("Arrow markup: {}", e)))?;
        
        let start_x = (arrow.start_x * width as f32) as f32;
        let start_y = (arrow.start_y * height as f32) as f32;
        let end_x = (arrow.end_x * width as f32) as f32;
        let end_y = (arrow.end_y * height as f32) as f32;
        let color = Rgba(arrow.color);
        
        // Draw main line
        draw_line_segment_mut(canvas, (start_x, start_y), (end_x, end_y), color);
        
        // TODO: Add arrow head drawing
        // For now, just draw the line
        
        Ok(())
    }
    
    fn draw_text(&self, _canvas: &mut RgbaImage, data: &serde_json::Value, _width: u32, _height: u32) -> Result<(), RenderError> {
        let _text: TextMarkup = serde_json::from_value(data.clone())
            .map_err(|e| RenderError::InvalidMarkup(format!("Text markup: {}", e)))?;
        
        // TODO: Implement text rendering with ab_glyph
        // For now, skip text rendering
        
        Ok(())
    }
    
    fn draw_freehand(&self, canvas: &mut RgbaImage, data: &serde_json::Value, width: u32, height: u32) -> Result<(), RenderError> {
        let freehand: FreehandMarkup = serde_json::from_value(data.clone())
            .map_err(|e| RenderError::InvalidMarkup(format!("Freehand markup: {}", e)))?;
        
        let color = Rgba(freehand.color);
        
        // Draw lines between consecutive points
        for window in freehand.points.windows(2) {
            let start_x = (window[0].0 * width as f32) as f32;
            let start_y = (window[0].1 * height as f32) as f32;
            let end_x = (window[1].0 * width as f32) as f32;
            let end_y = (window[1].1 * height as f32) as f32;
            
            draw_line_segment_mut(canvas, (start_x, start_y), (end_x, end_y), color);
        }
        
        Ok(())
    }
}

// === Integration with MediaState ===

impl super::album::MediaState {
    /// Render this media state at the specified resolution
    pub fn render(&self, base_image_data: &[u8], resolution: RenderResolution) -> Result<Vec<u8>, RenderError> {
        let renderer = MediaRenderer::new(base_image_data, None)?;
        renderer.render(self, resolution)
    }
    
    /// Generate thumbnail for this media state
    pub fn render_thumbnail(&self, base_image_data: &[u8]) -> Result<Vec<u8>, RenderError> {
        let renderer = MediaRenderer::new(base_image_data, None)?;
        renderer.render_thumbnail(self)
    }
    
    /// Generate preview for this media state
    pub fn render_preview(&self, base_image_data: &[u8]) -> Result<Vec<u8>, RenderError> {
        let renderer = MediaRenderer::new(base_image_data, None)?;
        renderer.render_preview(self)
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/album/sync.rs
// =====================================================================

//! Synchronization utilities for albums

use super::crdt::{CrdtError, ReplicaId, LogicalTime, CrdtCollection};
use super::album::{MediaAlbum, AlbumMetadata};
use super::operations::{EditOp, MediaId, MediaType};
use crate::storage::block_manager::BlockManager;
use std::sync::Arc;
use serde::{Serialize, Deserialize};

// === Sync Message Types ===

#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum SyncMessage {
    /// Request sync for a specific album
    SyncRequest {
        album_id: String,
        last_seen: LogicalTime,
    },
    
    /// Response with album updates
    SyncResponse {
        album_id: String,
        album_data: Option<Vec<u8>>, // Serialized album if full sync
        incremental_ops: Vec<(MediaId, Vec<EditOp>)>, // Per-media ops if incremental
    },
    
    /// Announce new album
    AlbumAnnouncement {
        album_id: String,
        title: String,
        created_by: ReplicaId,
    },
    
    /// Request media data
    MediaRequest {
        block_id: [u8; 32],
    },
    
    /// Response with media data
    MediaResponse {
        block_id: [u8; 32],
        data: Vec<u8>,
    },
}

// === Album Sync Manager ===

pub struct AlbumSyncManager {
    albums: std::collections::HashMap<String, MediaAlbum>,
    block_manager: Arc<BlockManager>,
    replica_id: ReplicaId,
}

impl AlbumSyncManager {
    pub fn new(block_manager: Arc<BlockManager>, replica_id: ReplicaId) -> Self {
        Self {
            albums: std::collections::HashMap::new(),
            block_manager,
            replica_id,
        }
    }
    
    /// Create a new album
    pub fn create_album(&mut self, title: String) -> Result<String, CrdtError> {
        let album_id = uuid::Uuid::new_v4().to_string();
        let album = MediaAlbum::new(album_id.clone(), title, self.replica_id.clone())
            .with_block_manager(self.block_manager.clone());
        
        self.albums.insert(album_id.clone(), album);
        Ok(album_id)
    }
    
    /// Get an album
    pub fn get_album(&self, album_id: &str) -> Option<&MediaAlbum> {
        self.albums.get(album_id)
    }
    
    /// Get a mutable album
    pub fn get_album_mut(&mut self, album_id: &str) -> Option<&mut MediaAlbum> {
        self.albums.get_mut(album_id)
    }
    
    /// Add media to an album
    pub async fn add_media_to_album(
        &mut self, 
        album_id: &str, 
        media_id: MediaId,
        media_data: &[u8],
        media_type: MediaType,
        filename: String
    ) -> Result<(), CrdtError> {
        // Store media in block system
        let block_file = crate::storage::block_file::BlockFile::new(self.block_manager.clone());
        block_file.append(media_data).await.map_err(CrdtError::Flow)?;
        
        let block_id = block_file.root_block().await
            .ok_or_else(|| CrdtError::InvalidOperation("Failed to get block ID".to_string()))?;
        
        // Create set_media operation
        let op = EditOp::set_media(
            self.replica_id.clone(),
            block_id,
            media_type,
            filename,
            media_data.len()
        );
        
        // Apply to album
        if let Some(album) = self.albums.get_mut(album_id) {
            album.apply_to_item(&media_id, op)?;
        }
        
        Ok(())
    }
    
    /// Apply an operation to a media item
    pub fn apply_operation(
        &mut self, 
        album_id: &str, 
        media_id: &MediaId, 
        op: EditOp
    ) -> Result<(), CrdtError> {
        if let Some(album) = self.albums.get_mut(album_id) {
            album.apply_to_item(media_id, op)
        } else {
            Err(CrdtError::InvalidOperation(format!("Album {} not found", album_id)))
        }
    }
    
    /// Merge an album from another replica
    pub fn merge_album(&mut self, other_album: MediaAlbum) -> Result<(), CrdtError> {
        let album_id = other_album.album_id.clone();
        
        if let Some(existing_album) = self.albums.get_mut(&album_id) {
            existing_album.merge_collection(&other_album)?;
        } else {
            self.albums.insert(album_id, other_album);
        }
        
        Ok(())
    }
    
    /// Generate sync message for an album
    pub fn generate_sync_request(&self, album_id: &str, last_seen: LogicalTime) -> SyncMessage {
        SyncMessage::SyncRequest {
            album_id: album_id.to_string(),
            last_seen,
        }
    }
    
    /// Handle incoming sync request
    pub fn handle_sync_request(&self, album_id: &str, _last_seen: LogicalTime) -> Option<SyncMessage> {
        if let Some(album) = self.albums.get(album_id) {
            // For now, always send full album data
            // TODO: Implement incremental sync based on last_seen
            if let Ok(album_data) = album.to_bytes() {
                return Some(SyncMessage::SyncResponse {
                    album_id: album_id.to_string(),
                    album_data: Some(album_data),
                    incremental_ops: Vec::new(),
                });
            }
        }
        None
    }
    
    /// Handle incoming sync response
    pub fn handle_sync_response(&mut self, response: SyncMessage) -> Result<(), CrdtError> {
        match response {
            SyncMessage::SyncResponse { album_id, album_data, incremental_ops } => {
                if let Some(album_data) = album_data {
                    // Full sync
                    let other_album = MediaAlbum::from_bytes(&album_data)?;
                    self.merge_album(other_album)?;
                } else {
                    // Incremental sync
                    if let Some(album) = self.albums.get_mut(&album_id) {
                        for (media_id, ops) in incremental_ops {
                            for op in ops {
                                album.apply_to_item(&media_id, op)?;
                            }
                        }
                    }
                }
            }
            _ => return Err(CrdtError::InvalidOperation("Expected sync response".to_string())),
        }
        Ok(())
    }
    
    /// Get media data by block ID
    pub async fn get_media_data(&self, block_id: &[u8; 32]) -> Result<Vec<u8>, CrdtError> {
        self.block_manager.read_block(block_id).await.map_err(CrdtError::Flow)
    }
    
    /// List all albums
    pub fn list_albums(&self) -> Vec<(String, &AlbumMetadata)> {
        self.albums.iter()
            .map(|(id, album)| (id.clone(), &album.metadata))
            .collect()
    }
}

// === Test Utilities ===

#[cfg(test)]
pub mod test_utils {
    use super::*;
    use tempfile::TempDir;
    
    pub fn create_test_sync_manager() -> (AlbumSyncManager, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let test_dir = temp_dir.path().to_path_buf();
        
        // Create rimsd directories
        let mut rimsd_dirs = Vec::new();
        for i in 0..3 {
            let device_dir = test_dir.join(format!("rimsd_{}", i));
            let rimsd_dir = device_dir.join(".rimsd");
            std::fs::create_dir_all(&rimsd_dir).unwrap();
            rimsd_dirs.push(rimsd_dir);
        }
        
        let metadata_path = test_dir.join("metadata.json");
        let block_manager = Arc::new(BlockManager::new(
            rimsd_dirs,
            metadata_path,
            2, // threshold
            3, // total_shards
        ).unwrap());
        
        let sync_manager = AlbumSyncManager::new(block_manager, "test_replica".to_string());
        
        (sync_manager, temp_dir)
    }
    
    pub fn create_test_sync_manager_with_distinct_devices() -> (AlbumSyncManager, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let test_dir = temp_dir.path().to_path_buf();
        
        // Create rimsd directories with unique device signatures
        let mut rimsd_dirs = Vec::new();
        for i in 0..3 {
            let device_dir = test_dir.join(format!("rimsd_{}", i));
            let rimsd_dir = device_dir.join(".rimsd");
            std::fs::create_dir_all(&rimsd_dir).unwrap();
            
            // Create Soradyne device ID file (most important identifier)
            let soradyne_device_id_file = rimsd_dir.join("soradyne_device_id.txt");
            std::fs::write(&soradyne_device_id_file, uuid::Uuid::new_v4().to_string()).unwrap();
            
            // Create a unique device signature file for each "device"
            let device_signature = rimsd_dir.join("device_signature.txt");
            std::fs::write(&device_signature, format!("device-{}-{}", i, uuid::Uuid::new_v4())).unwrap();
            
            // Create a mock filesystem UUID file
            let fs_uuid_file = rimsd_dir.join("fs_uuid.txt");
            std::fs::write(&fs_uuid_file, format!("fs-uuid-{}-{}", i, uuid::Uuid::new_v4())).unwrap();
            
            // Create mock hardware info
            let hw_info_file = rimsd_dir.join("hardware_info.txt");
            std::fs::write(&hw_info_file, format!("hw-serial-{}-manufacturer-{}", i, i)).unwrap();
            
            rimsd_dirs.push(rimsd_dir);
        }
        
        let metadata_path = test_dir.join("metadata.json");
        let block_manager = Arc::new(BlockManager::new(
            rimsd_dirs,
            metadata_path,
            2, // threshold
            3, // total_shards
        ).unwrap());
        
        let sync_manager = AlbumSyncManager::new(block_manager, "test_replica".to_string());
        
        (sync_manager, temp_dir)
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/album/mod.rs
// =====================================================================

//! Album CRDT implementation for collaborative photo/video/audio albums
//! 
//! This module provides CRDT-based data structures for managing shared albums
//! with edit histories, comments, reactions, and media metadata.

pub mod crdt;
pub mod album;
pub mod operations;
pub mod sync;
pub mod renderer;

pub use crdt::*;
pub use album::*;
pub use operations::*;
pub use sync::*;
pub use renderer::*;


// =====================================================================
// FILE: packages/soradyne_core/src/album/operations.rs
// =====================================================================

//! Operation types for album editing

use super::crdt::*;
use serde::{Serialize, Deserialize};
use uuid::Uuid;

// Type aliases
pub type MediaId = String;
pub type UserId = String;

#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub enum Permission {
    View,
    Comment,
    Edit,
    Admin,
}

// === Edit Operation ===

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct EditOp {
    pub op_id: OpId,
    pub timestamp: LogicalTime,
    pub author: ReplicaId,
    pub op_type: String,
    pub payload: serde_json::Value,
}

impl CrdtOp for EditOp {
    fn id(&self) -> OpId { 
        self.op_id 
    }
    
    fn timestamp(&self) -> LogicalTime { 
        self.timestamp 
    }
    
    fn author(&self) -> ReplicaId { 
        self.author.clone() 
    }
    
    fn op_type(&self) -> &str { 
        &self.op_type 
    }
}

impl EditOp {
    pub fn new(author: ReplicaId, op_type: String, payload: serde_json::Value) -> Self {
        Self {
            op_id: Uuid::new_v4(),
            timestamp: chrono::Utc::now().timestamp_millis() as u64,
            author,
            op_type,
            payload,
        }
    }
}

// === Operation Payloads ===

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct SetMediaPayload {
    pub block_id: [u8; 32],
    pub media_type: MediaType,
    pub filename: String,
    pub size: usize,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub enum MediaType {
    Photo,
    Video,
    Audio,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct CommentPayload {
    pub text: String,
    pub parent: Option<OpId>, // For threaded comments
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct ReactionPayload {
    pub target: OpId, // What we're reacting to
    pub emoji: String,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct CropPayload {
    pub left: f32,
    pub top: f32,
    pub right: f32,
    pub bottom: f32,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct RotatePayload {
    pub angle: f32, // degrees
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct MarkupPayload {
    pub markup_type: MarkupType,
    pub data: serde_json::Value,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub enum MarkupType {
    Arrow,
    Circle,
    Rectangle,
    Text,
    Freehand,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct SharePayload {
    pub user_id: UserId,
    pub permission: Permission,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct DeletePayload {
    pub target: OpId, // What we're deleting
}

// === Operation Builders ===

impl EditOp {
    pub fn set_media(author: ReplicaId, block_id: [u8; 32], media_type: MediaType, filename: String, size: usize) -> Self {
        let payload = SetMediaPayload { block_id, media_type, filename, size };
        Self::new(author, "set_media".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn add_comment(author: ReplicaId, text: String, parent: Option<OpId>) -> Self {
        let payload = CommentPayload { text, parent };
        Self::new(author, "add_comment".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn add_reaction(author: ReplicaId, target: OpId, emoji: String) -> Self {
        let payload = ReactionPayload { target, emoji };
        Self::new(author, "add_reaction".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn set_crop(author: ReplicaId, left: f32, top: f32, right: f32, bottom: f32) -> Self {
        let payload = CropPayload { left, top, right, bottom };
        Self::new(author, "set_crop".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn rotate(author: ReplicaId, angle: f32) -> Self {
        let payload = RotatePayload { angle };
        Self::new(author, "rotate".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn add_markup(author: ReplicaId, markup_type: MarkupType, data: serde_json::Value) -> Self {
        let payload = MarkupPayload { markup_type, data };
        Self::new(author, "add_markup".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn share_with(author: ReplicaId, user_id: UserId, permission: Permission) -> Self {
        let payload = SharePayload { user_id, permission };
        Self::new(author, "share_with".to_string(), serde_json::to_value(payload).unwrap())
    }
    
    pub fn delete(author: ReplicaId, target: OpId) -> Self {
        let payload = DeletePayload { target };
        Self::new(author, "delete".to_string(), serde_json::to_value(payload).unwrap())
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/album/album.rs
// =====================================================================

//! Album and media item implementations

use super::crdt::*;
use super::operations::{self, EditOp, MediaType, MarkupType};
use std::collections::{HashMap, HashSet};
use serde::{Serialize, Deserialize};
use crate::storage::block_manager::BlockManager;
use std::sync::Arc;

// Use operations module types to avoid ambiguity
use operations::{MediaId, UserId, Permission};
use operations::{SetMediaPayload, CommentPayload, DeletePayload, ReactionPayload};
use operations::{CropPayload, RotatePayload, MarkupPayload, SharePayload};


// === Simple Log CRDT Implementation ===

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct LogCrdt {
    ops: Vec<EditOp>,
}

impl LogCrdt {
    pub fn new() -> Self {
        Self { ops: Vec::new() }
    }
    
    pub fn get_state(&self) -> Result<MediaState, CrdtError> {
        Ok(self.reduce())
    }
    
    fn sort_ops(&mut self) {
        self.ops.sort_by(|a, b| {
            a.timestamp().cmp(&b.timestamp())
                .then_with(|| a.author().cmp(&b.author()))
                .then_with(|| a.id().cmp(&b.id()))
        });
        
        // Deduplicate by op_id
        self.ops.dedup_by(|a, b| a.id() == b.id());
    }
}

impl Crdt<EditOp> for LogCrdt {
    type State = MediaState;
    type Error = CrdtError;
    
    fn apply_local(&mut self, op: EditOp) -> Result<(), Self::Error> {
        if !self.has_op(&op.id()) {
            self.ops.push(op);
            self.sort_ops();
        }
        Ok(())
    }
    
    fn merge(&mut self, other: &Self) -> Result<(), Self::Error> {
        for op in &other.ops {
            if !self.has_op(&op.id()) {
                self.ops.push(op.clone());
            }
        }
        self.sort_ops();
        Ok(())
    }
    
    fn ops(&self) -> &[EditOp] {
        &self.ops
    }
    
    fn reduce(&self) -> Self::State {
        MediaReducer::reduce(&self.ops).unwrap_or_default()
    }
    
    fn has_op(&self, op_id: &OpId) -> bool {
        self.ops.iter().any(|op| op.id() == *op_id)
    }
    
    fn ops_since(&self, timestamp: LogicalTime) -> Vec<EditOp> {
        self.ops.iter()
            .filter(|op| op.timestamp() > timestamp)
            .cloned()
            .collect()
    }
}

// === Media State ===

#[derive(Clone, Debug, Default, Serialize, Deserialize)]
pub struct MediaState {
    pub media: Option<MediaInfo>,
    pub comments: Vec<Comment>,
    pub reactions: HashMap<String, Vec<ReplicaId>>, // emoji -> users
    pub crop: Option<CropData>,
    pub rotation: f32,
    pub markup: Vec<MarkupElement>,
    pub deleted_items: HashSet<OpId>,
    pub shared_with: HashMap<UserId, Permission>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MediaInfo {
    pub block_id: [u8; 32],
    pub media_type: MediaType,
    pub filename: String,
    pub mime_type: String,
    pub size: usize,
    pub added_by: ReplicaId,
    pub added_at: LogicalTime,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Comment {
    pub id: OpId,
    pub author: ReplicaId,
    pub text: String,
    pub parent: Option<OpId>,
    pub timestamp: LogicalTime,
    pub deleted: bool,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct CropData {
    pub left: f32,
    pub top: f32,
    pub right: f32,
    pub bottom: f32,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MarkupElement {
    pub id: OpId,
    pub markup_type: MarkupType,
    pub data: serde_json::Value,
    pub author: ReplicaId,
    pub timestamp: LogicalTime,
}


// === Media Reducer ===

pub struct MediaReducer;

impl Reducer<EditOp> for MediaReducer {
    type State = MediaState;
    type Error = CrdtError;
    
    fn reduce(ops: &[EditOp]) -> Result<Self::State, Self::Error> {
        let mut state = MediaState::default();
        
        for op in ops {
            Self::apply_to_state(&mut state, op)?;
        }
        
        Ok(state)
    }
    
    fn apply_to_state(state: &mut Self::State, op: &EditOp) -> Result<(), Self::Error> {
        match op.op_type.as_str() {
            "add_media" | "set_media" => {
                // Handle both add_media (from web interface) and set_media operations
                if let Some(block_id_hex) = op.payload.get("block_id").and_then(|v| v.as_str()) {
                    if let Ok(block_id_bytes) = hex::decode(block_id_hex) {
                        if block_id_bytes.len() == 32 {
                            let mut block_id = [0u8; 32];
                            block_id.copy_from_slice(&block_id_bytes);
                            
                            let filename = op.payload.get("filename")
                                .and_then(|v| v.as_str())
                                .unwrap_or("unknown")
                                .to_string();
                            
                            let size = op.payload.get("size")
                                .and_then(|v| v.as_u64())
                                .unwrap_or(0) as usize;
                            
                            state.media = Some(MediaInfo {
                                block_id,
                                media_type: MediaType::Photo, // Default to photo
                                filename,
                                mime_type: "image/jpeg".to_string(),
                                size,
                                added_by: op.author(),
                                added_at: op.timestamp(),
                            });
                        }
                    }
                }
                
                // Also handle the structured payload format
                if let Ok(payload) = serde_json::from_value::<SetMediaPayload>(op.payload.clone()) {
                    let mime_type = match payload.media_type {
                        MediaType::Photo => "image/jpeg".to_string(),
                        MediaType::Video => "video/mp4".to_string(),
                        MediaType::Audio => "audio/mp3".to_string(),
                    };
                    
                    state.media = Some(MediaInfo {
                        block_id: payload.block_id,
                        media_type: payload.media_type,
                        filename: payload.filename,
                        mime_type,
                        size: payload.size,
                        added_by: op.author(),
                        added_at: op.timestamp(),
                    });
                }
            }
            
            "add_comment" => {
                // Handle simple text payload from web interface
                if let Some(text) = op.payload.get("text").and_then(|v| v.as_str()) {
                    state.comments.push(Comment {
                        id: op.id(),
                        author: op.author(),
                        text: text.to_string(),
                        parent: None,
                        timestamp: op.timestamp(),
                        deleted: false,
                    });
                }
                
                // Also handle structured payload format
                if let Ok(payload) = serde_json::from_value::<CommentPayload>(op.payload.clone()) {
                    state.comments.push(Comment {
                        id: op.id(),
                        author: op.author(),
                        text: payload.text,
                        parent: payload.parent,
                        timestamp: op.timestamp(),
                        deleted: false,
                    });
                }
            }
            
            "delete" => {
                if let Ok(payload) = serde_json::from_value::<DeletePayload>(op.payload.clone()) {
                    state.deleted_items.insert(payload.target);
                    // Mark comment as deleted but keep it for replies
                    for comment in &mut state.comments {
                        if comment.id == payload.target {
                            comment.deleted = true;
                        }
                    }
                }
            }
            
            "add_reaction" => {
                if let Ok(payload) = serde_json::from_value::<ReactionPayload>(op.payload.clone()) {
                    state.reactions
                        .entry(payload.emoji)
                        .or_insert_with(Vec::new)
                        .push(op.author());
                }
            }
            
            "set_crop" => {
                if let Ok(payload) = serde_json::from_value::<CropPayload>(op.payload.clone()) {
                    state.crop = Some(CropData {
                        left: payload.left,
                        top: payload.top,
                        right: payload.right,
                        bottom: payload.bottom,
                    });
                }
            }
            
            "rotate" => {
                // Handle simple degrees payload from web interface
                if let Some(degrees) = op.payload.get("degrees").and_then(|v| v.as_f64()) {
                    state.rotation = degrees as f32; // LWW semantics
                }
                
                // Also handle structured payload format
                if let Ok(payload) = serde_json::from_value::<RotatePayload>(op.payload.clone()) {
                    state.rotation = payload.angle; // LWW semantics
                }
            }
            
            "add_markup" => {
                if let Ok(payload) = serde_json::from_value::<MarkupPayload>(op.payload.clone()) {
                    state.markup.push(MarkupElement {
                        id: op.id(),
                        markup_type: payload.markup_type,
                        data: payload.data,
                        author: op.author(),
                        timestamp: op.timestamp(),
                    });
                }
            }
            
            "share_with" => {
                if let Ok(payload) = serde_json::from_value::<SharePayload>(op.payload.clone()) {
                    state.shared_with.insert(payload.user_id, payload.permission);
                }
            }
            
            _ => {
                // Unknown operation type - ignore gracefully for forward compatibility
            }
        }
        
        Ok(())
    }
}

// === Album Collection ===

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MediaAlbum {
    pub album_id: String,
    pub items: HashMap<MediaId, LogCrdt>,
    pub metadata: AlbumMetadata,
    #[serde(skip)]
    pub block_manager: Option<Arc<BlockManager>>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct AlbumMetadata {
    pub title: String,
    pub created_by: ReplicaId,
    pub created_at: LogicalTime,
    pub shared_with: HashMap<UserId, Permission>,
}

impl Default for AlbumMetadata {
    fn default() -> Self {
        Self {
            title: "Untitled Album".to_string(),
            created_by: "unknown".to_string(),
            created_at: chrono::Utc::now().timestamp_millis() as u64,
            shared_with: HashMap::new(),
        }
    }
}

impl MediaAlbum {
    pub fn new(album_id: String, title: String, created_by: ReplicaId) -> Self {
        Self {
            album_id,
            items: HashMap::new(),
            metadata: AlbumMetadata {
                title,
                created_by,
                created_at: chrono::Utc::now().timestamp_millis() as u64,
                shared_with: HashMap::new(),
            },
            block_manager: None,
        }
    }
    
    pub fn with_block_manager(mut self, block_manager: Arc<BlockManager>) -> Self {
        self.block_manager = Some(block_manager);
        self
    }
    
    /// Serialize the album to bytes for storage in block system
    pub fn to_bytes(&self) -> Result<Vec<u8>, CrdtError> {
        Ok(serde_json::to_vec(self)?)
    }
    
    /// Deserialize album from bytes
    pub fn from_bytes(data: &[u8]) -> Result<Self, CrdtError> {
        Ok(serde_json::from_slice(data)?)
    }
}

impl CrdtCollection<MediaId, EditOp> for MediaAlbum {
    type ItemCrdt = LogCrdt;
    type Error = CrdtError;
    
    fn get_or_create(&mut self, key: &MediaId) -> &mut Self::ItemCrdt {
        self.items.entry(key.clone()).or_insert_with(LogCrdt::new)
    }
    
    fn get(&self, key: &MediaId) -> Option<&Self::ItemCrdt> {
        self.items.get(key)
    }
    
    fn apply_to_item(&mut self, key: &MediaId, op: EditOp) -> Result<(), Self::Error> {
        let crdt = self.get_or_create(key);
        crdt.apply_local(op)
    }
    
    fn merge_collection(&mut self, other: &Self) -> Result<(), Self::Error> {
        // Merge metadata (LWW based on created_at)
        if other.metadata.created_at > self.metadata.created_at {
            self.metadata = other.metadata.clone();
        }
        
        // Merge individual media items
        for (key, other_crdt) in &other.items {
            let our_crdt = self.get_or_create(key);
            our_crdt.merge(other_crdt)?;
        }
        Ok(())
    }
    
    fn keys(&self) -> Vec<MediaId> {
        self.items.keys().cloned().collect()
    }
    
    fn reduce_all(&self) -> HashMap<MediaId, MediaState> {
        self.items.iter()
            .map(|(k, v)| (k.clone(), v.reduce()))
            .collect()
    }
}


// =====================================================================
// FILE: packages/soradyne_core/src/flow/conflict.rs
// =====================================================================

pub trait ConflictResolver<T> {
    fn resolve(&self, local: &T, remote: &T) -> T;
}

pub struct LastWriteWins;

impl<T: Clone> ConflictResolver<T> for LastWriteWins {
    fn resolve(&self, _local: &T, remote: &T) -> T {
        remote.clone()
    }
}



// =====================================================================
// FILE: packages/soradyne_core/src/flow/error.rs
// =====================================================================

use thiserror::Error;

#[derive(Error, Debug)]
pub enum FlowError {
    #[error("Persistence error: {0}")]
    PersistenceError(String),

    #[error("Subscription error: {0}")]
    SubscriptionError(String),
    
    #[error("Storage backend error: {0}")]
    StorageBackendError(String),
    
    #[error("Configuration error: {0}")]
    ConfigurationError(String),
    
    #[error("Device identity error: {0}")]
    DeviceIdentityError(String),
}



// =====================================================================
// FILE: packages/soradyne_core/src/flow/persistence.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/flow/subscription.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/flow/mod.rs
// =====================================================================

//! flow/mod.rs
//!
//! The core SelfDataFlow abstraction, extended for a live heartrate sync demo.
//! This module defines the SelfDataFlow type, metadata, and full operations
//! for live distributed data flows with eventual consistency.

mod conflict;
pub mod error;
mod subscription;
mod persistence;
mod routing;
pub mod traits;
pub mod examples;

pub use conflict::{ConflictResolver, LastWriteWins};
pub use error::FlowError;
pub use traits::{StorageBackend, FlowAuthenticator, FlowType, Diffable};

use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use uuid::Uuid;
use tokio::sync::broadcast;
use serde::{Serialize, Deserialize};


/// Metadata for a SelfDataFlow
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct FlowMetadata {
    pub id: Uuid,
    pub name: String,
    pub owner_id: Uuid,
    pub version: u64,
}

/// The core SelfDataFlow type
pub struct SelfDataFlow<T: Send + Sync + Clone + 'static> {
    value: Arc<Mutex<T>>,
    metadata: Arc<Mutex<FlowMetadata>>,
    subscribers: Arc<Mutex<HashMap<Uuid, Box<dyn Fn(&T) + Send + Sync>>>>,
    update_tx: broadcast::Sender<T>,
    resolver: Arc<dyn ConflictResolver<T> + Send + Sync>,
    storage: Option<Arc<dyn StorageBackend + Send + Sync>>,
    authenticator: Option<Arc<dyn FlowAuthenticator<T> + Send + Sync>>,
    flow_type: FlowType,
}

impl<T> SelfDataFlow<T>
where
    T: Send + Sync + Clone + Serialize + for<'de> Deserialize<'de> + 'static,
{
    /// Create a new SelfDataFlow
    pub fn new(name: &str, owner_id: Uuid, initial_value: T, flow_type: FlowType) -> Self {
        let metadata = FlowMetadata {
            id: Uuid::new_v4(),
            name: name.to_string(),
            owner_id,
            version: 1,
        };
        let (update_tx, _) = broadcast::channel(32);

        Self {
            metadata: Arc::new(Mutex::new(metadata)),
            value: Arc::new(Mutex::new(initial_value)),
            subscribers: Arc::new(Mutex::new(HashMap::new())),
            update_tx,
            resolver: Arc::new(LastWriteWins),
            storage: None,
            authenticator: None,
            flow_type,
        }
    }

    /// Set the storage backend for this flow
    pub fn with_storage(mut self, storage: impl StorageBackend + Send + Sync + 'static) -> Self {
        self.storage = Some(Arc::new(storage));
        self
    }

    /// Set the authenticator for this flow
    pub fn with_authenticator(mut self, authenticator: impl FlowAuthenticator<T> + Send + Sync + 'static) -> Self {
        self.authenticator = Some(Arc::new(authenticator));
        self
    }
    
    /// Get the flow type
    pub fn flow_type(&self) -> FlowType {
        self.flow_type
    }

    /// Get the current value
    pub fn get_value(&self) -> Option<T> {
        self.value.lock().ok().map(|v| v.clone())
    }
    
    /// Create a new SelfDataFlow with default settings (for backward compatibility)
    pub fn new_default(name: &str, owner_id: Uuid, initial_value: T) -> Self {
        Self::new(name, owner_id, initial_value, FlowType::Custom)
    }

    /// Update the value and notify subscribers
    pub fn update(&self, new_value: T) {
        {
            let mut value = self.value.lock().unwrap();
            *value = new_value.clone();
        }
        if let Ok(mut metadata) = self.metadata.lock() {
            metadata.version += 1;
        }
        if let Ok(mut guard) = self.value.lock() {
            *guard = new_value.clone();
            let _ = self.update_tx.send(new_value.clone());
            if let Ok(subs) = self.subscribers.lock() {
                for callback in subs.values() {
                    callback(&new_value);
                }
            }
        }
    }
    
    /// Update the value using a diff and notify subscribers
    /// This is more efficient for incremental updates as it avoids sending the entire object
    pub fn update_with_diff<D>(&self, diff: &D) 
    where 
        T: Diffable<Diff = D>
    {
        if let Ok(mut metadata) = self.metadata.lock() {
            metadata.version += 1;
        }
        
        if let Ok(mut guard) = self.value.lock() {
            let new_value = guard.apply(diff);
            *guard = new_value.clone();
            let _ = self.update_tx.send(new_value.clone());
            if let Ok(subs) = self.subscribers.lock() {
                for callback in subs.values() {
                    callback(&new_value);
                }
            }
        }
    }
    
    /// Broadcast a diff to subscribers without updating the local value
    /// This is useful when you want to send incremental updates to peers
    pub fn broadcast_diff<D>(&self, diff: &D) 
    where 
        T: Diffable<Diff = D>
    {
        if let Ok(guard) = self.value.lock() {
            let new_value = guard.apply(diff);
            let _ = self.update_tx.send(new_value);
        }
    }

    /// Merge a remote value
    pub fn merge(&self, remote_value: T) {
        if let Ok(mut metadata) = self.metadata.lock() {
            metadata.version += 1;
        }
        if let Ok(mut guard) = self.value.lock() {
            let resolved = self.resolver.resolve(&*guard, &remote_value);
            *guard = resolved.clone();
            let _ = self.update_tx.send(resolved.clone());
            if let Ok(subs) = self.subscribers.lock() {
                for callback in subs.values() {
                    callback(&resolved);
                }
            }
        }
    }
    
    /// Merge a remote diff
    pub fn merge_diff<D>(&self, diff: &D) 
    where 
        T: Diffable<Diff = D>
    {
        if let Ok(mut metadata) = self.metadata.lock() {
            metadata.version += 1;
        }
        
        if let Ok(mut guard) = self.value.lock() {
            let remote_value = guard.apply(diff);
            let resolved = self.resolver.resolve(&*guard, &remote_value);
            *guard = resolved.clone();
            let _ = self.update_tx.send(resolved.clone());
            if let Ok(subs) = self.subscribers.lock() {
                for callback in subs.values() {
                    callback(&resolved);
                }
            }
        }
    }

    /// Subscribe to updates
    pub fn subscribe(&self, callback: Box<dyn Fn(&T) + Send + Sync>) -> Uuid {
        let id = Uuid::new_v4();
        if let Ok(mut subs) = self.subscribers.lock() {
            subs.insert(id, callback);
        }
        id
    }

    /// Unsubscribe
    pub fn unsubscribe(&self, id: Uuid) {
        if let Ok(mut subs) = self.subscribers.lock() {
            subs.remove(&id);
        }
    }
    
    /// Sign the current flow data
    pub fn sign(&self) -> Result<Vec<u8>, FlowError> {
        if let Some(authenticator) = &self.authenticator {
            if let Ok(guard) = self.value.lock() {
                return authenticator.sign(&*guard);
            }
            Err(FlowError::PersistenceError("Failed to access flow data".to_string()))
        } else {
            Err(FlowError::PersistenceError("No authenticator configured".to_string()))
        }
    }
    
    /// Verify a signature for the current flow data
    pub fn verify(&self, signature: &[u8]) -> bool {
        if let Some(authenticator) = &self.authenticator {
            if let Ok(guard) = self.value.lock() {
                return authenticator.verify(&*guard, signature);
            }
        }
        false
    }

    /// Persist the flow data using the configured storage backend
    pub fn persist(&self) -> Result<(), FlowError> {
        if let Some(storage) = &self.storage {
            if let Ok(metadata) = self.metadata.lock() {
                if let Ok(guard) = self.value.lock() {
                    if let Ok(json) = serde_json::to_string(&*guard) {
                        return storage.store(metadata.id, json.as_bytes());
                    }
                }
            }
            Err(FlowError::PersistenceError("Failed to serialize flow data".to_string()))
        } else {
            Err(FlowError::PersistenceError("No storage backend configured".to_string()))
        }
    }

    /// Load flow data from the configured storage backend
    pub fn load(&mut self) -> Result<(), FlowError> {
        if let Some(storage) = &self.storage {
            if let Ok(metadata) = self.metadata.lock() {
                let data = storage.load(metadata.id)?;
                if let Ok(value) = serde_json::from_slice::<T>(&data) {
                    if let Ok(mut guard) = self.value.lock() {
                        *guard = value;
                        return Ok(());
                    }
                }
                return Err(FlowError::PersistenceError("Failed to deserialize flow data".to_string()));
            }
            Err(FlowError::PersistenceError("Failed to access flow metadata".to_string()))
        } else {
            Err(FlowError::PersistenceError("No storage backend configured".to_string()))
        }
    }
    
    /// Check if flow data exists in the storage backend
    pub fn exists(&self) -> bool {
        if let Some(storage) = &self.storage {
            if let Ok(metadata) = self.metadata.lock() {
                return storage.exists(metadata.id);
            }
        }
        false
    }
    
    /// Delete flow data from the storage backend
    pub fn delete(&self) -> Result<(), FlowError> {
        if let Some(storage) = &self.storage {
            if let Ok(metadata) = self.metadata.lock() {
                return storage.delete(metadata.id);
            }
            Err(FlowError::PersistenceError("Failed to access flow metadata".to_string()))
        } else {
            Err(FlowError::PersistenceError("No storage backend configured".to_string()))
        }
    }
}



// =====================================================================
// FILE: packages/soradyne_core/src/flow/routing.rs
// =====================================================================



// =====================================================================
// FILE: packages/soradyne_core/src/flow/traits.rs
// =====================================================================

use uuid::Uuid;
use crate::flow::FlowError;

/// Trait for types that can be diffed and patched
pub trait Diffable {
    /// The type representing a diff/delta between two instances
    type Diff;
    
    /// Create a diff between self and another instance
    fn diff(&self, other: &Self) -> Self::Diff;
    
    /// Apply a diff to self, returning a new instance
    fn apply(&self, diff: &Self::Diff) -> Self;
}

/// Trait for storage backends that can persist flow data
pub trait StorageBackend {
    /// Store data for a specific flow
    fn store(&self, flow_id: Uuid, data: &[u8]) -> Result<(), FlowError>;
    
    /// Load data for a specific flow
    fn load(&self, flow_id: Uuid) -> Result<Vec<u8>, FlowError>;
    
    /// Check if data exists for a specific flow
    fn exists(&self, flow_id: Uuid) -> bool;
    
    /// Delete data for a specific flow
    fn delete(&self, flow_id: Uuid) -> Result<(), FlowError>;
}

/// Trait for flow authenticators that can sign and verify flow data
pub trait FlowAuthenticator<T> {
    /// Sign the provided data
    fn sign(&self, data: &T) -> Result<Vec<u8>, FlowError>;
    
    /// Verify the signature for the provided data
    fn verify(&self, data: &T, signature: &[u8]) -> bool;
}

/// Enum representing different types of flows
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum FlowType {
    /// Real-time scalar data (e.g., heart rate)
    RealTimeScalar,
    
    /// File catalog (e.g., photo album)
    FileCatalog,
    
    /// Chat conversation
    Chat,
    
    /// Robot state
    RobotState,
    
    /// Custom flow type
    Custom,
}


// =====================================================================
// FILE: packages/soradyne_core/src/flow/examples/mod.rs
// =====================================================================

//! Examples demonstrating SelfDataFlow usage

pub mod robot_joints;
pub use robot_joints::run_robot_joint_demo;


// =====================================================================
// FILE: packages/soradyne_core/src/flow/examples/robot_joints.rs
// =====================================================================

//! Robot Joints Example
//!
//! This example demonstrates using SelfDataFlow with the Diffable trait
//! for efficient updates to robot joint positions.

use std::collections::HashMap;
use uuid::Uuid;
use serde::{Serialize, Deserialize};
use crate::flow::{SelfDataFlow, FlowType, Diffable};

/// Represents a single robot joint position
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct JointPosition {
    /// Joint name
    pub name: String,
    /// Current angle in radians
    pub angle: f64,
    /// Current velocity in radians per second
    pub velocity: f64,
    /// Last update timestamp
    pub timestamp: u64,
}

/// Represents the state of all joints in a robot
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct RobotJointState {
    /// Robot identifier
    pub robot_id: String,
    /// Map of joint name to joint position
    pub joints: HashMap<String, JointPosition>,
    /// Last update timestamp
    pub last_update: u64,
}

/// Represents a change to a single joint
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct JointDiff {
    /// Joint name
    pub name: String,
    /// New angle (if changed)
    pub angle: Option<f64>,
    /// New velocity (if changed)
    pub velocity: Option<f64>,
    /// Update timestamp
    pub timestamp: u64,
}

/// Represents changes to multiple joints
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct RobotJointDiff {
    /// List of joint changes
    pub changes: Vec<JointDiff>,
    /// Update timestamp
    pub timestamp: u64,
}

impl Diffable for RobotJointState {
    type Diff = RobotJointDiff;
    
    fn diff(&self, other: &Self) -> Self::Diff {
        let mut changes = Vec::new();
        
        // Check for changes in existing joints
        for (name, joint) in &other.joints {
            if let Some(self_joint) = self.joints.get(name) {
                let mut diff_needed = false;
                let mut joint_diff = JointDiff {
                    name: name.clone(),
                    angle: None,
                    velocity: None,
                    timestamp: joint.timestamp,
                };
                
                // Check if angle changed
                if (joint.angle - self_joint.angle).abs() > 0.001 {
                    joint_diff.angle = Some(joint.angle);
                    diff_needed = true;
                }
                
                // Check if velocity changed
                if (joint.velocity - self_joint.velocity).abs() > 0.001 {
                    joint_diff.velocity = Some(joint.velocity);
                    diff_needed = true;
                }
                
                if diff_needed {
                    changes.push(joint_diff);
                }
            } else {
                // New joint
                changes.push(JointDiff {
                    name: name.clone(),
                    angle: Some(joint.angle),
                    velocity: Some(joint.velocity),
                    timestamp: joint.timestamp,
                });
            }
        }
        
        RobotJointDiff {
            changes,
            timestamp: other.last_update,
        }
    }
    
    fn apply(&self, diff: &Self::Diff) -> Self {
        let mut new_state = self.clone();
        new_state.last_update = diff.timestamp;
        
        for change in &diff.changes {
            let joint = new_state.joints.entry(change.name.clone())
                .or_insert_with(|| JointPosition {
                    name: change.name.clone(),
                    angle: 0.0,
                    velocity: 0.0,
                    timestamp: 0,
                });
            
            if let Some(angle) = change.angle {
                joint.angle = angle;
            }
            
            if let Some(velocity) = change.velocity {
                joint.velocity = velocity;
            }
            
            joint.timestamp = change.timestamp;
        }
        
        new_state
    }
}

/// Create a demo robot joint flow
pub fn create_robot_joint_flow(robot_id: &str, owner_id: Uuid) -> SelfDataFlow<RobotJointState> {
    // Create initial state with some joints
    let mut initial_joints = HashMap::new();
    
    // Add some initial joints
    initial_joints.insert("shoulder".to_string(), JointPosition {
        name: "shoulder".to_string(),
        angle: 0.0,
        velocity: 0.0,
        timestamp: 0,
    });
    
    initial_joints.insert("elbow".to_string(), JointPosition {
        name: "elbow".to_string(),
        angle: 0.0,
        velocity: 0.0,
        timestamp: 0,
    });
    
    initial_joints.insert("wrist".to_string(), JointPosition {
        name: "wrist".to_string(),
        angle: 0.0,
        velocity: 0.0,
        timestamp: 0,
    });
    
    let initial_state = RobotJointState {
        robot_id: robot_id.to_string(),
        joints: initial_joints,
        last_update: 0,
    };
    
    // Create the flow
    SelfDataFlow::new(
        &format!("Robot Joints - {}", robot_id),
        owner_id,
        initial_state,
        FlowType::RobotState,
    )
}

/// Demo function showing how to use the robot joint flow with diffs
pub fn run_robot_joint_demo() {
    let owner_id = Uuid::new_v4();
    let flow = create_robot_joint_flow("robot-1", owner_id);
    
    // Subscribe to updates
    let _subscription_id = flow.subscribe(Box::new(|state| {
        println!("Robot state updated:");
        println!("  Robot ID: {}", state.robot_id);
        println!("  Last update: {}", state.last_update);
        println!("  Joints:");
        for (name, joint) in &state.joints {
            println!("    {}: angle={:.2}, velocity={:.2}", 
                     name, joint.angle, joint.velocity);
        }
        println!();
    }));
    
    // Update using full state
    if let Some(current_state) = flow.get_value() {
        let mut new_state = current_state.clone();
        
        // Update shoulder joint
        if let Some(joint) = new_state.joints.get_mut("shoulder") {
            joint.angle = 0.5;
            joint.velocity = 0.1;
            joint.timestamp = 1;
        }
        
        new_state.last_update = 1;
        flow.update(new_state);
    }
    
    // Update using diff (more efficient)
    let diff = RobotJointDiff {
        changes: vec![
            JointDiff {
                name: "elbow".to_string(),
                angle: Some(0.75),
                velocity: Some(0.2),
                timestamp: 2,
            },
            JointDiff {
                name: "wrist".to_string(),
                angle: Some(0.3),
                velocity: None, // Don't change velocity
                timestamp: 2,
            },
        ],
        timestamp: 2,
    };
    
    flow.update_with_diff(&diff);
    
    // Create a new joint with diff
    let new_joint_diff = RobotJointDiff {
        changes: vec![
            JointDiff {
                name: "gripper".to_string(),
                angle: Some(0.0),
                velocity: Some(0.0),
                timestamp: 3,
            },
        ],
        timestamp: 3,
    };
    
    flow.update_with_diff(&new_joint_diff);
    
    // Update multiple joints with a single diff
    let multi_diff = RobotJointDiff {
        changes: vec![
            JointDiff {
                name: "shoulder".to_string(),
                angle: Some(0.6),
                velocity: None,
                timestamp: 4,
            },
            JointDiff {
                name: "elbow".to_string(),
                angle: Some(0.8),
                velocity: Some(0.0),
                timestamp: 4,
            },
        ],
        timestamp: 4,
    };
    
    flow.update_with_diff(&multi_diff);
    
    println!("Robot joint demo completed!");
}


// =====================================================================
// FILE: packages/soradyne_flutter/lib/soradyne_flutter.dart
// =====================================================================

library soradyne_flutter;

export 'src/soradyne_client.dart';
export 'src/album_service.dart';
export 'src/crdt/crdt_document.dart';
export 'src/messaging/realtime_messaging.dart';
export 'src/models/models.dart';




// =====================================================================
// FILE: packages/soradyne_flutter/lib/src/soradyne_client.dart
// =====================================================================

import 'dart:ffi';
import 'package:ffi/ffi.dart';

class SoradyneClient {
  static SoradyneClient? _instance;
  late final DynamicLibrary _lib;
  bool _initialized = false;

  SoradyneClient._();

  static SoradyneClient get instance {
    _instance ??= SoradyneClient._();
    return _instance!;
  }

  Future<void> initialize({String? dataDirectory}) async {
    if (_initialized) return;
    
    _lib = _loadLibrary();
    final result = _lib.lookupFunction<Int32 Function(), int Function()>('soradyne_init')();
    
    if (result != 0) {
      throw Exception('Failed to initialize Soradyne core');
    }
    
    _initialized = true;
  }

  AlbumService get albums => AlbumService._(this);
  RealtimeMessaging get messaging => RealtimeMessaging._(this);

  DynamicLibrary _loadLibrary() {
    if (Platform.isMacOS) return DynamicLibrary.open('libsoradyne.dylib');
    if (Platform.isLinux) return DynamicLibrary.open('libsoradyne.so');
    if (Platform.isWindows) return DynamicLibrary.open('soradyne.dll');
    if (Platform.isAndroid) return DynamicLibrary.open('libsoradyne.so');
    if (Platform.isIOS) return DynamicLibrary.process();
    throw UnsupportedError('Platform not supported');
  }

  void dispose() {
    if (_initialized) {
      _lib.lookupFunction<Void Function(), void Function()>('soradyne_cleanup')();
      _initialized = false;
    }
  }
}


// =====================================================================
// FILE: packages/soradyne_flutter/lib/src/crdt/crdt_document.dart
// =====================================================================

abstract class CrdtDocument<T> {
  String get id;
  T get state;
  Stream<T> get stateStream;
  
  Future<void> applyOperation(Map<String, dynamic> operation);
  Future<void> merge(CrdtDocument<T> other);
  Future<void> sync();
}

class CrdtAlbum extends CrdtDocument<AlbumState> {
  // Implementation that wraps your existing album CRDT
}



// =====================================================================
// FILE: packages/giantt_core/test/compatibility/python_dart_comparison_test.dart
// =====================================================================

import 'dart:io';
import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

/// Tests that verify Dart implementation produces identical outputs to Python
void main() {
  group('Python-Dart Compatibility Tests', () {
    late Directory tempDir;
    late String itemsPath;
    late String occludeItemsPath;

    setUp(() async {
      tempDir = await Directory.systemTemp.createTemp('giantt_test_');
      final includeDir = Directory('${tempDir.path}/include');
      final occludeDir = Directory('${tempDir.path}/occlude');
      await includeDir.create(recursive: true);
      await occludeDir.create(recursive: true);
      
      itemsPath = '${includeDir.path}/items.txt';
      occludeItemsPath = '${occludeDir.path}/items.txt';
      
      // Create initial empty files with headers
      await File(itemsPath).writeAsString(_getItemsHeader());
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());
    });

    tearDown(() async {
      await tempDir.delete(recursive: true);
    });

    test('Item parsing matches Python exactly', () {
      print('\n=== Item Parsing Compatibility Test ===');
      
      // Test cases from Python implementation
      final testCases = [
        // Basic item
        '○ simple_task 1d "Simple Task" {}',
        
        // Item with priority
        '○ priority_task! 2d "Priority Task" {}',
        
        // Item with high priority
        '○ high_task!! 3d "High Priority Task" {}',
        
        // Item with charts
        '○ chart_task 1d "Chart Task" {"Chart1","Chart2"}',
        
        // Item with tags
        '○ tag_task 1d "Tag Task" {} tag1,tag2,tag3',
        
        // Item with relations
        '○ rel_task 1d "Related Task" {} >>> ⊢[dep1,dep2]',
        
        // Complex item with everything
        '◑ complex!! 5d "Complex Task" {"Chart1"} tag1,tag2 >>> ⊢[dep1] ►[blocked1]',
        
        // Item with JSON-escaped title
        '○ json_task 1d "Task with \\"quotes\\" and \\n newlines" {}',
        
        // Item with time constraint
        '○ time_task 1d "Time Task" {} @@@ window(5d:2d,severe)',
        
        // Item with comments
        '○ comment_task 1d "Comment Task" {} # User comment ### Auto comment',
      ];

      print('Testing ${testCases.length} Python format strings:');
      
      for (int i = 0; i < testCases.length; i++) {
        final testCase = testCases[i];
        print('\nTest ${i + 1}:');
        print('  Python: $testCase');
        
        try {
          // Parse with Dart
          final dartItem = GianttParser.fromString(testCase);
          
          // Convert back to string
          final dartOutput = dartItem.toFileString();
          print('  Dart:   $dartOutput');
          
          // Check if they match
          final normalizedPython = _normalizeItemString(testCase);
          final normalizedDart = _normalizeItemString(dartOutput);
          final match = normalizedDart == normalizedPython;
          
          print('  Result: ${match ? "✓ MATCH" : "✗ DIFFER"}');
          
          if (!match) {
            print('  Normalized Python: "$normalizedPython"');
            print('  Normalized Dart:   "$normalizedDart"');
          }
          
          // Should match original (allowing for minor formatting differences)
          expect(normalizedDart, equals(normalizedPython),
                 reason: 'Failed for test case: $testCase');
        } catch (e) {
          print('  Result: ✗ PARSE ERROR - $e');
          rethrow;
        }
      }
      
      print('\n✓ All ${testCases.length} test cases passed!');
    });

    test('Status symbol parsing matches Python', () {
      final statusTests = {
        '○': GianttStatus.notStarted,
        '◑': GianttStatus.inProgress,
        '⊘': GianttStatus.blocked,
        '●': GianttStatus.completed,
      };

      for (final entry in statusTests.entries) {
        final symbol = entry.key;
        final expectedStatus = entry.value;
        
        final testItem = '$symbol test_id 1d "Test" {}';
        final parsed = GianttParser.fromString(testItem);
        
        expect(parsed.status, equals(expectedStatus),
               reason: 'Status symbol $symbol should parse to ${expectedStatus.name}');
        expect(parsed.status.symbol, equals(symbol),
               reason: 'Status ${expectedStatus.name} should have symbol $symbol');
      }
    });

    test('Priority symbol parsing matches Python', () {
      final priorityTests = {
        ',,,': GianttPriority.lowest,
        '...': GianttPriority.low,
        '': GianttPriority.neutral,
        '?': GianttPriority.unsure,
        '!': GianttPriority.medium,
        '!!': GianttPriority.high,
        '!!!': GianttPriority.critical,
      };

      for (final entry in priorityTests.entries) {
        final symbol = entry.key;
        final expectedPriority = entry.value;
        
        final testItem = '○ test_id$symbol 1d "Test" {}';
        final parsed = GianttParser.fromString(testItem);
        
        expect(parsed.priority, equals(expectedPriority),
               reason: 'Priority symbol "$symbol" should parse to ${expectedPriority.name}');
        expect(parsed.priority.symbol, equals(symbol),
               reason: 'Priority ${expectedPriority.name} should have symbol "$symbol"');
      }
    });

    test('Relation symbol parsing matches Python', () {
      final relationTests = {
        '⊢': 'REQUIRES',
        '⋲': 'ANYOF', 
        '≫': 'SUPERCHARGES',
        '∴': 'INDICATES',
        '∪': 'TOGETHER',
        '⊟': 'CONFLICTS',
        '►': 'BLOCKS',
        '≻': 'SUFFICIENT',
      };

      for (final entry in relationTests.entries) {
        final symbol = entry.key;
        final expectedType = entry.value;
        
        final testItem = '○ test_id 1d "Test" {} >>> $symbol[target1,target2]';
        final parsed = GianttParser.fromString(testItem);
        
        expect(parsed.relations.containsKey(expectedType), isTrue,
               reason: 'Should parse relation type $expectedType from symbol $symbol');
        expect(parsed.relations[expectedType], equals(['target1', 'target2']),
               reason: 'Should parse relation targets correctly');
      }
    });

    test('Duration parsing matches Python', () {
      final durationTests = [
        '1d',
        '2w', 
        '3mo',
        '1.5d',
        '2.5w',
        '0.5mo',
        '1y',
        '6mo2w3d',
        '1y6mo',
        '2w3d4h',
      ];

      for (final durationStr in durationTests) {
        final testItem = '○ test_id $durationStr "Test" {}';
        final parsed = GianttParser.fromString(testItem);
        
        // Should parse without error
        expect(parsed.duration.toString(), equals(durationStr),
               reason: 'Duration $durationStr should round-trip correctly');
      }
    });

    test('Graph operations match Python behavior', () async {
      // Create test items that form a dependency chain
      final items = [
        '○ task_a 1d "Task A" {}',
        '○ task_b 1d "Task B" {} >>> ⊢[task_a]',
        '○ task_c 1d "Task C" {} >>> ⊢[task_b]',
      ];

      // Write items to file
      final content = _getItemsHeader() + '\n' + items.join('\n') + '\n';
      await File(itemsPath).writeAsString(content);
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());

      // Load graph
      final graph = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      
      // Should have 3 items
      expect(graph.items.length, equals(3));
      
      // Should be able to topologically sort
      final sorted = graph.topologicalSort();
      expect(sorted.length, equals(3));
      
      // Order should be: task_a, task_b, task_c
      expect(sorted[0].id, equals('task_a'));
      expect(sorted[1].id, equals('task_b'));
      expect(sorted[2].id, equals('task_c'));
      
      // Save and reload should preserve order
      FileRepository.saveGraph(itemsPath, occludeItemsPath, graph);
      final reloaded = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      final resorted = reloaded.topologicalSort();
      
      expect(resorted.map((i) => i.id).toList(), 
             equals(['task_a', 'task_b', 'task_c']));
    });

    test('Cycle detection matches Python behavior', () async {
      // Create items with circular dependency
      final items = [
        '○ task_a 1d "Task A" {} >>> ⊢[task_b]',
        '○ task_b 1d "Task B" {} >>> ⊢[task_a]',
      ];

      final content = _getItemsHeader() + '\n' + items.join('\n') + '\n';
      await File(itemsPath).writeAsString(content);
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());

      final graph = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      
      // Should detect cycle
      expect(() => graph.topologicalSort(), 
             throwsA(isA<CycleDetectedException>()));
    });

    test('Doctor functionality matches Python behavior', () async {
      // Create items with dangling reference
      final items = [
        '○ task_a 1d "Task A" {} >>> ⊢[nonexistent]',
        '○ task_b 1d "Task B" {}',
      ];

      final content = _getItemsHeader() + '\n' + items.join('\n') + '\n';
      await File(itemsPath).writeAsString(content);
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());

      final graph = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      final doctor = GraphDoctor(graph);
      
      // Should find dangling reference issue
      final issues = doctor.fullDiagnosis();
      expect(issues.length, equals(1));
      expect(issues.first.type, equals(IssueType.danglingReference));
      expect(issues.first.itemId, equals('task_a'));
      
      // Should be able to fix it
      final fixed = doctor.fixIssues();
      expect(fixed.length, equals(1));
      
      // After fixing, should have no issues
      final afterFix = doctor.fullDiagnosis();
      expect(afterFix.length, equals(0));
    });
  });
}

String _getItemsHeader() {
  return '''
##############################################
#                                            #
#                Giantt Items                #
#                                            #
#   This file contains all include Giantt   #
#   items in topological order according    #
#   to the REQUIRES (⊢) relation.           #
#   You can use #include directives at the  #
#   top of this file to include other       #
#   Giantt item files.                      #
#   Edit this file manually at your own     #
#   risk.                                    #
#                                            #
##############################################
''';
}

String _getOccludeHeader() {
  return '''
##############################################
#                                            #
#            Giantt Occluded Items           #
#                                            #
#   This file contains all occluded Giantt  #
#   items in topological order according    #
#   to the REQUIRES (⊢) relation.           #
#   Edit this file manually at your own     #
#   risk.                                    #
#                                            #
##############################################
''';
}

/// Normalize item strings for comparison by removing extra whitespace
String _normalizeItemString(String item) {
  return item.trim().replaceAll(RegExp(r'\s+'), ' ');
}


// =====================================================================
// FILE: packages/giantt_core/test/compatibility/python_dart_execution_test.dart
// =====================================================================

import 'dart:io';
import 'dart:convert';
import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';
import 'package:giantt_core/src/storage/file_header_generator.dart';

/// Tests that execute actual Python CLI and compare with Dart implementation
void main() {
  group('Python-Dart Execution Comparison', () {
    late Directory tempDir;
    late Directory pythonTempDir;
    late String dartItemsPath;
    late String dartOccludeItemsPath;
    late String pythonItemsPath;
    late String pythonOccludeItemsPath;

    setUp(() async {
      // Create temp directories for both implementations
      tempDir = await Directory.systemTemp.createTemp('giantt_dart_test_');
      pythonTempDir = await Directory.systemTemp.createTemp('giantt_python_test_');
      
      // Set up Dart paths
      final dartIncludeDir = Directory('${tempDir.path}/include');
      final dartOccludeDir = Directory('${tempDir.path}/occlude');
      await dartIncludeDir.create(recursive: true);
      await dartOccludeDir.create(recursive: true);
      
      dartItemsPath = '${dartIncludeDir.path}/items.txt';
      dartOccludeItemsPath = '${dartOccludeDir.path}/items.txt';
      
      // Set up Python paths
      final pythonIncludeDir = Directory('${pythonTempDir.path}/include');
      final pythonOccludeDir = Directory('${pythonTempDir.path}/occlude');
      await pythonIncludeDir.create(recursive: true);
      await pythonOccludeDir.create(recursive: true);
      
      pythonItemsPath = '${pythonIncludeDir.path}/items.txt';
      pythonOccludeItemsPath = '${pythonOccludeDir.path}/items.txt';
      
      // Initialize both with empty files
      await File(dartItemsPath).writeAsString(_getItemsHeader());
      await File(dartOccludeItemsPath).writeAsString(_getOccludeHeader());
      await File(pythonItemsPath).writeAsString(_getItemsHeader());
      await File(pythonOccludeItemsPath).writeAsString(_getOccludeHeader());
    });

    tearDown(() async {
      await tempDir.delete(recursive: true);
      await pythonTempDir.delete(recursive: true);
    });

    test('Python and Dart init commands produce identical directory structure', () async {
      // Clean up existing directories
      await tempDir.delete(recursive: true);
      await pythonTempDir.delete(recursive: true);
      
      // Run Python init
      final pythonResult = await Process.run(
        'python3', 
        ['../../docs/port_reference/giantt_cli.py', 'init', '--data-dir', pythonTempDir.path],
        workingDirectory: '.',
      );
      
      // Run Dart init
      final dartResult = await Process.run(
        'dart', 
        ['run', 'bin/giantt.dart', 'init', '--data-dir', tempDir.path],
        workingDirectory: '.',
      );
      
      // Compare directory structures
      final pythonFiles = await _getDirectoryStructure(pythonTempDir.path);
      final dartFiles = await _getDirectoryStructure(tempDir.path);
      
      expect(dartFiles.length, equals(pythonFiles.length), 
             reason: 'Should create same number of files');
      
      // Check that both created the same relative file structure
      final pythonRelative = pythonFiles.map((f) => f.replaceFirst(pythonTempDir.path, '')).toSet();
      final dartRelative = dartFiles.map((f) => f.replaceFirst(tempDir.path, '')).toSet();
      
      expect(dartRelative, equals(pythonRelative),
             reason: 'Should create identical directory structure');
    });

    test('Python and Dart add commands produce identical file content', () async {
      // Test cases to add
      final testCases = [
        {
          'id': 'simple_task',
          'title': 'Simple Task',
          'args': [],
        },
        {
          'id': 'priority_task',
          'title': 'Priority Task',
          'args': ['--priority', 'HIGH'],
        },
        {
          'id': 'complex_task',
          'title': 'Complex Task with "quotes"',
          'args': ['--duration', '2.5d', '--priority', 'MEDIUM', '--charts', 'Chart1,Chart2', '--tags', 'urgent,test'],
        },
      ];
      
      for (final testCase in testCases) {
        // Run Python add
        final pythonArgs = [
          '../../docs/port_reference/giantt_cli.py', 'add',
          '--file', pythonItemsPath,
          '--occlude-file', pythonOccludeItemsPath,
          testCase['id'] as String,
          testCase['title'] as String,
          ...(testCase['args'] as List<dynamic>).cast<String>(),
        ];
        
        final pythonResult = await Process.run('python3', pythonArgs, workingDirectory: '.');
        
        // Run Dart add
        final dartArgs = [
          'run', 'bin/giantt.dart', 'add',
          '--file', dartItemsPath,
          '--occlude-file', dartOccludeItemsPath,
          testCase['id'] as String,
          testCase['title'] as String,
          ...(testCase['args'] as List<dynamic>).cast<String>(),
        ];
        
        final dartResult = await Process.run('dart', dartArgs, workingDirectory: '.');
        
        expect(dartResult.exitCode, equals(pythonResult.exitCode),
               reason: 'Exit codes should match for ${testCase['id']}');
      }
      
      // Compare final file contents
      final pythonContent = await File(pythonItemsPath).readAsString();
      final dartContent = await File(dartItemsPath).readAsString();
      
      // Extract just the item lines (skip headers)
      final pythonItems = _extractItemLines(pythonContent);
      final dartItems = _extractItemLines(dartContent);
      
      expect(dartItems.length, equals(pythonItems.length),
             reason: 'Should have same number of items');
      
      for (int i = 0; i < pythonItems.length; i++) {
        expect(dartItems[i], equals(pythonItems[i]),
               reason: 'Item $i should match exactly');
      }
    });

    test('Python and Dart show commands produce identical output', () async {
      // First add an item to both
      await _addTestItem('test_item', 'Test Item', pythonItemsPath, pythonOccludeItemsPath, dartItemsPath, dartOccludeItemsPath);
      
      // Run Python show
      final pythonResult = await Process.run(
        'python3', 
        ['../../docs/port_reference/giantt_cli.py', 'show', '--file', pythonItemsPath, '--occlude-file', pythonOccludeItemsPath, 'test_item'],
        workingDirectory: '.',
      );
      
      // Run Dart show
      final dartResult = await Process.run(
        'dart', 
        ['run', 'bin/giantt.dart', 'show', '--file', dartItemsPath, '--occlude-file', dartOccludeItemsPath, 'test_item'],
        workingDirectory: '.',
      );
      
      expect(dartResult.exitCode, equals(pythonResult.exitCode),
             reason: 'Exit codes should match');
      
      // Normalize whitespace for comparison
      final pythonOutput = _normalizeOutput(pythonResult.stdout);
      final dartOutput = _normalizeOutput(dartResult.stdout);
      
      expect(dartOutput, equals(pythonOutput),
             reason: 'Show output should be identical');
    });

    test('Python and Dart sort commands produce identical ordering', () async {
      // Add items with dependencies in both systems
      await _addTestItem('task_a', 'Task A', pythonItemsPath, pythonOccludeItemsPath, dartItemsPath, dartOccludeItemsPath);
      await _addTestItem('task_b', 'Task B', pythonItemsPath, pythonOccludeItemsPath, dartItemsPath, dartOccludeItemsPath, requires: 'task_a');
      await _addTestItem('task_c', 'Task C', pythonItemsPath, pythonOccludeItemsPath, dartItemsPath, dartOccludeItemsPath, requires: 'task_b');
      
      // Run Python sort
      final pythonResult = await Process.run(
        'python3', 
        ['../../docs/port_reference/giantt_cli.py', 'sort', '--file', pythonItemsPath, '--occlude-file', pythonOccludeItemsPath],
        workingDirectory: '.',
      );
      
      // Run Dart sort
      final dartResult = await Process.run(
        'dart', 
        ['run', 'bin/giantt.dart', 'sort', '--file', dartItemsPath, '--occlude-file', dartOccludeItemsPath],
        workingDirectory: '.',
      );
      
      expect(dartResult.exitCode, equals(pythonResult.exitCode),
             reason: 'Sort exit codes should match');
      
      // Compare final file ordering
      final pythonContent = await File(pythonItemsPath).readAsString();
      final dartContent = await File(dartItemsPath).readAsString();
      
      final pythonItems = _extractItemLines(pythonContent);
      final dartItems = _extractItemLines(dartContent);
      
      expect(dartItems, equals(pythonItems),
             reason: 'Sorted order should be identical');
    });

    test('Python and Dart doctor commands produce identical issue detection', () async {
      // Add items with issues to both systems
      await _addTestItem('broken_task', 'Broken Task', pythonItemsPath, pythonOccludeItemsPath, dartItemsPath, dartOccludeItemsPath, requires: 'nonexistent');
      
      // Run Python doctor
      final pythonResult = await Process.run(
        'python3', 
        ['../../docs/port_reference/giantt_cli.py', 'doctor', '--file', pythonItemsPath, '--occlude-file', pythonOccludeItemsPath],
        workingDirectory: '.',
      );
      
      // Run Dart doctor
      final dartResult = await Process.run(
        'dart', 
        ['run', 'bin/giantt.dart', 'doctor', '--file', dartItemsPath, '--occlude-file', dartOccludeItemsPath],
        workingDirectory: '.',
      );
      
      expect(dartResult.exitCode, equals(pythonResult.exitCode),
             reason: 'Doctor exit codes should match');
      
      // Both should detect issues or show error messages
      if (pythonResult.exitCode == 2) {
        // Python shows usage error in stderr, Dart shows in stderr too
        expect(pythonResult.stderr, contains('Error'),
               reason: 'Python should show error message');
        expect(dartResult.stderr, contains('Error'),
               reason: 'Dart should show error message');
      }
    });
  });
}

/// Helper to add a test item to both Python and Dart systems
Future<void> _addTestItem(String id, String title, String pythonItemsPath, String pythonOccludeItemsPath, 
                         String dartItemsPath, String dartOccludeItemsPath, {String? requires}) async {
  final extraArgs = requires != null ? ['--requires', requires] : <String>[];
  
  // Add to Python
  await Process.run(
    'python3', 
    ['../../docs/port_reference/giantt_cli.py', 'add', '--file', pythonItemsPath, '--occlude-file', pythonOccludeItemsPath, id, title, ...extraArgs],
    workingDirectory: '.',
  );
  
  // Add to Dart
  await Process.run(
    'dart', 
    ['run', 'bin/giantt.dart', 'add', '--file', dartItemsPath, '--occlude-file', dartOccludeItemsPath, id, title, ...extraArgs],
    workingDirectory: '.',
  );
}

/// Get all files in a directory recursively
Future<List<String>> _getDirectoryStructure(String dirPath) async {
  final files = <String>[];
  final dir = Directory(dirPath);
  
  if (!await dir.exists()) return files;
  
  await for (final entity in dir.list(recursive: true)) {
    if (entity is File) {
      files.add(entity.path);
    }
  }
  
  files.sort();
  return files;
}

/// Extract item lines from file content (skip headers and empty lines)
List<String> _extractItemLines(String content) {
  return content
      .split('\n')
      .where((line) => line.trim().isNotEmpty && !line.trim().startsWith('#'))
      .toList();
}

/// Normalize output for comparison (remove extra whitespace, sort lines if needed)
String _normalizeOutput(String output) {
  return output.trim().replaceAll(RegExp(r'\s+'), ' ');
}

String _getItemsHeader() {
  return FileHeaderGenerator.generateItemsFileHeader();
}

String _getOccludeHeader() {
  return FileHeaderGenerator.generateOccludedItemsFileHeader();
}


// =====================================================================
// FILE: packages/giantt_core/test/compatibility/python_dart_validation_test.dart
// =====================================================================

import 'dart:io';
import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

/// Tests that validate Dart implementation against known Python outputs
void main() {
  group('Python-Dart Output Validation', () {
    late Directory tempDir;
    late String itemsPath;
    late String occludeItemsPath;

    setUp(() async {
      tempDir = await Directory.systemTemp.createTemp('giantt_validation_');
      final includeDir = Directory('${tempDir.path}/include');
      final occludeDir = Directory('${tempDir.path}/occlude');
      await includeDir.create(recursive: true);
      await occludeDir.create(recursive: true);
      
      itemsPath = '${includeDir.path}/items.txt';
      occludeItemsPath = '${occludeDir.path}/items.txt';
    });

    tearDown(() async {
      await tempDir.delete(recursive: true);
    });

    test('File format output matches Python exactly', () async {
      print('\n=== File Format Validation ===');
      
      // Expected Python outputs for comparison
      final expectedPythonOutputs = [
        '○ task_a 1d "First Task" {}',
        '◑ task_b!! 2d "Second Task" {"Chart1"} urgent >>> ⊢[task_a]',
        '● task_c... 0.5d "Task with \\"quotes\\" and special chars" {"Chart1","Chart2"} done,tested >>> ⊢[task_b] ►[task_d]',
      ];
      
      print('Expected Python CLI outputs:');
      for (int i = 0; i < expectedPythonOutputs.length; i++) {
        print('  Python[$i]: ${expectedPythonOutputs[i]}');
      }
      
      // Create items using Dart CLI equivalent operations
      final graph = GianttGraph();
      
      // Add items that would be created by Python CLI
      final items = [
        GianttItem(
          id: 'task_a',
          title: 'First Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.parse('1d'),
          charts: [],
          tags: [],
          relations: {},
        ),
        GianttItem(
          id: 'task_b',
          title: 'Second Task',
          status: GianttStatus.inProgress,
          priority: GianttPriority.high,
          duration: GianttDuration.parse('2d'),
          charts: ['Chart1'],
          tags: ['urgent'],
          relations: {'REQUIRES': ['task_a']},
        ),
        GianttItem(
          id: 'task_c',
          title: 'Task with "quotes" and special chars',
          status: GianttStatus.completed,
          priority: GianttPriority.low,
          duration: GianttDuration.parse('0.5d'),
          charts: ['Chart1', 'Chart2'],
          tags: ['done', 'tested'],
          relations: {'REQUIRES': ['task_b'], 'BLOCKS': ['task_d']},
        ),
      ];

      for (final item in items) {
        graph.addItem(item);
      }

      // Save to files
      await File(itemsPath).writeAsString(_getItemsHeader());
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());
      FileRepository.saveGraph(itemsPath, occludeItemsPath, graph);

      // Read the generated file content
      final content = await File(itemsPath).readAsString();
      final lines = content.split('\n').where((line) => 
        line.trim().isNotEmpty && !line.trim().startsWith('#')).toList();

      print('\nActual Dart CLI outputs:');
      for (int i = 0; i < lines.length; i++) {
        print('  Dart[$i]:   ${lines[i]}');
      }
      
      print('\nComparison Results:');
      for (int i = 0; i < expectedPythonOutputs.length && i < lines.length; i++) {
        final match = lines[i] == expectedPythonOutputs[i];
        print('  Line $i: ${match ? "✓ MATCH" : "✗ DIFFER"}');
        if (!match) {
          print('    Expected: ${expectedPythonOutputs[i]}');
          print('    Actual:   ${lines[i]}');
        }
      }

      // Validate exact format matches Python output
      expect(lines.length, equals(3), reason: 'Should have exactly 3 item lines');
      
      // Validate each line matches Python exactly
      for (int i = 0; i < expectedPythonOutputs.length; i++) {
        expect(lines[i], equals(expectedPythonOutputs[i]),
               reason: 'Line $i should match Python output exactly');
      }
    });

    test('Topological sort order matches Python', () async {
      print('\n=== Topological Sort Validation ===');
      
      // Expected Python topological sort order for this dependency chain
      final expectedPythonOrder = ['a_first', 'b_middle', 'z_last'];
      print('Expected Python topological order: $expectedPythonOrder');
      
      // Create a dependency chain that Python would sort in a specific order
      final graph = GianttGraph();
      
      // Add items in random order to test sorting
      final items = [
        GianttItem(
          id: 'z_last',
          title: 'Last Task',
          duration: GianttDuration.parse('1d'),
          relations: {'REQUIRES': ['b_middle']},
        ),
        GianttItem(
          id: 'a_first', 
          title: 'First Task',
          duration: GianttDuration.parse('1d'),
          relations: {},
        ),
        GianttItem(
          id: 'b_middle',
          title: 'Middle Task', 
          duration: GianttDuration.parse('1d'),
          relations: {'REQUIRES': ['a_first']},
        ),
      ];

      print('\nAdding items in random order:');
      // Add in random order
      graph.addItem(items[0]); // z_last
      print('  Added: z_last (requires b_middle)');
      graph.addItem(items[1]); // a_first  
      print('  Added: a_first (no dependencies)');
      graph.addItem(items[2]); // b_middle
      print('  Added: b_middle (requires a_first)');

      // Sort should produce: a_first, b_middle, z_last
      final sorted = graph.topologicalSort();
      final sortedIds = sorted.map((item) => item.id).toList();
      
      print('\nDart topological sort result: $sortedIds');
      print('Comparison: ${sortedIds.toString() == expectedPythonOrder.toString() ? "✓ MATCH" : "✗ DIFFER"}');
      
      expect(sortedIds, equals(expectedPythonOrder),
             reason: 'Topological sort should match Python dependency order');

      // Save and reload to verify file order
      await File(itemsPath).writeAsString(_getItemsHeader());
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());
      FileRepository.saveGraph(itemsPath, occludeItemsPath, graph);
      
      print('\nTesting file save/reload persistence:');
      final reloaded = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      final reloadedSorted = reloaded.topologicalSort();
      final reloadedIds = reloadedSorted.map((item) => item.id).toList();
      
      print('After save/reload: $reloadedIds');
      print('Persistence check: ${reloadedIds.toString() == expectedPythonOrder.toString() ? "✓ PRESERVED" : "✗ LOST"}');
      
      expect(reloadedIds, equals(expectedPythonOrder),
             reason: 'File save/load should preserve topological order');
    });

    test('Symbol mappings are identical to Python', () {
      print('\n=== Symbol Mapping Validation ===');
      
      // Expected Python symbol mappings
      final expectedStatusSymbols = {
        'NOT_STARTED': '○',
        'IN_PROGRESS': '◑', 
        'BLOCKED': '⊘',
        'COMPLETED': '●',
      };
      
      final expectedPrioritySymbols = {
        'LOWEST': ',,,',
        'LOW': '...',
        'NEUTRAL': '',
        'UNSURE': '?',
        'MEDIUM': '!',
        'HIGH': '!!',
        'CRITICAL': '!!!',
      };
      
      final expectedRelationSymbols = {
        'REQUIRES': '⊢',
        'BLOCKS': '►',
        'ANYOF': '⋲',
        'SUFFICIENT': '≻',
        'SUPERCHARGES': '≫',
        'INDICATES': '∴',
        'TOGETHER': '∪',
        'CONFLICTS': '⊟',
      };
      
      print('Python Status Symbols:');
      expectedStatusSymbols.forEach((name, symbol) => 
        print('  $name: "$symbol"'));
      
      print('\nPython Priority Symbols:');
      expectedPrioritySymbols.forEach((name, symbol) => 
        print('  $name: "$symbol"'));
      
      print('\nPython Relation Symbols:');
      expectedRelationSymbols.forEach((name, symbol) => 
        print('  $name: "$symbol"'));

      // Test all status symbols
      final statusMappings = {
        GianttStatus.notStarted: '○',
        GianttStatus.inProgress: '◑', 
        GianttStatus.blocked: '⊘',
        GianttStatus.completed: '●',
      };

      print('\nDart Status Symbol Validation:');
      for (final entry in statusMappings.entries) {
        final match = entry.key.symbol == entry.value;
        print('  ${entry.key.name}: "${entry.key.symbol}" ${match ? "✓" : "✗"}');
        expect(entry.key.symbol, equals(entry.value),
               reason: 'Status ${entry.key.name} should have symbol ${entry.value}');
      }

      // Test all priority symbols
      final priorityMappings = {
        GianttPriority.lowest: ',,,',
        GianttPriority.low: '...',
        GianttPriority.neutral: '',
        GianttPriority.unsure: '?',
        GianttPriority.medium: '!',
        GianttPriority.high: '!!',
        GianttPriority.critical: '!!!',
      };

      print('\nDart Priority Symbol Validation:');
      for (final entry in priorityMappings.entries) {
        final match = entry.key.symbol == entry.value;
        print('  ${entry.key.name}: "${entry.key.symbol}" ${match ? "✓" : "✗"}');
        expect(entry.key.symbol, equals(entry.value),
               reason: 'Priority ${entry.key.name} should have symbol "${entry.value}"');
      }

      // Test relation symbols in item output
      final item = GianttItem(
        id: 'test',
        title: 'Test',
        duration: GianttDuration.parse('1d'),
        relations: {
          'REQUIRES': ['dep1'],
          'BLOCKS': ['blocked1'],
          'ANYOF': ['any1'],
          'SUFFICIENT': ['suff1'],
        },
      );

      final output = item.toFileString();
      print('\nDart Relation Symbol Validation:');
      print('  Generated output: $output');
      
      final relationTests = {
        'REQUIRES': '⊢[dep1]',
        'BLOCKS': '►[blocked1]',
        'ANYOF': '⋲[any1]',
        'SUFFICIENT': '≻[suff1]',
      };
      
      for (final test in relationTests.entries) {
        final containsSymbol = output.contains(test.value);
        print('  ${test.key}: ${test.value} ${containsSymbol ? "✓" : "✗"}');
        expect(output, contains(test.value), 
               reason: '${test.key} should use ${test.value}');
      }
    });

    test('Round-trip parsing preserves data exactly', () {
      print('\n=== Round-trip Parsing Validation ===');
      
      // Test complex items that exercise all parsing features
      final testCases = [
        '○ simple 1d "Simple Task" {}',
        '◑ complex!! 2.5d "Complex Task" {"Chart1","Chart2"} tag1,tag2 >>> ⊢[dep1,dep2] ►[block1]',
        '● escaped 1d "Task with \\"quotes\\" and \\n newlines" {}',
        '⊘ priority,,, 3d "Lowest Priority" {} urgent',
        '○ relations 1d "All Relations" {} >>> ⊢[r1] ⋲[a1] ≫[s1] ∴[i1] ∪[t1] ⊟[c1] ►[b1] ≻[sf1]',
      ];

      print('Testing round-trip parsing (Python format → Dart parse → Dart format):');
      
      for (int i = 0; i < testCases.length; i++) {
        final testCase = testCases[i];
        print('\nTest case ${i + 1}:');
        print('  Input:  $testCase');
        
        // Parse the string
        final parsed = GianttParser.fromString(testCase);
        
        // Convert back to string
        final output = parsed.toFileString();
        print('  Output: $output');
        
        // Check if they match
        final normalizedInput = _normalizeString(testCase);
        final normalizedOutput = _normalizeString(output);
        final match = normalizedOutput == normalizedInput;
        
        print('  Result: ${match ? "✓ PRESERVED" : "✗ CHANGED"}');
        
        if (!match) {
          print('  Normalized Input:  "$normalizedInput"');
          print('  Normalized Output: "$normalizedOutput"');
        }
        
        // Should be identical (normalized for whitespace)
        expect(normalizedOutput, equals(normalizedInput),
               reason: 'Round-trip should preserve: $testCase');
      }
    });

    test('Error handling matches Python behavior', () {
      // Test cycle detection
      final graph = GianttGraph();
      graph.addItem(GianttItem(
        id: 'a',
        title: 'Task A',
        duration: GianttDuration.parse('1d'),
        relations: {'REQUIRES': ['b']},
      ));
      graph.addItem(GianttItem(
        id: 'b', 
        title: 'Task B',
        duration: GianttDuration.parse('1d'),
        relations: {'REQUIRES': ['a']},
      ));

      expect(() => graph.topologicalSort(), 
             throwsA(isA<CycleDetectedException>()),
             reason: 'Should detect cycles like Python');

      // Test invalid parsing
      expect(() => GianttParser.fromString('invalid format'),
             throwsA(isA<GianttParseException>()),
             reason: 'Should throw parse exceptions like Python');
    });

    test('Doctor issues match Python detection', () async {
      // Create graph with known issues that Python would detect
      final graph = GianttGraph();
      
      // Add item with dangling reference
      graph.addItem(GianttItem(
        id: 'broken',
        title: 'Broken Task',
        duration: GianttDuration.parse('1d'),
        relations: {'REQUIRES': ['nonexistent']},
      ));
      
      // Add incomplete chain
      graph.addItem(GianttItem(
        id: 'blocker',
        title: 'Blocker Task', 
        duration: GianttDuration.parse('1d'),
        relations: {'BLOCKS': ['blocked']},
      ));
      graph.addItem(GianttItem(
        id: 'blocked',
        title: 'Blocked Task',
        duration: GianttDuration.parse('1d'),
        relations: {}, // Missing REQUIRES relation
      ));

      final doctor = GraphDoctor(graph);
      final issues = doctor.fullDiagnosis();
      
      // Should find exactly the issues Python would find
      expect(issues.length, equals(2), reason: 'Should find 2 issues');
      
      final issueTypes = issues.map((i) => i.type).toSet();
      expect(issueTypes, contains(IssueType.danglingReference),
             reason: 'Should detect dangling reference');
      expect(issueTypes, contains(IssueType.incompleteChain),
             reason: 'Should detect incomplete chain');

      // Test auto-fix
      final fixed = doctor.fixIssues();
      expect(fixed.length, equals(2), reason: 'Should fix both issues');
      
      final afterFix = doctor.fullDiagnosis();
      expect(afterFix.length, equals(0), reason: 'Should have no issues after fix');
    });

    test('CLI operations produce expected file changes', () async {
      // Initialize files
      await File(itemsPath).writeAsString(_getItemsHeader());
      await File(occludeItemsPath).writeAsString(_getOccludeHeader());

      // Simulate CLI add operation
      final graph = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      
      final newItem = GianttItem(
        id: 'cli_test',
        title: 'CLI Test Item',
        status: GianttStatus.notStarted,
        priority: GianttPriority.high,
        duration: GianttDuration.parse('2d'),
        charts: ['TestChart'],
        tags: ['cli', 'test'],
        relations: {},
      );
      
      graph.addItem(newItem);
      FileRepository.saveGraph(itemsPath, occludeItemsPath, graph);

      // Verify file content
      final content = await File(itemsPath).readAsString();
      expect(content, contains('○ cli_test!! 2d "CLI Test Item" {"TestChart"} cli,test'),
             reason: 'CLI add should produce correct file format');

      // Simulate CLI show operation
      final reloaded = FileRepository.loadGraph(itemsPath, occludeItemsPath);
      final found = reloaded.items['cli_test'];
      expect(found, isNotNull, reason: 'CLI show should find added item');
      expect(found!.title, equals('CLI Test Item'));
      expect(found.priority, equals(GianttPriority.high));
    });
  });
}

String _getItemsHeader() {
  return '''
##############################################
#                                            #
#                Giantt Items                #
#                                            #
#   This file contains all include Giantt   #
#   items in topological order according    #
#   to the REQUIRES (⊢) relation.           #
#   You can use #include directives at the  #
#   top of this file to include other       #
#   Giantt item files.                      #
#   Edit this file manually at your own     #
#   risk.                                    #
#                                            #
##############################################

''';
}

String _getOccludeHeader() {
  return '''
##############################################
#                                            #
#            Giantt Occluded Items           #
#                                            #
#   This file contains all occluded Giantt  #
#   items in topological order according    #
#   to the REQUIRES (⊢) relation.           #
#   Edit this file manually at your own     #
#   risk.                                    #
#                                            #
##############################################

''';
}

String _normalizeString(String str) {
  return str.trim().replaceAll(RegExp(r'\s+'), ' ');
}


// =====================================================================
// FILE: packages/giantt_core/test/graph/topological_sort_test.dart
// =====================================================================

import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

void main() {
  group('Topological Sort Tests', () {
    late GianttGraph graph;

    setUp(() {
      graph = GianttGraph();
    });

    group('Basic Sorting', () {
      test('should sort items with no dependencies', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);

        final sorted = graph.topologicalSort();
        expect(sorted.length, equals(3));
        
        // Should be sorted by ID since no dependencies
        expect(sorted[0].id, equals('A'));
        expect(sorted[1].id, equals('B'));
        expect(sorted[2].id, equals('C'));
      });

      test('should sort items with simple dependencies', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);

        // A requires B, B requires C
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');

        final sorted = graph.topologicalSort();
        expect(sorted.length, equals(3));
        
        // C should come first (no dependencies), then B, then A
        expect(sorted[0].id, equals('C'));
        expect(sorted[1].id, equals('B'));
        expect(sorted[2].id, equals('A'));
      });

      test('should handle complex dependency chains', () {
        final items = ['A', 'B', 'C', 'D', 'E'].map((id) => 
          GianttItem(id: id, title: 'Task $id', duration: GianttDuration.parse('1d'))
        ).toList();

        for (final item in items) {
          graph.addItem(item);
        }

        // Create dependencies: A->B->C, A->D->E, C->E
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');
        graph.addRelation('A', RelationType.requires, 'D');
        graph.addRelation('D', RelationType.requires, 'E');
        graph.addRelation('C', RelationType.requires, 'E');

        final sorted = graph.topologicalSort();
        expect(sorted.length, equals(5));

        // E should come first, then C and D, then B, then A
        expect(sorted[0].id, equals('E'));
        expect(sorted[4].id, equals('A')); // A should be last
        
        // Verify topological order is maintained
        final positions = <String, int>{};
        for (int i = 0; i < sorted.length; i++) {
          positions[sorted[i].id] = i;
        }

        // Check that dependencies come before dependents
        expect(positions['B']! < positions['A']!, isTrue);
        expect(positions['C']! < positions['B']!, isTrue);
        expect(positions['D']! < positions['A']!, isTrue);
        expect(positions['E']! < positions['D']!, isTrue);
        expect(positions['E']! < positions['C']!, isTrue);
      });
    });

    group('Dependency Depth Calculation', () {
      test('should calculate correct dependency depths', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);

        // A requires B, B requires C
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');

        final sorted = graph.topologicalSort();
        
        // Verify depth-based ordering
        expect(sorted[0].id, equals('C')); // Depth 0
        expect(sorted[1].id, equals('B')); // Depth 1
        expect(sorted[2].id, equals('A')); // Depth 2
      });

      test('should handle items with same depth deterministically', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));
        final itemD = GianttItem(id: 'D', title: 'Task D', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);
        graph.addItem(itemD);

        // A and B both require C and D
        graph.addRelation('A', RelationType.requires, 'C');
        graph.addRelation('A', RelationType.requires, 'D');
        graph.addRelation('B', RelationType.requires, 'C');
        graph.addRelation('B', RelationType.requires, 'D');

        final sorted = graph.topologicalSort();
        
        // C and D should come first (sorted by ID), then A and B (sorted by ID)
        expect(sorted[0].id, equals('C'));
        expect(sorted[1].id, equals('D'));
        expect(sorted[2].id, equals('A'));
        expect(sorted[3].id, equals('B'));
      });
    });

    group('Cycle Detection', () {
      test('should detect and report simple cycles', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);

        graph.addRelation('A', RelationType.requires, 'B');

        expect(() => graph.addRelation('B', RelationType.requires, 'A'), 
               throwsA(isA<CycleDetectedException>()));
      });

      test('should detect and report complex cycles', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);

        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');

        expect(() => graph.addRelation('C', RelationType.requires, 'A'), 
               throwsA(isA<CycleDetectedException>()));
      });

      test('should provide detailed cycle information', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);

        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');

        try {
          graph.addRelation('C', RelationType.requires, 'A');
          fail('Expected CycleDetectedException');
        } catch (e) {
          expect(e, isA<CycleDetectedException>());
          final cycleException = e as CycleDetectedException;
          expect(cycleException.cycleItems, isNotEmpty);
          expect(cycleException.toString(), contains('Cycle detected'));
        }
      });
    });

    group('Insert Between', () {
      test('should insert item between two connected items', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);

        // A requires B
        graph.addRelation('A', RelationType.requires, 'B');

        // Insert C between A and B
        graph.insertBetween(itemC, 'B', 'A');

        // Verify the new structure: A requires C, C requires B
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        final updatedC = graph.items['C']!;

        expect(updatedA.relations['REQUIRES'], contains('C'));
        expect(updatedA.relations['REQUIRES'], isNot(contains('B')));
        expect(updatedC.relations['REQUIRES'], contains('B'));
        expect(updatedB.relations['BLOCKS'], contains('C'));
        expect(updatedB.relations['BLOCKS'], isNot(contains('A')));
      });

      test('should maintain topological order after insertion', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);

        graph.addRelation('A', RelationType.requires, 'B');
        graph.insertBetween(itemC, 'B', 'A');

        final sorted = graph.topologicalSort();
        final positions = <String, int>{};
        for (int i = 0; i < sorted.length; i++) {
          positions[sorted[i].id] = i;
        }

        // B should come before C, C should come before A
        expect(positions['B']! < positions['C']!, isTrue);
        expect(positions['C']! < positions['A']!, isTrue);
      });

      test('should throw when trying to insert between non-existent items', () {
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        expect(() => graph.insertBetween(itemC, 'A', 'B'), 
               throwsArgumentError);
      });
    });

    group('ANYOF Relations', () {
      test('should handle ANYOF relations in topological sort', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));
        final itemC = GianttItem(id: 'C', title: 'Task C', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);
        graph.addItem(itemC);

        // A requires any of B or C
        graph.addRelation('A', RelationType.anyof, 'B');
        graph.addRelation('A', RelationType.anyof, 'C');

        final sorted = graph.topologicalSort();
        final positions = <String, int>{};
        for (int i = 0; i < sorted.length; i++) {
          positions[sorted[i].id] = i;
        }

        // Both B and C should come before A
        expect(positions['B']! < positions['A']!, isTrue);
        expect(positions['C']! < positions['A']!, isTrue);
      });

      test('should detect cycles in ANYOF relations', () {
        final itemA = GianttItem(id: 'A', title: 'Task A', duration: GianttDuration.parse('1d'));
        final itemB = GianttItem(id: 'B', title: 'Task B', duration: GianttDuration.parse('1d'));

        graph.addItem(itemA);
        graph.addItem(itemB);

        graph.addRelation('A', RelationType.anyof, 'B');

        expect(() => graph.addRelation('B', RelationType.anyof, 'A'), 
               throwsA(isA<CycleDetectedException>()));
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/test/graph/relation_system_test.dart
// =====================================================================

import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

void main() {
  group('Relation System Tests', () {
    late GianttGraph graph;
    late GianttItem itemA;
    late GianttItem itemB;
    late GianttItem itemC;

    setUp(() {
      graph = GianttGraph();
      
      itemA = GianttItem(
        id: 'A',
        title: 'Task A',
        duration: GianttDuration.parse('1d'),
      );
      
      itemB = GianttItem(
        id: 'B', 
        title: 'Task B',
        duration: GianttDuration.parse('1d'),
      );
      
      itemC = GianttItem(
        id: 'C',
        title: 'Task C', 
        duration: GianttDuration.parse('1d'),
      );

      graph.addItem(itemA);
      graph.addItem(itemB);
      graph.addItem(itemC);
    });

    group('Bidirectional Relations', () {
      test('should create BLOCKS when adding REQUIRES', () {
        graph.addRelation('A', RelationType.requires, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['REQUIRES'], contains('B'));
        expect(updatedB.relations['BLOCKS'], contains('A'));
      });

      test('should create REQUIRES when adding BLOCKS', () {
        graph.addRelation('A', RelationType.blocks, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['BLOCKS'], contains('B'));
        expect(updatedB.relations['REQUIRES'], contains('A'));
      });

      test('should create SUFFICIENT when adding ANYOF', () {
        graph.addRelation('A', RelationType.anyof, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['ANYOF'], contains('B'));
        expect(updatedB.relations['SUFFICIENT'], contains('A'));
      });

      test('should create ANYOF when adding SUFFICIENT', () {
        graph.addRelation('A', RelationType.sufficient, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['SUFFICIENT'], contains('B'));
        expect(updatedB.relations['ANYOF'], contains('A'));
      });

      test('should create symmetric relations for CONFLICTS', () {
        graph.addRelation('A', RelationType.conflicts, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['CONFLICTS'], contains('B'));
        expect(updatedB.relations['CONFLICTS'], contains('A'));
      });

      test('should create symmetric relations for TOGETHER', () {
        graph.addRelation('A', RelationType.together, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['TOGETHER'], contains('B'));
        expect(updatedB.relations['TOGETHER'], contains('A'));
      });
    });

    group('Relation Removal', () {
      test('should remove both directions when removing REQUIRES', () {
        graph.addRelation('A', RelationType.requires, 'B');
        graph.removeRelation('A', RelationType.requires, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['REQUIRES'] ?? [], isEmpty);
        expect(updatedB.relations['BLOCKS'] ?? [], isEmpty);
      });

      test('should remove both directions when removing ANYOF', () {
        graph.addRelation('A', RelationType.anyof, 'B');
        graph.removeRelation('A', RelationType.anyof, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['ANYOF'] ?? [], isEmpty);
        expect(updatedB.relations['SUFFICIENT'] ?? [], isEmpty);
      });
    });

    group('Cycle Detection', () {
      test('should detect simple cycle in REQUIRES relations', () {
        graph.addRelation('A', RelationType.requires, 'B');
        
        expect(
          () => graph.addRelation('B', RelationType.requires, 'A'),
          throwsA(isA<CycleDetectedException>()),
        );
      });

      test('should detect complex cycle in REQUIRES relations', () {
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');
        
        expect(
          () => graph.addRelation('C', RelationType.requires, 'A'),
          throwsA(isA<CycleDetectedException>()),
        );
      });

      test('should detect cycle in ANYOF relations', () {
        graph.addRelation('A', RelationType.anyof, 'B');
        
        expect(
          () => graph.addRelation('B', RelationType.anyof, 'A'),
          throwsA(isA<CycleDetectedException>()),
        );
      });

      test('should allow non-strict relations without cycle detection', () {
        // These should not cause cycle detection errors
        expect(() {
          graph.addRelation('A', RelationType.supercharges, 'B');
          graph.addRelation('B', RelationType.supercharges, 'A');
        }, returnsNormally);

        expect(() {
          graph.addRelation('A', RelationType.indicates, 'B');
          graph.addRelation('B', RelationType.indicates, 'A');
        }, returnsNormally);
      });
    });

    group('Error Handling', () {
      test('should throw when adding relation to non-existent item', () {
        expect(
          () => graph.addRelation('A', RelationType.requires, 'NONEXISTENT'),
          throwsArgumentError,
        );
      });

      test('should throw when adding relation from non-existent item', () {
        expect(
          () => graph.addRelation('NONEXISTENT', RelationType.requires, 'B'),
          throwsArgumentError,
        );
      });

      test('should handle removing non-existent relations gracefully', () {
        expect(
          () => graph.removeRelation('A', RelationType.requires, 'B'),
          returnsNormally,
        );
      });
    });

    group('Complex Scenarios', () {
      test('should handle multiple relations between same items', () {
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('A', RelationType.supercharges, 'B');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        
        expect(updatedA.relations['REQUIRES'], contains('B'));
        expect(updatedA.relations['SUPERCHARGES'], contains('B'));
        expect(updatedB.relations['BLOCKS'], contains('A'));
        expect(updatedB.relations['SUPERCHARGES'], contains('A'));
      });

      test('should prevent duplicate relations', () {
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('A', RelationType.requires, 'B'); // Duplicate
        
        final updatedA = graph.items['A']!;
        expect(updatedA.relations['REQUIRES']!.where((id) => id == 'B').length, equals(1));
      });

      test('should handle chain of dependencies', () {
        graph.addRelation('A', RelationType.requires, 'B');
        graph.addRelation('B', RelationType.requires, 'C');
        
        final updatedA = graph.items['A']!;
        final updatedB = graph.items['B']!;
        final updatedC = graph.items['C']!;
        
        expect(updatedA.relations['REQUIRES'], contains('B'));
        expect(updatedB.relations['REQUIRES'], contains('C'));
        expect(updatedB.relations['BLOCKS'], contains('A'));
        expect(updatedC.relations['BLOCKS'], contains('B'));
      });
    });

    group('Graph Operations', () {
      test('should find items by substring', () {
        final found = graph.findBySubstring('Task A');
        expect(found.id, equals('A'));
      });

      test('should find items by exact ID', () {
        final found = graph.findBySubstring('B');
        expect(found.id, equals('B'));
      });

      test('should throw when no items match substring', () {
        expect(
          () => graph.findBySubstring('Nonexistent'),
          throwsArgumentError,
        );
      });

      test('should throw when multiple items match substring', () {
        final itemD = GianttItem(
          id: 'D',
          title: 'Task D',
          duration: GianttDuration.parse('1d'),
        );
        graph.addItem(itemD);
        
        expect(
          () => graph.findBySubstring('Task'),
          throwsArgumentError,
        );
      });
    });

    group('Occlusion', () {
      test('should separate included and occluded items', () {
        final occludedItem = itemA.copyWith(occlude: true);
        graph.removeItem('A');
        graph.addItem(occludedItem);
        
        expect(graph.includedItems.keys, containsAll(['B', 'C']));
        expect(graph.includedItems.keys, isNot(contains('A')));
        expect(graph.occludedItems.keys, contains('A'));
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/test/models/symbol_conversion_test.dart
// =====================================================================

import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

void main() {
  group('Symbol Conversion Tests', () {
    group('GianttStatus', () {
      test('should have correct symbols matching Python', () {
        expect(GianttStatus.notStarted.symbol, equals('○'));
        expect(GianttStatus.inProgress.symbol, equals('◑'));
        expect(GianttStatus.blocked.symbol, equals('⊘'));
        expect(GianttStatus.completed.symbol, equals('●'));
      });

      test('should convert from symbol correctly', () {
        expect(GianttStatus.fromSymbol('○'), equals(GianttStatus.notStarted));
        expect(GianttStatus.fromSymbol('◑'), equals(GianttStatus.inProgress));
        expect(GianttStatus.fromSymbol('⊘'), equals(GianttStatus.blocked));
        expect(GianttStatus.fromSymbol('●'), equals(GianttStatus.completed));
      });

      test('should convert from name correctly', () {
        expect(GianttStatus.fromName('NOT_STARTED'), equals(GianttStatus.notStarted));
        expect(GianttStatus.fromName('IN_PROGRESS'), equals(GianttStatus.inProgress));
        expect(GianttStatus.fromName('BLOCKED'), equals(GianttStatus.blocked));
        expect(GianttStatus.fromName('COMPLETED'), equals(GianttStatus.completed));
      });

      test('should throw on invalid symbol', () {
        expect(() => GianttStatus.fromSymbol('X'), throwsArgumentError);
      });

      test('should throw on invalid name', () {
        expect(() => GianttStatus.fromName('INVALID'), throwsArgumentError);
      });
    });

    group('GianttPriority', () {
      test('should have correct symbols matching Python', () {
        expect(GianttPriority.lowest.symbol, equals(',,,'));
        expect(GianttPriority.low.symbol, equals('...'));
        expect(GianttPriority.neutral.symbol, equals(''));
        expect(GianttPriority.unsure.symbol, equals('?'));
        expect(GianttPriority.medium.symbol, equals('!'));
        expect(GianttPriority.high.symbol, equals('!!'));
        expect(GianttPriority.critical.symbol, equals('!!!'));
      });

      test('should convert from symbol correctly', () {
        expect(GianttPriority.fromSymbol(',,,'), equals(GianttPriority.lowest));
        expect(GianttPriority.fromSymbol('...'), equals(GianttPriority.low));
        expect(GianttPriority.fromSymbol(''), equals(GianttPriority.neutral));
        expect(GianttPriority.fromSymbol('?'), equals(GianttPriority.unsure));
        expect(GianttPriority.fromSymbol('!'), equals(GianttPriority.medium));
        expect(GianttPriority.fromSymbol('!!'), equals(GianttPriority.high));
        expect(GianttPriority.fromSymbol('!!!'), equals(GianttPriority.critical));
      });

      test('should convert from name correctly', () {
        expect(GianttPriority.fromName('LOWEST'), equals(GianttPriority.lowest));
        expect(GianttPriority.fromName('LOW'), equals(GianttPriority.low));
        expect(GianttPriority.fromName('NEUTRAL'), equals(GianttPriority.neutral));
        expect(GianttPriority.fromName('UNSURE'), equals(GianttPriority.unsure));
        expect(GianttPriority.fromName('MEDIUM'), equals(GianttPriority.medium));
        expect(GianttPriority.fromName('HIGH'), equals(GianttPriority.high));
        expect(GianttPriority.fromName('CRITICAL'), equals(GianttPriority.critical));
      });
    });

    group('RelationType', () {
      test('should have correct symbols matching Python', () {
        expect(RelationType.requires.symbol, equals('⊢'));
        expect(RelationType.anyof.symbol, equals('⋲'));
        expect(RelationType.supercharges.symbol, equals('≫'));
        expect(RelationType.indicates.symbol, equals('∴'));
        expect(RelationType.together.symbol, equals('∪'));
        expect(RelationType.conflicts.symbol, equals('⊟'));
        expect(RelationType.blocks.symbol, equals('►'));
        expect(RelationType.sufficient.symbol, equals('≻'));
      });

      test('should convert from symbol correctly', () {
        expect(RelationType.fromSymbol('⊢'), equals(RelationType.requires));
        expect(RelationType.fromSymbol('⋲'), equals(RelationType.anyof));
        expect(RelationType.fromSymbol('≫'), equals(RelationType.supercharges));
        expect(RelationType.fromSymbol('∴'), equals(RelationType.indicates));
        expect(RelationType.fromSymbol('∪'), equals(RelationType.together));
        expect(RelationType.fromSymbol('⊟'), equals(RelationType.conflicts));
        expect(RelationType.fromSymbol('►'), equals(RelationType.blocks));
        expect(RelationType.fromSymbol('≻'), equals(RelationType.sufficient));
      });

      test('should convert from name correctly', () {
        expect(RelationType.fromName('REQUIRES'), equals(RelationType.requires));
        expect(RelationType.fromName('ANYOF'), equals(RelationType.anyof));
        expect(RelationType.fromName('SUPERCHARGES'), equals(RelationType.supercharges));
        expect(RelationType.fromName('INDICATES'), equals(RelationType.indicates));
        expect(RelationType.fromName('TOGETHER'), equals(RelationType.together));
        expect(RelationType.fromName('CONFLICTS'), equals(RelationType.conflicts));
        expect(RelationType.fromName('BLOCKS'), equals(RelationType.blocks));
        expect(RelationType.fromName('SUFFICIENT'), equals(RelationType.sufficient));
      });
    });

    group('Duration parsing', () {
      test('should parse simple durations', () {
        final duration = GianttDuration.parse('5d');
        expect(duration.parts.length, equals(1));
        expect(duration.parts[0].amount, equals(5.0));
        expect(duration.parts[0].unit, equals('d'));
      });

      test('should parse compound durations', () {
        final duration = GianttDuration.parse('6mo8d3.5s');
        expect(duration.parts.length, equals(3));
        expect(duration.parts[0].amount, equals(6.0));
        expect(duration.parts[0].unit, equals('mo'));
        expect(duration.parts[1].amount, equals(8.0));
        expect(duration.parts[1].unit, equals('d'));
        expect(duration.parts[2].amount, equals(3.5));
        expect(duration.parts[2].unit, equals('s'));
      });

      test('should normalize units correctly', () {
        final hourDuration = DurationPart.create(2.0, 'hr');
        expect(hourDuration.unit, equals('h'));
        
        final dayDuration = DurationPart.create(1.0, 'day');
        expect(dayDuration.unit, equals('d'));
      });

      test('should calculate total seconds correctly', () {
        final duration = GianttDuration.parse('1h30min');
        expect(duration.totalSeconds, equals(5400.0)); // 3600 + 1800
      });
    });

    group('TimeConstraint parsing', () {
      test('should parse window constraints', () {
        final constraint = TimeConstraint.fromString('window(5d:2d,severe)');
        expect(constraint, isNotNull);
        expect(constraint!.type, equals(TimeConstraintType.window));
        expect(constraint.duration.toString(), equals('5d'));
        expect(constraint.gracePeriod?.toString(), equals('2d'));
        expect(constraint.consequenceType, equals(ConsequenceType.severe));
      });

      test('should parse deadline constraints', () {
        final constraint = TimeConstraint.fromString('due(2024-12-31:2d,severe)');
        expect(constraint, isNotNull);
        expect(constraint!.type, equals(TimeConstraintType.deadline));
        expect(constraint.dueDate, equals('2024-12-31'));
        expect(constraint.gracePeriod?.toString(), equals('2d'));
        expect(constraint.consequenceType, equals(ConsequenceType.severe));
      });

      test('should parse recurring constraints', () {
        final constraint = TimeConstraint.fromString('every(7d:1d,warn,stack)');
        expect(constraint, isNotNull);
        expect(constraint!.type, equals(TimeConstraintType.recurring));
        expect(constraint.interval?.toString(), equals('7d'));
        expect(constraint.gracePeriod?.toString(), equals('1d'));
        expect(constraint.consequenceType, equals(ConsequenceType.warning));
        expect(constraint.stack, isTrue);
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/test/parser/giantt_parser_test.dart
// =====================================================================

import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

void main() {
  group('GianttParser Tests', () {
    group('Basic parsing', () {
      test('should parse simple item', () {
        const line = '○ task1 1d "Simple task" {} tag1,tag2';
        final item = GianttParser.fromString(line);
        
        expect(item.id, equals('task1'));
        expect(item.title, equals('Simple task'));
        expect(item.status, equals(GianttStatus.notStarted));
        expect(item.priority, equals(GianttPriority.neutral));
        expect(item.duration.toString(), equals('1d'));
        expect(item.charts, isEmpty);
        expect(item.tags, equals(['tag1', 'tag2']));
        expect(item.relations, isEmpty);
      });

      test('should parse item with priority', () {
        const line = '◑ task2!! 2w "High priority task" {"Chart1"} urgent';
        final item = GianttParser.fromString(line);
        
        expect(item.id, equals('task2'));
        expect(item.priority, equals(GianttPriority.high));
        expect(item.status, equals(GianttStatus.inProgress));
        expect(item.duration.toString(), equals('2w'));
        expect(item.charts, equals(['Chart1']));
        expect(item.tags, equals(['urgent']));
      });

      test('should parse item with all priority levels', () {
        final testCases = [
          ('task_lowest,,,', GianttPriority.lowest),
          ('task_low...', GianttPriority.low),
          ('task_neutral', GianttPriority.neutral),
          ('task_unsure?', GianttPriority.unsure),
          ('task_medium!', GianttPriority.medium),
          ('task_high!!', GianttPriority.high),
          ('task_critical!!!', GianttPriority.critical),
        ];

        for (final (idPriority, expectedPriority) in testCases) {
          final line = '○ $idPriority 1d "Test task" {}';
          final item = GianttParser.fromString(line);
          expect(item.priority, equals(expectedPriority));
        }
      });
    });

    group('Complex parsing', () {
      test('should parse item with relations', () {
        const line = '● task3 3d "Complex task" {"Chart1","Chart2"} tag1,tag2 >>> ⊢[dep1,dep2] ►[blocked1]';
        final item = GianttParser.fromString(line);
        
        expect(item.id, equals('task3'));
        expect(item.status, equals(GianttStatus.completed));
        expect(item.charts, equals(['Chart1', 'Chart2']));
        expect(item.relations['REQUIRES'], equals(['dep1', 'dep2']));
        expect(item.relations['BLOCKS'], equals(['blocked1']));
      });

      test('should parse item with time constraint', () {
        const line = '⊘ task4 1w "Blocked task" {} tag1 >>> ⊢[dep1] @@@ window(5d:2d,severe)';
        final item = GianttParser.fromString(line);
        
        expect(item.id, equals('task4'));
        expect(item.status, equals(GianttStatus.blocked));
        expect(item.timeConstraint, isNotNull);
        expect(item.timeConstraint!.type, equals(TimeConstraintType.window));
        expect(item.timeConstraint!.duration.toString(), equals('5d'));
        expect(item.timeConstraint!.gracePeriod?.toString(), equals('2d'));
      });

      test('should parse item with comments', () {
        const line = '○ task5 1d "Task with comments" {} tag1 # User comment ### Auto comment';
        final item = GianttParser.fromString(line);
        
        expect(item.id, equals('task5'));
        expect(item.userComment, equals('User comment'));
        expect(item.autoComment, equals('Auto comment'));
      });

      test('should parse item with JSON-escaped title', () {
        const line = r'○ task6 1d "Task with \"quotes\" and \n newlines" {} tag1';
        final item = GianttParser.fromString(line);
        
        expect(item.id, equals('task6'));
        expect(item.title, equals('Task with "quotes" and \n newlines'));
      });

      test('should parse compound duration', () {
        const line = '○ task7 6mo8d3.5s "Long task" {}';
        final item = GianttParser.fromString(line);
        
        expect(item.duration.parts.length, equals(3));
        expect(item.duration.parts[0].amount, equals(6.0));
        expect(item.duration.parts[0].unit, equals('mo'));
        expect(item.duration.parts[1].amount, equals(8.0));
        expect(item.duration.parts[1].unit, equals('d'));
        expect(item.duration.parts[2].amount, equals(3.5));
        expect(item.duration.parts[2].unit, equals('s'));
      });
    });

    group('Round-trip parsing', () {
      test('should maintain data integrity through parse -> toString -> parse', () {
        const originalLine = '◑ complex_task!! 2w3d "Complex task with everything" {"Chart1","Chart2"} urgent,important >>> ⊢[dep1,dep2] ►[blocked1] ≫[enhanced1] @@@ window(5d:2d,severe) # User note ### Auto note';
        
        final item = GianttParser.fromString(originalLine);
        final regeneratedLine = GianttParser.itemToString(item);
        final reparsedItem = GianttParser.fromString(regeneratedLine);
        
        expect(reparsedItem.id, equals(item.id));
        expect(reparsedItem.title, equals(item.title));
        expect(reparsedItem.status, equals(item.status));
        expect(reparsedItem.priority, equals(item.priority));
        expect(reparsedItem.duration, equals(item.duration));
        expect(reparsedItem.charts, equals(item.charts));
        expect(reparsedItem.tags, equals(item.tags));
        expect(reparsedItem.relations, equals(item.relations));
        expect(reparsedItem.userComment, equals(item.userComment));
        expect(reparsedItem.autoComment, equals(item.autoComment));
      });

      test('should handle empty charts and tags', () {
        const line = '○ simple_task 1d "Simple task" {}';
        final item = GianttParser.fromString(line);
        final regenerated = GianttParser.itemToString(item);
        final reparsed = GianttParser.fromString(regenerated);
        
        expect(reparsed.charts, isEmpty);
        expect(reparsed.tags, isEmpty);
        expect(reparsed.relations, isEmpty);
      });
    });

    group('Error handling', () {
      test('should throw on invalid pre-title format', () {
        const line = 'invalid format "Title" {}';
        expect(() => GianttParser.fromString(line), throwsA(isA<GianttParseException>()));
      });

      test('should throw on missing title quotes', () {
        const line = '○ task1 1d Title without quotes {}';
        expect(() => GianttParser.fromString(line), throwsA(isA<GianttParseException>()));
      });

      test('should throw on unbalanced quotes', () {
        const line = '○ task1 1d "Unbalanced quote {}';
        expect(() => GianttParser.fromString(line), throwsA(isA<GianttParseException>()));
      });

      test('should throw on invalid status symbol', () {
        const line = 'X task1 1d "Invalid status" {}';
        expect(() => GianttParser.fromString(line), throwsA(isA<GianttParseException>()));
      });

      test('should throw on invalid duration', () {
        const line = '○ task1 invalid_duration "Title" {}';
        expect(() => GianttParser.fromString(line), throwsA(isA<GianttParseException>()));
      });

      test('should handle empty and comment lines', () {
        expect(() => GianttParser.fromString(''), throwsA(isA<GianttParseException>()));
        expect(() => GianttParser.fromString('# This is a comment'), throwsA(isA<GianttParseException>()));
        expect(() => GianttParser.fromString('   '), throwsA(isA<GianttParseException>()));
      });
    });

    group('All relation types', () {
      test('should parse all relation types correctly', () {
        const line = '○ task1 1d "Task with all relations" {} >>> ⊢[req1] ⋲[any1] ≫[super1] ∴[ind1] ∪[tog1] ⊟[conf1] ►[block1] ≻[suff1]';
        final item = GianttParser.fromString(line);
        
        expect(item.relations['REQUIRES'], equals(['req1']));
        expect(item.relations['ANYOF'], equals(['any1']));
        expect(item.relations['SUPERCHARGES'], equals(['super1']));
        expect(item.relations['INDICATES'], equals(['ind1']));
        expect(item.relations['TOGETHER'], equals(['tog1']));
        expect(item.relations['CONFLICTS'], equals(['conf1']));
        expect(item.relations['BLOCKS'], equals(['block1']));
        expect(item.relations['SUFFICIENT'], equals(['suff1']));
      });
    });

    group('Time constraints', () {
      test('should parse all time constraint types', () {
        final testCases = [
          ('window(5d:2d,severe)', TimeConstraintType.window),
          ('due(2024-12-31:1d,warn)', TimeConstraintType.deadline),
          ('every(7d:1d,escalating,stack)', TimeConstraintType.recurring),
        ];

        for (final (constraintStr, expectedType) in testCases) {
          final line = '○ task1 1d "Test task" {} >>> @@@ $constraintStr';
          final item = GianttParser.fromString(line);
          expect(item.timeConstraint?.type, equals(expectedType));
        }
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/test/storage/robust_file_operations_test.dart
// =====================================================================

import 'dart:io';
import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

void main() {
  group('Robust File Operations Tests', () {
    late Directory tempDir;
    
    setUp(() async {
      tempDir = await Directory.systemTemp.createTemp('giantt_robust_test_');
    });
    
    tearDown(() async {
      if (tempDir.existsSync()) {
        await tempDir.delete(recursive: true);
      }
    });

    group('BackupManager', () {
      test('should create backup with incremental naming', () async {
        final testFile = File('${tempDir.path}/test.txt');
        await testFile.writeAsString('original content');

        final backupPath = BackupManager.createBackup(testFile.path);
        
        expect(File(backupPath).existsSync(), isTrue);
        expect(backupPath, endsWith('.1.backup'));
        expect(await File(backupPath).readAsString(), equals('original content'));
      });

      test('should create multiple backups with different numbers', () async {
        final testFile = File('${tempDir.path}/test.txt');
        await testFile.writeAsString('content 1');

        final backup1 = BackupManager.createBackup(testFile.path);
        
        await testFile.writeAsString('content 2');
        final backup2 = BackupManager.createBackup(testFile.path);

        expect(backup1, endsWith('.1.backup'));
        expect(backup2, endsWith('.2.backup'));
        expect(await File(backup1).readAsString(), equals('content 1'));
        expect(await File(backup2).readAsString(), equals('content 2'));
      });

      test('should clean up old backups beyond retention count', () async {
        final testFile = File('${tempDir.path}/test.txt');
        
        // Create 5 backups
        for (int i = 1; i <= 5; i++) {
          await testFile.writeAsString('content $i');
          BackupManager.createBackup(testFile.path, retentionCount: 3);
        }

        final allBackups = BackupManager.getAllBackups(testFile.path);
        expect(allBackups.length, equals(3));
      });

      test('should find most recent backup', () async {
        final testFile = File('${tempDir.path}/test.txt');
        await testFile.writeAsString('content 1');
        BackupManager.createBackup(testFile.path);
        
        await testFile.writeAsString('content 2');
        BackupManager.createBackup(testFile.path);

        final mostRecent = BackupManager.getMostRecentBackup(testFile.path);
        expect(mostRecent, isNotNull);
        expect(mostRecent, endsWith('.2.backup'));
      });

      test('should detect identical backup and remove it', () async {
        final testFile = File('${tempDir.path}/test.txt');
        await testFile.writeAsString('same content');
        
        BackupManager.createBackup(testFile.path);
        
        // Write same content again
        await testFile.writeAsString('same content');
        
        expect(BackupManager.isIdenticalToMostRecentBackup(testFile.path), isTrue);
        
        BackupManager.removeDuplicateBackup(testFile.path);
        expect(BackupManager.getMostRecentBackup(testFile.path), isNull);
      });
    });

    group('AtomicFileWriter', () {
      test('should write file atomically', () async {
        final testFile = '${tempDir.path}/atomic_test.txt';
        const content = 'atomic content';

        AtomicFileWriter.writeFile(testFile, content);
        
        expect(File(testFile).existsSync(), isTrue);
        expect(await File(testFile).readAsString(), equals(content));
      });

      test('should create backup before writing', () async {
        final testFile = File('${tempDir.path}/backup_test.txt');
        await testFile.writeAsString('original');

        AtomicFileWriter.writeFile(testFile.path, 'updated');
        
        final backup = BackupManager.getMostRecentBackup(testFile.path);
        expect(backup, isNotNull);
        expect(await File(backup!).readAsString(), equals('original'));
        expect(await testFile.readAsString(), equals('updated'));
      });

      test('should write multiple files atomically', () async {
        final file1 = '${tempDir.path}/file1.txt';
        final file2 = '${tempDir.path}/file2.txt';
        
        final contents = {
          file1: 'content 1',
          file2: 'content 2',
        };

        AtomicFileWriter.writeFiles(contents);
        
        expect(await File(file1).readAsString(), equals('content 1'));
        expect(await File(file2).readAsString(), equals('content 2'));
      });

      test('should rollback on failure in multi-file write', () async {
        final file1 = File('${tempDir.path}/file1.txt');
        await file1.writeAsString('original 1');
        
        final file2 = File('${tempDir.path}/file2.txt');
        await file2.writeAsString('original 2');

        // Create a scenario that might fail (invalid path)
        final contents = {
          file1.path: 'new content 1',
          '${tempDir.path}/invalid\x00path/file.txt': 'should fail',
        };

        expect(() => AtomicFileWriter.writeFiles(contents), throwsA(isA<GraphException>()));
        
        // Original files should be unchanged
        expect(await file1.readAsString(), equals('original 1'));
        expect(await file2.readAsString(), equals('original 2'));
      });

      test('should check if file write is safe', () {
        final testFile = '${tempDir.path}/safe_test.txt';
        expect(AtomicFileWriter.canWriteFile(testFile, 'test content'), isTrue);
        
        // Test with invalid path (if possible on current platform)
        expect(AtomicFileWriter.canWriteFile('/invalid/path/file.txt', 'test'), isFalse);
      });
    });

    group('FileHeaderGenerator', () {
      test('should create banner with proper formatting', () {
        final banner = FileHeaderGenerator.createBanner('Test\nBanner');
        
        expect(banner, contains('Test'));
        expect(banner, contains('Banner'));
        expect(banner, startsWith('#'));
        expect(banner, endsWith('#\n'));
      });

      test('should generate items file header', () {
        final header = FileHeaderGenerator.generateItemsFileHeader();
        
        expect(header, contains('Giantt Items'));
        expect(header, contains('topological'));
        expect(header, contains('#include'));
      });

      test('should generate occluded items header', () {
        final header = FileHeaderGenerator.generateOccludedItemsFileHeader();
        
        expect(header, contains('Occluded Items'));
        expect(header, contains('topological'));
      });

      test('should generate custom header', () {
        final header = FileHeaderGenerator.generateCustomHeader('Custom Title', 'Custom description');
        
        expect(header, contains('Custom Title'));
        expect(header, contains('Custom description'));
      });

      test('should generate include directive', () {
        final directive = FileHeaderGenerator.generateIncludeDirective('path/to/file.txt');
        expect(directive, equals('#include path/to/file.txt\n'));
      });
    });

    group('PathResolver', () {
      test('should resolve relative paths correctly', () {
        final basePath = '/base/path';
        final relativePath = '../other/file.txt';
        
        final resolved = PathResolver.resolvePath(basePath, relativePath);
        expect(resolved, equals('/base/other/file.txt'));
      });

      test('should handle absolute paths', () {
        expect(PathResolver.isAbsolutePath('/absolute/path'), isTrue);
        expect(PathResolver.isAbsolutePath('relative/path'), isFalse);
        
        if (Platform.isWindows) {
          expect(PathResolver.isAbsolutePath('C:\\Windows'), isTrue);
          expect(PathResolver.isAbsolutePath('\\\\server\\share'), isTrue);
        }
      });

      test('should normalize paths for current platform', () {
        final path = 'some/path/with/forward/slashes';
        final normalized = PathResolver.normalizePath(path);
        
        if (Platform.isWindows) {
          expect(normalized, contains('\\'));
        } else {
          expect(normalized, contains('/'));
        }
      });

      test('should create safe filenames', () {
        final unsafeName = 'file<>:"/\\|?*name.txt';
        final safeName = PathResolver.getSafeFilename(unsafeName);
        
        expect(safeName, isNot(contains('<')));
        expect(safeName, isNot(contains('>')));
        expect(safeName, isNot(contains(':')));
        expect(safeName, contains('file'));
        expect(safeName, contains('name.txt'));
      });

      test('should ensure directory exists', () {
        final testDir = '${tempDir.path}/new/nested/directory';
        
        PathResolver.ensureDirectoryExists(testDir);
        expect(Directory(testDir).existsSync(), isTrue);
      });

      test('should get relative path between directories', () {
        final from = '/base/current/dir';
        final to = '/base/other/target';
        
        final relative = PathResolver.getRelativePath(from, to);
        expect(relative, equals('../../other/target'));
      });
    });

    group('FileRepository Integration', () {
      test('should initialize workspace with proper structure', () {
        final workspacePath = '${tempDir.path}/workspace';
        
        FileRepository.initializeWorkspace(workspacePath);
        
        expect(Directory('$workspacePath/include').existsSync(), isTrue);
        expect(Directory('$workspacePath/occlude').existsSync(), isTrue);
        expect(File('$workspacePath/include/items.txt').existsSync(), isTrue);
        expect(File('$workspacePath/occlude/items.txt').existsSync(), isTrue);
      });

      test('should validate workspace correctly', () {
        final workspacePath = '${tempDir.path}/workspace';
        FileRepository.initializeWorkspace(workspacePath);
        
        expect(() => FileRepository.validateWorkspace(workspacePath), returnsNormally);
      });

      test('should save and load graph with atomic operations', () async {
        final workspacePath = '${tempDir.path}/workspace';
        FileRepository.initializeWorkspace(workspacePath);
        
        final paths = FileRepository.getDefaultFilePaths(workspacePath);
        
        // Create a test graph
        final graph = GianttGraph();
        final item = GianttItem(
          id: 'test1',
          title: 'Test Item',
          duration: GianttDuration.parse('1d'),
        );
        graph.addItem(item);
        
        // Save the graph
        FileRepository.saveGraph(paths['items']!, paths['occlude_items']!, graph);
        
        // Load it back
        final loadedGraph = FileRepository.loadGraph(paths['items']!, paths['occlude_items']!);
        
        expect(loadedGraph.items.length, equals(1));
        expect(loadedGraph.items['test1'], isNotNull);
        expect(loadedGraph.items['test1']!.title, equals('Test Item'));
      });

      test('should handle file headers in saved files', () async {
        final workspacePath = '${tempDir.path}/workspace';
        FileRepository.initializeWorkspace(workspacePath);
        
        final paths = FileRepository.getDefaultFilePaths(workspacePath);
        
        final graph = GianttGraph();
        FileRepository.saveGraph(paths['items']!, paths['occlude_items']!, graph);
        
        final itemsContent = await File(paths['items']!).readAsString();
        final occludeContent = await File(paths['occlude_items']!).readAsString();
        
        expect(itemsContent, contains('Giantt Items'));
        expect(occludeContent, contains('Occluded Items'));
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/test/storage/include_system_test.dart
// =====================================================================

import 'dart:io';
import 'package:test/test.dart';
import 'package:giantt_core/giantt_core.dart';

void main() {
  group('Include System Tests', () {
    late Directory tempDir;
    
    setUp(() async {
      tempDir = await Directory.systemTemp.createTemp('giantt_test_');
    });
    
    tearDown(() async {
      if (tempDir.existsSync()) {
        await tempDir.delete(recursive: true);
      }
    });

    group('Include Directive Parsing', () {
      test('should parse include directives from file header', () async {
        final testFile = File('${tempDir.path}/test.txt');
        await testFile.writeAsString('''
#include shared/common.txt
#include ../other/tasks.txt

○ task1 1d "Main task" {}
○ task2 2d "Another task" {}
''');

        final includes = FileRepository.parseIncludeDirectives(testFile.path);
        expect(includes, equals(['shared/common.txt', '../other/tasks.txt']));
      });

      test('should stop parsing includes when non-directive line is found', () async {
        final testFile = File('${tempDir.path}/test.txt');
        await testFile.writeAsString('''
#include first.txt
#include second.txt

○ task1 1d "Task" {}
#include third.txt
''');

        final includes = FileRepository.parseIncludeDirectives(testFile.path);
        expect(includes, equals(['first.txt', 'second.txt']));
      });

      test('should handle empty files gracefully', () async {
        final testFile = File('${tempDir.path}/empty.txt');
        await testFile.writeAsString('');

        final includes = FileRepository.parseIncludeDirectives(testFile.path);
        expect(includes, isEmpty);
      });

      test('should handle files with no includes', () async {
        final testFile = File('${tempDir.path}/no_includes.txt');
        await testFile.writeAsString('''
○ task1 1d "Task without includes" {}
○ task2 2d "Another task" {}
''');

        final includes = FileRepository.parseIncludeDirectives(testFile.path);
        expect(includes, isEmpty);
      });

      test('should handle non-existent files', () {
        final includes = FileRepository.parseIncludeDirectives('${tempDir.path}/nonexistent.txt');
        expect(includes, isEmpty);
      });
    });

    group('Graph Loading with Includes', () {
      test('should load graph from single file without includes', () async {
        final testFile = File('${tempDir.path}/simple.txt');
        await testFile.writeAsString('''
○ task1 1d "Simple task" {}
◑ task2!! 2w "High priority task" {"Chart1"}
''');

        final graph = FileRepository.loadGraphFromFile(testFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['task1'], isNotNull);
        expect(graph.items['task2'], isNotNull);
        expect(graph.items['task2']!.priority, equals(GianttPriority.high));
      });

      test('should load graph with simple includes', () async {
        // Create included file
        final includedFile = File('${tempDir.path}/included.txt');
        await includedFile.writeAsString('''
○ shared_task 1d "Shared task" {}
''');

        // Create main file with include
        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include included.txt

○ main_task 2d "Main task" {}
''');

        final graph = FileRepository.loadGraphFromFile(mainFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['shared_task'], isNotNull);
        expect(graph.items['main_task'], isNotNull);
      });

      test('should handle relative path includes', () async {
        // Create subdirectory
        final subDir = Directory('${tempDir.path}/shared');
        await subDir.create();

        // Create included file in subdirectory
        final includedFile = File('${subDir.path}/common.txt');
        await includedFile.writeAsString('''
○ common_task 1d "Common task" {}
''');

        // Create main file with relative include
        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include shared/common.txt

○ main_task 2d "Main task" {}
''');

        final graph = FileRepository.loadGraphFromFile(mainFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['common_task'], isNotNull);
        expect(graph.items['main_task'], isNotNull);
      });

      test('should handle nested includes', () async {
        // Create deeply included file
        final deepFile = File('${tempDir.path}/deep.txt');
        await deepFile.writeAsString('''
○ deep_task 1d "Deep task" {}
''');

        // Create middle file that includes deep file
        final middleFile = File('${tempDir.path}/middle.txt');
        await middleFile.writeAsString('''
#include deep.txt

○ middle_task 2d "Middle task" {}
''');

        // Create main file that includes middle file
        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include middle.txt

○ main_task 3d "Main task" {}
''');

        final graph = FileRepository.loadGraphFromFile(mainFile.path);
        expect(graph.items.length, equals(3));
        expect(graph.items['deep_task'], isNotNull);
        expect(graph.items['middle_task'], isNotNull);
        expect(graph.items['main_task'], isNotNull);
      });

      test('should detect circular includes', () async {
        // Create file A that includes B
        final fileA = File('${tempDir.path}/a.txt');
        await fileA.writeAsString('''
#include b.txt

○ task_a 1d "Task A" {}
''');

        // Create file B that includes A (circular)
        final fileB = File('${tempDir.path}/b.txt');
        await fileB.writeAsString('''
#include a.txt

○ task_b 1d "Task B" {}
''');

        expect(
          () => FileRepository.loadGraphFromFile(fileA.path),
          throwsA(isA<GraphException>()),
        );
      });

      test('should handle missing included files gracefully', () async {
        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include nonexistent.txt

○ main_task 1d "Main task" {}
''');

        expect(
          () => FileRepository.loadGraphFromFile(mainFile.path),
          throwsA(isA<GraphException>()),
        );
      });

      test('should skip invalid lines with warning', () async {
        final testFile = File('${tempDir.path}/with_invalid.txt');
        await testFile.writeAsString('''
○ valid_task 1d "Valid task" {}
invalid line without proper format
◑ another_valid!! 2d "Another valid task" {}
''');

        final graph = FileRepository.loadGraphFromFile(testFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['valid_task'], isNotNull);
        expect(graph.items['another_valid'], isNotNull);
      });
    });

    group('Dual File Loading', () {
      test('should load from both include and occlude files', () async {
        // Create include file
        final includeFile = File('${tempDir.path}/include.txt');
        await includeFile.writeAsString('''
○ active_task 1d "Active task" {}
''');

        // Create occlude file
        final occludeFile = File('${tempDir.path}/occlude.txt');
        await occludeFile.writeAsString('''
● archived_task 1d "Archived task" {}
''');

        final graph = FileRepository.loadGraph(includeFile.path, occludeFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['active_task'], isNotNull);
        expect(graph.items['archived_task'], isNotNull);
        expect(graph.items['active_task']!.occlude, isFalse);
        expect(graph.items['archived_task']!.occlude, isTrue);
      });

      test('should handle includes in both files', () async {
        // Create shared include
        final sharedFile = File('${tempDir.path}/shared.txt');
        await sharedFile.writeAsString('''
○ shared_task 1d "Shared task" {}
''');

        // Create include file with include directive
        final includeFile = File('${tempDir.path}/include.txt');
        await includeFile.writeAsString('''
#include shared.txt

○ active_task 1d "Active task" {}
''');

        // Create occlude file
        final occludeFile = File('${tempDir.path}/occlude.txt');
        await occludeFile.writeAsString('''
● archived_task 1d "Archived task" {}
''');

        final graph = FileRepository.loadGraph(includeFile.path, occludeFile.path);
        expect(graph.items.length, equals(3));
        expect(graph.items['shared_task'], isNotNull);
        expect(graph.items['active_task'], isNotNull);
        expect(graph.items['archived_task'], isNotNull);
      });
    });

    group('Include Structure Display', () {
      test('should show simple include structure', () async {
        final includedFile = File('${tempDir.path}/included.txt');
        await includedFile.writeAsString('○ task 1d "Task" {}');

        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include included.txt

○ main_task 1d "Main task" {}
''');

        // This should not throw and should print the structure
        expect(
          () => FileRepository.showIncludeStructure(mainFile.path),
          returnsNormally,
        );
      });

      test('should show recursive include structure', () async {
        final deepFile = File('${tempDir.path}/deep.txt');
        await deepFile.writeAsString('○ deep 1d "Deep" {}');

        final middleFile = File('${tempDir.path}/middle.txt');
        await middleFile.writeAsString('''
#include deep.txt

○ middle 1d "Middle" {}
''');

        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include middle.txt

○ main 1d "Main" {}
''');

        expect(
          () => FileRepository.showIncludeStructure(mainFile.path, recursive: true),
          returnsNormally,
        );
      });

      test('should handle circular includes in structure display', () async {
        final fileA = File('${tempDir.path}/a.txt');
        await fileA.writeAsString('''
#include b.txt

○ task_a 1d "Task A" {}
''');

        final fileB = File('${tempDir.path}/b.txt');
        await fileB.writeAsString('''
#include a.txt

○ task_b 1d "Task B" {}
''');

        expect(
          () => FileRepository.showIncludeStructure(fileA.path, recursive: true),
          returnsNormally,
        );
      });
    });

    group('Path Handling', () {
      test('should handle absolute paths correctly', () async {
        final absolutePath = '${tempDir.path}/absolute.txt';
        final absoluteFile = File(absolutePath);
        await absoluteFile.writeAsString('○ abs_task 1d "Absolute task" {}');

        final mainFile = File('${tempDir.path}/main.txt');
        await mainFile.writeAsString('''
#include $absolutePath

○ main_task 1d "Main task" {}
''');

        final graph = FileRepository.loadGraphFromFile(mainFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['abs_task'], isNotNull);
        expect(graph.items['main_task'], isNotNull);
      });

      test('should handle parent directory references', () async {
        // Create subdirectory
        final subDir = Directory('${tempDir.path}/sub');
        await subDir.create();

        // Create file in parent directory
        final parentFile = File('${tempDir.path}/parent.txt');
        await parentFile.writeAsString('○ parent_task 1d "Parent task" {}');

        // Create file in subdirectory that references parent
        final subFile = File('${subDir.path}/sub.txt');
        await subFile.writeAsString('''
#include ../parent.txt

○ sub_task 1d "Sub task" {}
''');

        final graph = FileRepository.loadGraphFromFile(subFile.path);
        expect(graph.items.length, equals(2));
        expect(graph.items['parent_task'], isNotNull);
        expect(graph.items['sub_task'], isNotNull);
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/test/validation/graph_doctor_test.dart
// =====================================================================

import 'package:test/test.dart';
import '../../lib/src/validation/graph_doctor.dart';
import '../../lib/src/graph/giantt_graph.dart';
import '../../lib/src/models/giantt_item.dart';
import '../../lib/src/models/status.dart';
import '../../lib/src/models/priority.dart';
import '../../lib/src/models/duration.dart';

void main() {
  group('GraphDoctor Tests', () {
    late GianttGraph graph;
    late GraphDoctor doctor;

    setUp(() {
      graph = GianttGraph();
      doctor = GraphDoctor(graph);
    });

    group('Basic Health Checks', () {
      test('should find no issues in empty graph', () {
        final issues = doctor.fullDiagnosis();
        expect(issues, isEmpty);
      });

      test('should find no issues in healthy graph', () {
        // Create a simple healthy graph
        final item1 = GianttItem(
          id: 'task1',
          title: 'First Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item2 = GianttItem(
          id: 'task2',
          title: 'Second Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'REQUIRES': ['task1']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item3 = GianttItem(
          id: 'task1',
          title: 'First Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'BLOCKS': ['task2']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item3); // Updated item1 with BLOCKS relation
        graph.addItem(item2);

        final issues = doctor.fullDiagnosis();
        expect(issues, isEmpty);
      });
    });

    group('Dangling Reference Detection', () {
      test('should detect dangling REQUIRES reference', () {
        final item = GianttItem(
          id: 'task1',
          title: 'Task with bad dependency',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'REQUIRES': ['nonexistent']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item);
        final issues = doctor.fullDiagnosis();

        expect(issues, hasLength(1));
        expect(issues.first.type, equals(IssueType.danglingReference));
        expect(issues.first.itemId, equals('task1'));
        expect(issues.first.message, contains('nonexistent'));
        expect(issues.first.message, contains('requires'));
      });

      test('should detect dangling BLOCKS reference', () {
        final item = GianttItem(
          id: 'task1',
          title: 'Task with bad block',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'BLOCKS': ['missing_task']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item);
        final issues = doctor.fullDiagnosis();

        expect(issues, hasLength(1));
        expect(issues.first.type, equals(IssueType.danglingReference));
        expect(issues.first.relatedIds, contains('missing_task'));
      });

      test('should detect multiple dangling references', () {
        final item = GianttItem(
          id: 'task1',
          title: 'Task with multiple bad refs',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {
            'REQUIRES': ['missing1', 'missing2'],
            'BLOCKS': ['missing3']
          },
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item);
        final issues = doctor.fullDiagnosis();

        expect(issues, hasLength(3));
        expect(issues.every((issue) => issue.type == IssueType.danglingReference), isTrue);
      });
    });

    group('Incomplete Chain Detection', () {
      test('should detect item that blocks but is not required', () {
        final item1 = GianttItem(
          id: 'task1',
          title: 'Blocking Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'BLOCKS': ['task2']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item2 = GianttItem(
          id: 'task2',
          title: 'Blocked Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {}, // Missing REQUIRES relation
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item1);
        graph.addItem(item2);
        final issues = doctor.fullDiagnosis();

        expect(issues, hasLength(1));
        expect(issues.first.type, equals(IssueType.incompleteChain));
        expect(issues.first.itemId, equals('task1'));
        expect(issues.first.message, contains('blocks'));
        expect(issues.first.message, contains('task2'));
        expect(issues.first.message, contains("isn't required by it"));
      });

      test('should detect item that requires but is not blocked', () {
        final item1 = GianttItem(
          id: 'task1',
          title: 'Required Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {}, // Missing BLOCKS relation
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item2 = GianttItem(
          id: 'task2',
          title: 'Requiring Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'REQUIRES': ['task1']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item1);
        graph.addItem(item2);
        final issues = doctor.fullDiagnosis();

        expect(issues, hasLength(1));
        expect(issues.first.type, equals(IssueType.incompleteChain));
        expect(issues.first.itemId, equals('task2'));
        expect(issues.first.message, contains('requires'));
        expect(issues.first.message, contains('task1'));
        expect(issues.first.message, contains("isn't blocked by it"));
      });
    });

    group('Issue Fixing', () {
      test('should fix dangling reference by removing it', () {
        final item = GianttItem(
          id: 'task1',
          title: 'Task with bad dependency',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'REQUIRES': ['nonexistent', 'valid_task']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final validItem = GianttItem(
          id: 'valid_task',
          title: 'Valid Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'BLOCKS': ['task1']}, // Add the reciprocal relation to avoid incomplete chain
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item);
        graph.addItem(validItem);

        final issues = doctor.fullDiagnosis();
        expect(issues, hasLength(1)); // Should only find the dangling reference

        final fixedIssues = doctor.fixIssues();
        expect(fixedIssues, hasLength(1));

        // Check that the dangling reference was removed
        final updatedItem = graph.items['task1']!;
        expect(updatedItem.relations['REQUIRES'], equals(['valid_task']));
        expect(updatedItem.relations['REQUIRES'], isNot(contains('nonexistent')));

        // Verify no more issues
        final remainingIssues = doctor.fullDiagnosis();
        expect(remainingIssues, isEmpty);
      });

      test('should not fix issues that require manual intervention', () {
        // Create an incomplete chain that would require adding a relation
        final item1 = GianttItem(
          id: 'task1',
          title: 'Blocking Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'BLOCKS': ['task2']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item2 = GianttItem(
          id: 'task2',
          title: 'Blocked Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item1);
        graph.addItem(item2);

        final issues = doctor.fullDiagnosis();
        expect(issues, hasLength(1));

        // Try to fix the issue - incomplete chain issues can be auto-fixed
        final fixedIssues = doctor.fixIssues();
        expect(fixedIssues, hasLength(1));

        // Issue should be fixed
        final remainingIssues = doctor.fullDiagnosis();
        expect(remainingIssues, isEmpty);
      });
    });

    group('Quick Check', () {
      test('should return issue count without detailed analysis', () {
        final item = GianttItem(
          id: 'task1',
          title: 'Task with bad dependency',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'REQUIRES': ['nonexistent']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item);
        final issueCount = doctor.quickCheck();

        expect(issueCount, equals(1));
      });
    });

    group('Issue Type Filtering', () {
      test('should get issues by specific type', () {
        final item1 = GianttItem(
          id: 'task1',
          title: 'Task with dangling ref',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'REQUIRES': ['nonexistent']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item2 = GianttItem(
          id: 'task2',
          title: 'Blocking Task',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {'BLOCKS': ['task3']},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        final item3 = GianttItem(
          id: 'task3',
          title: 'Task without proper requires',
          status: GianttStatus.notStarted,
          priority: GianttPriority.neutral,
          duration: GianttDuration.zero(),
          charts: [],
          tags: [],
          relations: {},
          timeConstraint: null,
          userComment: null,
          autoComment: null,
          occlude: false,
        );

        graph.addItem(item1);
        graph.addItem(item2);
        graph.addItem(item3);

        doctor.fullDiagnosis();

        final danglingIssues = doctor.getIssuesByType(IssueType.danglingReference);
        final chainIssues = doctor.getIssuesByType(IssueType.incompleteChain);

        expect(danglingIssues, hasLength(1));
        expect(chainIssues, hasLength(1));
        expect(danglingIssues.first.itemId, equals('task1'));
        expect(chainIssues.first.itemId, equals('task2'));
      });
    });
  });
}


// =====================================================================
// FILE: packages/giantt_core/lib/giantt_core.dart
// =====================================================================

/// Core giantt logic for task dependency management
library giantt_core;

// Models
export 'src/models/giantt_item.dart';
export 'src/models/relation.dart';
export 'src/models/time_constraint.dart';
export 'src/models/duration.dart';
export 'src/models/status.dart';
export 'src/models/priority.dart';
export 'src/models/chart.dart';
export 'src/models/log_entry.dart';
export 'src/models/graph_exceptions.dart';

// Parser
export 'src/parser/giantt_parser.dart';
export 'src/parser/parse_exceptions.dart';

// Graph
export 'src/graph/giantt_graph.dart';
export 'src/graph/cycle_detector.dart';

// Storage
export 'src/storage/file_repository.dart';
export 'src/storage/atomic_file_writer.dart';
export 'src/storage/backup_manager.dart';
export 'src/storage/file_header_generator.dart';
export 'src/storage/path_resolver.dart';

// Validation
export 'src/validation/graph_doctor.dart';
export 'src/validation/issue_types.dart';

// Logging
export 'src/logging/log_repository.dart';
export 'src/logging/log_collection.dart';


// =====================================================================
// FILE: packages/giantt_core/lib/src/graph/cycle_detector.dart
// =====================================================================

// TODO: Implement Step 4 - Cycle Detection
// This will contain cycle detection with detailed error paths


// =====================================================================
// FILE: packages/giantt_core/lib/src/graph/giantt_graph.dart
// =====================================================================

import '../models/giantt_item.dart';
import '../models/relation.dart';
import '../models/graph_exceptions.dart';

/// Main graph class for managing Giantt items and their relations
class GianttGraph {
  final Map<String, GianttItem> _items = {};

  /// Get all items in the graph
  Map<String, GianttItem> get items => Map.unmodifiable(_items);

  /// Add an item to the graph
  void addItem(GianttItem item) {
    _items[item.id] = item;
  }

  /// Remove an item from the graph
  void removeItem(String itemId) {
    _items.remove(itemId);
  }

  /// Find an item by substring in ID or title
  GianttItem findBySubstring(String substring) {
    final matches = _items.values.where((item) => 
        item.id == substring || 
        item.title.toLowerCase().contains(substring.toLowerCase())
    ).toList();
    
    if (matches.isEmpty) {
      throw ArgumentError('No items with ID "$substring" or title containing "$substring" found');
    }
    if (matches.length > 1) {
      final ids = matches.map((item) => item.id).join(', ');
      throw ArgumentError('Multiple matches found: $ids');
    }
    return matches.first;
  }

  /// Add a relation between two items with automatic bidirectional creation
  void addRelation(String fromId, RelationType relationType, String toId) {
    if (!_items.containsKey(fromId)) {
      throw ArgumentError('Item "$fromId" not found');
    }
    if (!_items.containsKey(toId)) {
      throw ArgumentError('Item "$toId" not found');
    }

    final fromItem = _items[fromId]!;
    final toItem = _items[toId]!;

    // Add the primary relation
    final updatedFromRelations = Map<String, List<String>>.from(fromItem.relations);
    updatedFromRelations.putIfAbsent(relationType.name, () => []);
    if (!updatedFromRelations[relationType.name]!.contains(toId)) {
      updatedFromRelations[relationType.name]!.add(toId);
    }

    // Create the automatic bidirectional relation
    final bidirectionalType = _getBidirectionalRelation(relationType);
    final updatedToRelations = Map<String, List<String>>.from(toItem.relations);
    if (bidirectionalType != null) {
      updatedToRelations.putIfAbsent(bidirectionalType.name, () => []);
      if (!updatedToRelations[bidirectionalType.name]!.contains(fromId)) {
        updatedToRelations[bidirectionalType.name]!.add(fromId);
      }
    }

    // Update items with new relations
    _items[fromId] = fromItem.copyWith(relations: updatedFromRelations);
    _items[toId] = toItem.copyWith(relations: updatedToRelations);

    // Check for cycles after adding the relation
    _validateNoCycles();
  }

  /// Remove a relation between two items and its bidirectional counterpart
  void removeRelation(String fromId, RelationType relationType, String toId) {
    if (!_items.containsKey(fromId) || !_items.containsKey(toId)) {
      return; // Items don't exist, nothing to remove
    }

    final fromItem = _items[fromId]!;
    final toItem = _items[toId]!;

    // Remove the primary relation
    final updatedFromRelations = Map<String, List<String>>.from(fromItem.relations);
    if (updatedFromRelations.containsKey(relationType.name)) {
      updatedFromRelations[relationType.name]!.remove(toId);
      if (updatedFromRelations[relationType.name]!.isEmpty) {
        updatedFromRelations.remove(relationType.name);
      }
    }

    // Remove the automatic bidirectional relation
    final bidirectionalType = _getBidirectionalRelation(relationType);
    final updatedToRelations = Map<String, List<String>>.from(toItem.relations);
    if (bidirectionalType != null && updatedToRelations.containsKey(bidirectionalType.name)) {
      updatedToRelations[bidirectionalType.name]!.remove(fromId);
      if (updatedToRelations[bidirectionalType.name]!.isEmpty) {
        updatedToRelations.remove(bidirectionalType.name);
      }
    }

    // Update items with modified relations
    _items[fromId] = fromItem.copyWith(relations: updatedFromRelations);
    _items[toId] = toItem.copyWith(relations: updatedToRelations);
  }

  /// Get the bidirectional relation type for automatic creation
  RelationType? _getBidirectionalRelation(RelationType relationType) {
    switch (relationType) {
      case RelationType.requires:
        return RelationType.blocks;
      case RelationType.blocks:
        return RelationType.requires;
      case RelationType.anyof:
        return RelationType.sufficient;
      case RelationType.sufficient:
        return RelationType.anyof;
      case RelationType.supercharges:
      case RelationType.indicates:
      case RelationType.together:
      case RelationType.conflicts:
        return relationType; // These are symmetric relations
    }
  }

  /// Validate that the graph has no cycles in strict dependencies
  void _validateNoCycles() {
    try {
      _safeTopologicalSort();
    } catch (e) {
      // Re-throw cycle exceptions, convert others to GraphException
      if (e is CycleDetectedException) rethrow;
      throw GraphException('Graph validation failed: $e');
    }
  }

  /// Performs a safe topological sort that detects cycles and provides detailed error information
  List<GianttItem> _safeTopologicalSort() {
    // Build adjacency list for strict relations
    final adjList = <String, Set<String>>{};
    for (final item in _items.values) {
      adjList[item.id] = <String>{};
    }

    for (final item in _items.values) {
      for (final relType in ['REQUIRES', 'ANYOF']) {
        final targets = item.relations[relType] ?? [];
        for (final target in targets) {
          if (_items.containsKey(target)) {
            adjList[item.id]!.add(target);
          }
        }
      }
    }

    // Calculate in-degrees
    final inDegree = <String, int>{};
    for (final node in adjList.keys) {
      inDegree[node] = 0;
    }
    for (final node in adjList.keys) {
      for (final neighbor in adjList[node]!) {
        inDegree[neighbor] = (inDegree[neighbor] ?? 0) + 1;
      }
    }

    // Find nodes with no dependencies
    final queue = <String>[];
    for (final entry in inDegree.entries) {
      if (entry.value == 0) {
        queue.add(entry.key);
      }
    }

    final sortedItems = <GianttItem>[];
    final visited = <String>{};

    while (queue.isNotEmpty) {
      final node = queue.removeAt(0);
      final item = _items[node];
      if (item != null) {
        sortedItems.add(item);
        visited.add(node);
      }

      for (final neighbor in adjList[node]!) {
        inDegree[neighbor] = inDegree[neighbor]! - 1;
        if (inDegree[neighbor] == 0) {
          queue.add(neighbor);
        }
      }
    }

    // If we haven't visited all nodes, there must be a cycle
    if (sortedItems.length != _items.length) {
      final cycle = _findCycle(adjList, visited);
      throw CycleDetectedException(cycle);
    }

    return sortedItems;
  }

  /// Find a cycle in the graph for detailed error reporting
  List<String> _findCycle(Map<String, Set<String>> adjList, Set<String> visited) {
    final unvisited = _items.keys.toSet().difference(visited);
    final stack = <String>[];
    final path = <String>[];

    bool dfs(String current) {
      if (stack.contains(current)) {
        final cycleStart = stack.indexOf(current);
        final cycle = stack.sublist(cycleStart);
        cycle.add(current); // Add one more occurrence to show complete cycle
        return true;
      }
      if (visited.contains(current)) {
        return false;
      }

      stack.add(current);
      for (final neighbor in adjList[current] ?? <String>{}) {
        if (dfs(neighbor)) {
          return true;
        }
      }
      stack.remove(current);
      return false;
    }

    // Start DFS from any unvisited node
    if (unvisited.isNotEmpty) {
      final startNode = unvisited.first;
      dfs(startNode);
    }

    return stack.isNotEmpty ? stack : ['unknown_cycle'];
  }

  /// Performs a deterministic topological sort of the graph
  List<GianttItem> topologicalSort() {
    // First get basic topological sort
    final sortedItems = _safeTopologicalSort();

    // Sort within each "level" by deterministic criteria
    sortedItems.sort((a, b) {
      // Primary sort by dependency depth
      final depthA = _getDependencyDepth(a);
      final depthB = _getDependencyDepth(b);
      final depthComparison = depthA.compareTo(depthB);
      if (depthComparison != 0) return depthComparison;

      // Secondary sort by ID (deterministic tie-breaker)
      return a.id.compareTo(b.id);
    });

    return sortedItems;
  }

  /// Get the maximum dependency depth of an item
  int _getDependencyDepth(GianttItem item) {
    final visited = <String>{};
    
    int calculateDepth(String itemId) {
      if (visited.contains(itemId)) {
        return 0; // Avoid infinite recursion
      }
      visited.add(itemId);
      
      final currentItem = _items[itemId];
      if (currentItem == null) return 0;
      
      // Consider both REQUIRES and ANYOF relations for depth calculation
      final requires = currentItem.relations['REQUIRES'] ?? [];
      final anyof = currentItem.relations['ANYOF'] ?? [];
      final allDeps = [...requires, ...anyof];
      
      if (allDeps.isEmpty) return 0;
      
      int maxDepth = 0;
      for (final depId in allDeps) {
        if (_items.containsKey(depId)) {
          final depDepth = calculateDepth(depId);
          maxDepth = maxDepth > depDepth ? maxDepth : depDepth;
        }
      }
      return maxDepth + 1;
    }
    
    return calculateDepth(item.id);
  }

  /// Insert a new item between two existing items in the dependency chain
  void insertBetween(GianttItem newItem, String beforeId, String afterId) {
    if (!_items.containsKey(beforeId) || !_items.containsKey(afterId)) {
      throw ArgumentError('Both before and after items must exist');
    }

    final beforeItem = _items[beforeId]!;
    final afterItem = _items[afterId]!;

    // Create new item with appropriate relations
    final updatedNewItem = newItem.copyWith(
      relations: {
        'REQUIRES': [beforeId],
        'BLOCKS': [afterId],
      },
    );

    // Update existing items' relations
    final updatedBeforeRelations = Map<String, List<String>>.from(beforeItem.relations);
    if (updatedBeforeRelations.containsKey('BLOCKS')) {
      updatedBeforeRelations['BLOCKS']!.remove(afterId);
      updatedBeforeRelations['BLOCKS']!.add(newItem.id);
    } else {
      updatedBeforeRelations['BLOCKS'] = [newItem.id];
    }

    final updatedAfterRelations = Map<String, List<String>>.from(afterItem.relations);
    if (updatedAfterRelations.containsKey('REQUIRES')) {
      updatedAfterRelations['REQUIRES']!.remove(beforeId);
      updatedAfterRelations['REQUIRES']!.add(newItem.id);
    } else {
      updatedAfterRelations['REQUIRES'] = [newItem.id];
    }

    // Update items in graph
    _items[beforeId] = beforeItem.copyWith(relations: updatedBeforeRelations);
    _items[afterId] = afterItem.copyWith(relations: updatedAfterRelations);
    _items[newItem.id] = updatedNewItem;

    // Validate no cycles were created
    _validateNoCycles();
  }

  /// Get items that are not occluded
  Map<String, GianttItem> get includedItems {
    return Map.fromEntries(
      _items.entries.where((entry) => !entry.value.occlude)
    );
  }

  /// Get items that are occluded
  Map<String, GianttItem> get occludedItems {
    return Map.fromEntries(
      _items.entries.where((entry) => entry.value.occlude)
    );
  }

  /// Occlude an item by ID
  void occludeItem(String itemId) {
    final item = _items[itemId];
    if (item != null) {
      _items[itemId] = item.setOcclude(true);
    }
  }

  /// Include (un-occlude) an item by ID
  void includeItem(String itemId) {
    final item = _items[itemId];
    if (item != null) {
      _items[itemId] = item.setOcclude(false);
    }
  }

  /// Occlude items by tag
  void occludeItemsByTag(String tag) {
    for (final entry in _items.entries) {
      if (entry.value.tags.contains(tag) && !entry.value.occlude) {
        _items[entry.key] = entry.value.setOcclude(true);
      }
    }
  }

  /// Include items by tag
  void includeItemsByTag(String tag) {
    for (final entry in _items.entries) {
      if (entry.value.tags.contains(tag) && entry.value.occlude) {
        _items[entry.key] = entry.value.setOcclude(false);
      }
    }
  }

  /// Create a copy of this graph
  GianttGraph copy() {
    final newGraph = GianttGraph();
    for (final item in _items.values) {
      newGraph.addItem(item.copyWith());
    }
    return newGraph;
  }

  /// Merge another graph into this one
  GianttGraph operator +(GianttGraph other) {
    final newGraph = copy();
    for (final item in other._items.values) {
      newGraph.addItem(item.copyWith());
    }
    return newGraph;
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/duration.dart
// =====================================================================

import 'package:meta/meta.dart';

/// A single part of a duration with an amount and unit
@immutable
class DurationPart {
  const DurationPart({
    required this.amount,
    required this.unit,
  });

  /// The numeric amount for this duration part
  final double amount;
  
  /// The unit (s, min, h, d, w, mo, y)
  final String unit;

  /// Unit conversion to seconds
  static const Map<String, int> _unitSeconds = {
    's': 1,
    'min': 60,
    'h': 3600,
    'hr': 3600,
    'd': 86400,
    'w': 604800,
    'mo': 2592000, // 30 days
    'y': 31536000, // 365 days
  };

  /// Unit normalization mapping
  static const Map<String, String> _unitNormalize = {
    'hr': 'h',
    'minute': 'min',
    'minutes': 'min',
    'hour': 'h',
    'hours': 'h',
    'day': 'd',
    'days': 'd',
    'week': 'w',
    'weeks': 'w',
    'month': 'mo',
    'months': 'mo',
    'year': 'y',
    'years': 'y',
  };

  /// Factory method to create a normalized DurationPart
  factory DurationPart.create(double amount, String unit) {
    final normalizedUnit = _unitNormalize[unit] ?? unit;
    
    if (!_unitSeconds.containsKey(normalizedUnit)) {
      throw ArgumentError('Invalid duration unit: $unit');
    }
    
    return DurationPart(amount: amount, unit: normalizedUnit);
  }

  /// Parse a duration part from a string like "5d" or "3.5h"
  static DurationPart parse(String durationStr) {
    final match = RegExp(r'^(\d+\.?\d*)([a-zA-Z]+)$').firstMatch(durationStr);
    if (match == null) {
      throw ArgumentError('Invalid duration part format: $durationStr');
    }
    
    final amount = double.parse(match.group(1)!);
    final unit = match.group(2)!;
    
    return DurationPart.create(amount, unit);
  }

  /// Get total seconds for this duration part
  double get totalSeconds => amount * (_unitSeconds[unit] ?? 0);

  @override
  String toString() {
    // For whole numbers, display as integers
    final amountStr = amount == amount.toInt() 
        ? amount.toInt().toString() 
        : amount.toString();
    return '$amountStr$unit';
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! DurationPart) return false;
    return amount == other.amount && unit == other.unit;
  }

  @override
  int get hashCode => Object.hash(amount, unit);
}

/// Handles compound durations like '6mo8d3.5s'
@immutable
class GianttDuration {
  const GianttDuration({this.parts = const []});

  /// The individual duration parts that make up this duration
  final List<DurationPart> parts;

  /// Parse a duration string into a GianttDuration object
  static GianttDuration parse(String durationStr) {
    if (durationStr.isEmpty) {
      throw ArgumentError('Empty duration string');
    }

    final pattern = RegExp(r'(\d+\.?\d*)([a-zA-Z]+)');
    final matches = pattern.allMatches(durationStr);
    final parts = <DurationPart>[];

    for (final match in matches) {
      final amount = double.parse(match.group(1)!);
      final unit = match.group(2)!;
      parts.add(DurationPart.create(amount, unit));
    }

    if (parts.isEmpty) {
      throw ArgumentError('No valid duration parts found in: $durationStr');
    }

    return GianttDuration(parts: parts);
  }

  /// Get total duration in seconds
  double get totalSeconds => parts.fold(0.0, (sum, part) => sum + part.totalSeconds);

  /// Create a duration with a single part
  factory GianttDuration.single(double amount, String unit) {
    return GianttDuration(parts: [DurationPart.create(amount, unit)]);
  }

  /// Create an empty duration (0 seconds)
  factory GianttDuration.zero() {
    return const GianttDuration(parts: []);
  }

  @override
  String toString() {
    if (parts.isEmpty) return '0s';
    return parts.map((part) => part.toString()).join('');
  }

  /// Add two durations
  GianttDuration operator +(GianttDuration other) {
    final totalSecs = totalSeconds + other.totalSeconds;
    
    // Convert back to largest sensible unit
    const unitOrder = ['y', 'mo', 'w', 'd', 'h', 'min', 's'];
    for (final unit in unitOrder) {
      final seconds = DurationPart._unitSeconds[unit]!;
      if (totalSecs >= seconds) {
        final amount = totalSecs / seconds;
        final finalAmount = amount == amount.toInt() ? amount.toInt().toDouble() : amount;
        return GianttDuration(parts: [DurationPart(amount: finalAmount, unit: unit)]);
      }
    }
    
    return GianttDuration(parts: [DurationPart(amount: totalSecs, unit: 's')]);
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! GianttDuration) return false;
    return totalSeconds == other.totalSeconds;
  }

  bool operator <(GianttDuration other) => totalSeconds < other.totalSeconds;
  bool operator >(GianttDuration other) => totalSeconds > other.totalSeconds;
  bool operator <=(GianttDuration other) => totalSeconds <= other.totalSeconds;
  bool operator >=(GianttDuration other) => totalSeconds >= other.totalSeconds;

  @override
  int get hashCode => Object.hashAll(parts);

  /// Create a copy with modified parts
  GianttDuration copyWith({List<DurationPart>? parts}) {
    return GianttDuration(parts: parts ?? this.parts);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/giantt_item.dart
// =====================================================================

import 'dart:convert';
import 'package:meta/meta.dart';
import 'status.dart';
import 'priority.dart';
import 'duration.dart';
import 'relation.dart';
import 'time_constraint.dart';

/// Represents a single Giantt item with all its properties
@immutable
class GianttItem {
  const GianttItem({
    required this.id,
    required this.title,
    this.description = '',
    this.status = GianttStatus.notStarted,
    this.priority = GianttPriority.neutral,
    required this.duration,
    this.charts = const [],
    this.tags = const [],
    this.relations = const {},
    this.timeConstraints = const [],
    this.userComment,
    this.autoComment,
    this.occlude = false,
  });

  /// Unique identifier for this item
  final String id;
  
  /// Display title
  final String title;
  
  /// Optional description
  final String description;
  
  /// Current status
  final GianttStatus status;
  
  /// Priority level
  final GianttPriority priority;
  
  /// Estimated duration
  final GianttDuration duration;
  
  /// Charts this item belongs to
  final List<String> charts;
  
  /// Tags for categorization
  final List<String> tags;
  
  /// Relations to other items (relation type -> list of target IDs)
  final Map<String, List<String>> relations;
  
  /// Time constraints for this item
  final List<TimeConstraint> timeConstraints;
  
  /// User-added comment
  final String? userComment;
  
  /// Auto-generated comment
  final String? autoComment;
  
  /// Whether this item is occluded (archived)
  final bool occlude;

  /// Set the occlude status of this item
  GianttItem setOcclude(bool occlude) {
    return copyWith(occlude: occlude);
  }

  /// Create a copy with modified properties
  GianttItem copyWith({
    String? id,
    String? title,
    String? description,
    GianttStatus? status,
    GianttPriority? priority,
    GianttDuration? duration,
    List<String>? charts,
    List<String>? tags,
    Map<String, List<String>>? relations,
    List<TimeConstraint>? timeConstraints,
    String? userComment,
    String? autoComment,
    bool? occlude,
  }) {
    return GianttItem(
      id: id ?? this.id,
      title: title ?? this.title,
      description: description ?? this.description,
      status: status ?? this.status,
      priority: priority ?? this.priority,
      duration: duration ?? this.duration,
      charts: charts ?? this.charts,
      tags: tags ?? this.tags,
      relations: relations ?? this.relations,
      timeConstraints: timeConstraints ?? this.timeConstraints,
      userComment: userComment ?? this.userComment,
      autoComment: autoComment ?? this.autoComment,
      occlude: occlude ?? this.occlude,
    );
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! GianttItem) return false;
    return id == other.id &&
           title == other.title &&
           description == other.description &&
           status == other.status &&
           priority == other.priority &&
           duration == other.duration &&
           charts.length == other.charts.length &&
           charts.every((chart) => other.charts.contains(chart)) &&
           tags.length == other.tags.length &&
           tags.every((tag) => other.tags.contains(tag)) &&
           relations.length == other.relations.length &&
           timeConstraints.length == other.timeConstraints.length &&
           timeConstraints.every((tc) => other.timeConstraints.contains(tc)) &&
           userComment == other.userComment &&
           autoComment == other.autoComment &&
           occlude == other.occlude;
  }

  @override
  int get hashCode => Object.hash(
    id, title, description, status, priority, duration,
    Object.hashAll(charts), Object.hashAll(tags),
    Object.hashAll(relations.entries), Object.hashAll(timeConstraints),
    userComment, autoComment, occlude
  );

  @override
  String toString() => 'GianttItem(id: $id, title: $title, status: $status)';

  /// Convert this item to its string representation for file storage
  String toFileString() {
    final buffer = StringBuffer();
    
    // Status, ID+Priority, Duration
    buffer.write('${status.symbol} $id${priority.symbol} $duration ');
    
    // JSON-encoded title
    buffer.write(jsonEncode(title));
    
    // Charts
    buffer.write(' {');
    if (charts.isNotEmpty) {
      buffer.write('"${charts.join('","')}"');
    } else {
      buffer.write('""');
    }
    buffer.write('}');
    
    // Tags
    if (tags.isNotEmpty) {
      buffer.write(' ${tags.join(',')}');
    }
    
    // Relations
    if (relations.isNotEmpty) {
      buffer.write(' >>> ');
      final relationParts = <String>[];
      
      for (final entry in relations.entries) {
        final relationType = entry.key;
        final targets = entry.value;
        if (targets.isNotEmpty) {
          // Find the symbol for this relation type
          final symbol = _getRelationSymbol(relationType);
          relationParts.add('$symbol[${targets.join(',')}]');
        }
      }
      buffer.write(relationParts.join(' '));
    }
    
    // Time constraints
    if (timeConstraints.isNotEmpty) {
      buffer.write(' @@@ ');
      buffer.write(timeConstraints.map((tc) => tc.toString()).join(' '));
    }
    
    // Comments
    if (userComment != null && userComment!.isNotEmpty) {
      buffer.write(' # $userComment');
    }
    if (autoComment != null && autoComment!.isNotEmpty) {
      buffer.write(' ### $autoComment');
    }
    
    return buffer.toString();
  }

  /// Get the symbol for a relation type name
  static String _getRelationSymbol(String relationType) {
    const typeToSymbol = {
      'REQUIRES': '⊢',
      'ANYOF': '⋲',
      'SUPERCHARGES': '≫',
      'INDICATES': '∴',
      'TOGETHER': '∪',
      'CONFLICTS': '⊟',
      'BLOCKS': '►',
      'SUFFICIENT': '≻',
    };
    
    return typeToSymbol[relationType] ?? '?';
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/status.dart
// =====================================================================

import 'package:meta/meta.dart';

/// Status of a Giantt item with corresponding symbols
enum GianttStatus {
  notStarted('○', 'NOT_STARTED'),
  inProgress('◑', 'IN_PROGRESS'), 
  blocked('⊘', 'BLOCKED'),
  completed('●', 'COMPLETED');

  const GianttStatus(this.symbol, this.name);

  /// The Unicode symbol representing this status
  final String symbol;
  
  /// The string name for this status
  final String name;

  /// Create a GianttStatus from its symbol
  static GianttStatus fromSymbol(String symbol) {
    for (final status in GianttStatus.values) {
      if (status.symbol == symbol) {
        return status;
      }
    }
    throw ArgumentError('Invalid status symbol: $symbol');
  }

  /// Create a GianttStatus from its name
  static GianttStatus fromName(String name) {
    for (final status in GianttStatus.values) {
      if (status.name == name) {
        return status;
      }
    }
    throw ArgumentError('Invalid status name: $name');
  }

  @override
  String toString() => symbol;
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/log_entry.dart
// =====================================================================

import 'dart:convert';
import 'package:meta/meta.dart';

/// Represents a single log entry
@immutable
class LogEntry {
  const LogEntry({
    required this.session,
    required this.timestamp,
    required this.message,
    this.tags = const {},
    this.metadata = const {},
    this.occlude = false,
  });

  /// Session identifier
  final String session;
  
  /// When this entry was created
  final DateTime timestamp;
  
  /// Log message content
  final String message;
  
  /// Associated tags
  final Set<String> tags;
  
  /// Additional metadata
  final Map<String, String> metadata;
  
  /// Whether this entry is occluded
  final bool occlude;

  /// Set the occlude status of this entry
  LogEntry setOcclude(bool occlude) {
    return copyWith(occlude: occlude);
  }

  /// Create a LogEntry from a JSON line
  static LogEntry fromJsonLine(String jsonLine, {bool occlude = false}) {
    try {
      final data = json.decode(jsonLine) as Map<String, dynamic>;
      return LogEntry(
        session: data['s'] as String,
        timestamp: DateTime.parse(data['t'] as String),
        message: data['m'] as String,
        tags: Set<String>.from(data['tags'] as List),
        metadata: Map<String, String>.from(data['meta'] as Map? ?? {}),
        occlude: occlude,
      );
    } catch (e) {
      throw FormatException('Invalid log entry format: $e');
    }
  }

  /// Convert to JSON line format
  String toJsonLine() {
    final data = {
      's': session,
      't': timestamp.toIso8601String(),
      'm': message,
      'tags': tags.toList()..sort(),
      'meta': metadata,
    };
    return json.encode(data);
  }

  /// Create a copy with modified properties
  LogEntry copyWith({
    String? session,
    DateTime? timestamp,
    String? message,
    Set<String>? tags,
    Map<String, String>? metadata,
    bool? occlude,
  }) {
    return LogEntry(
      session: session ?? this.session,
      timestamp: timestamp ?? this.timestamp,
      message: message ?? this.message,
      tags: tags ?? this.tags,
      metadata: metadata ?? this.metadata,
      occlude: occlude ?? this.occlude,
    );
  }

  /// Factory constructor to create a new log entry with current timestamp
  factory LogEntry.create(
    String sessionTag,
    String message, {
    List<String>? additionalTags,
    Map<String, String>? metadata,
    bool occlude = false,
  }) {
    final tags = <String>{sessionTag};
    if (additionalTags != null) {
      tags.addAll(additionalTags);
    }

    return LogEntry(
      session: sessionTag,
      timestamp: DateTime.now().toUtc(),
      message: message,
      tags: tags,
      metadata: metadata ?? {},
      occlude: occlude,
    );
  }

  /// Check if entry has a specific tag
  bool hasTag(String tag) => tags.contains(tag);

  /// Check if entry has any of the specified tags
  bool hasAnyTags(List<String> tagList) => tags.any(tagList.contains);

  /// Check if entry has all of the specified tags
  bool hasAllTags(List<String> tagList) => tagList.every(tags.contains);

  /// Add a tag to the entry
  LogEntry addTag(String tag) {
    final newTags = Set<String>.from(tags)..add(tag);
    return copyWith(tags: newTags);
  }

  /// Remove a tag from the entry
  LogEntry removeTag(String tag) {
    final newTags = Set<String>.from(tags)..remove(tag);
    return copyWith(tags: newTags);
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! LogEntry) return false;
    return session == other.session &&
           timestamp == other.timestamp &&
           message == other.message &&
           tags.length == other.tags.length &&
           tags.every((tag) => other.tags.contains(tag)) &&
           metadata.length == other.metadata.length &&
           occlude == other.occlude;
  }

  @override
  int get hashCode => Object.hash(
    session, timestamp, message, 
    Object.hashAll(tags), Object.hashAll(metadata.entries), occlude
  );

  @override
  String toString() => 'LogEntry(session: $session, message: $message)';
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/graph_exceptions.dart
// =====================================================================

/// Exception thrown when a cycle is detected in the dependency graph
class CycleDetectedException implements Exception {
  const CycleDetectedException(this.cycleItems);

  /// The items that form the cycle
  final List<String> cycleItems;

  @override
  String toString() {
    final cycleStr = cycleItems.join(' -> ');
    return 'Cycle detected in dependencies: $cycleStr';
  }
}

/// Exception thrown when parsing fails
class GianttParseException implements Exception {
  const GianttParseException(this.message, [this.input]);

  /// Error message
  final String message;
  
  /// The input that caused the error
  final String? input;

  @override
  String toString() {
    if (input != null) {
      return 'Parse error: $message\nInput: $input';
    }
    return 'Parse error: $message';
  }
}

/// Exception thrown when graph operations fail
class GraphException implements Exception {
  const GraphException(this.message);

  /// Error message
  final String message;

  @override
  String toString() => 'Graph error: $message';
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/time_constraint.dart
// =====================================================================

import 'package:meta/meta.dart';
import 'duration.dart';

/// Type of time constraint
enum TimeConstraintType {
  window('window'),
  deadline('deadline'),
  recurring('recurring');

  const TimeConstraintType(this.value);
  final String value;
}

/// Type of consequence for time constraint violations
enum ConsequenceType {
  severe('severe'),
  warning('warn'),
  escalating('escalating');

  const ConsequenceType(this.value);
  final String value;

  static ConsequenceType fromString(String value) {
    for (final type in ConsequenceType.values) {
      if (type.value == value) return type;
    }
    throw ArgumentError('Invalid consequence type: $value');
  }
}

/// Escalation rate for escalating consequences
enum EscalationRate {
  lowest(',,,'),
  low('...'),
  neutral(''),
  unsure('?'),
  medium('!'),
  high('!!'),
  critical('!!!');

  const EscalationRate(this.symbol);
  final String symbol;

  static EscalationRate fromString(String symbol) {
    for (final rate in EscalationRate.values) {
      if (rate.symbol == symbol) return rate;
    }
    throw ArgumentError('Invalid escalation rate: $symbol');
  }
}

/// Represents a time constraint on a Giantt item
@immutable
class TimeConstraint {
  const TimeConstraint({
    required this.type,
    required this.duration,
    this.gracePeriod,
    this.consequenceType = ConsequenceType.warning,
    this.escalationRate = EscalationRate.neutral,
    this.dueDate,
    this.interval,
    this.stack = false,
  });

  /// The type of time constraint
  final TimeConstraintType type;
  
  /// The main duration for the constraint
  final GianttDuration duration;
  
  /// Optional grace period
  final GianttDuration? gracePeriod;
  
  /// Type of consequence for violations
  final ConsequenceType consequenceType;
  
  /// Escalation rate for escalating consequences
  final EscalationRate escalationRate;
  
  /// Due date for deadline constraints (YYYY-MM-DD format)
  final String? dueDate;
  
  /// Interval for recurring constraints
  final GianttDuration? interval;
  
  /// Whether recurring constraints should stack
  final bool stack;

  /// Parse a time constraint from a string
  static TimeConstraint? fromString(String? constraintStr) {
    if (constraintStr == null || constraintStr.isEmpty) {
      return null;
    }

    // Parse window constraints: window(5d:2d,severe)
    final windowMatch = RegExp(r'window\((\d+[smhdwy]+)(?::(\d+[smhdwy]+))?,([^)]+)\)').firstMatch(constraintStr);
    if (windowMatch != null) {
      final window = GianttDuration.parse(windowMatch.group(1)!);
      final grace = windowMatch.group(2) != null ? GianttDuration.parse(windowMatch.group(2)!) : null;
      final consequence = _parseConsequence(windowMatch.group(3)!);

      return TimeConstraint(
        type: TimeConstraintType.window,
        duration: window,
        gracePeriod: grace,
        consequenceType: consequence['type']!,
        escalationRate: consequence['rate']!,
      );
    }

    // Parse deadline constraints: due(2024-12-31:2d,severe)
    final deadlineMatch = RegExp(r'due\((\d{4}-\d{2}-\d{2})(?::(\d+[smhdwy]+))?,([^)]+)\)').firstMatch(constraintStr);
    if (deadlineMatch != null) {
      final dueDate = deadlineMatch.group(1)!;
      final grace = deadlineMatch.group(2) != null ? GianttDuration.parse(deadlineMatch.group(2)!) : null;
      final consequence = _parseConsequence(deadlineMatch.group(3)!);

      return TimeConstraint(
        type: TimeConstraintType.deadline,
        duration: GianttDuration.parse('1d'), // Default to 1 day for deadline
        gracePeriod: grace,
        consequenceType: consequence['type']!,
        escalationRate: consequence['rate']!,
        dueDate: dueDate,
      );
    }

    // Parse recurring constraints: every(7d:1d,warn,stack)
    final recurringMatch = RegExp(r'every\((\d+[smhdwy]+)(?::(\d+[smhdwy]+))?,([^)]+)\)').firstMatch(constraintStr);
    if (recurringMatch != null) {
      final interval = GianttDuration.parse(recurringMatch.group(1)!);
      final grace = recurringMatch.group(2) != null ? GianttDuration.parse(recurringMatch.group(2)!) : null;
      final consequenceStr = recurringMatch.group(3)!;

      final stack = consequenceStr.contains('stack');
      final cleanConsequenceStr = consequenceStr.replaceAll(',stack', '').replaceAll('stack,', '').replaceAll('stack', '');
      final consequence = _parseConsequence(cleanConsequenceStr);

      return TimeConstraint(
        type: TimeConstraintType.recurring,
        duration: interval,
        gracePeriod: grace,
        consequenceType: consequence['type']!,
        escalationRate: consequence['rate']!,
        interval: interval,
        stack: stack,
      );
    }

    throw ArgumentError('Invalid time constraint format: $constraintStr');
  }

  /// Parse consequence information from a string
  static Map<String, dynamic> _parseConsequence(String consequenceStr) {
    final parts = consequenceStr.split(',').map((s) => s.trim()).toList();
    final baseConsequence = parts[0];

    for (final part in parts) {
      if (part.startsWith('escalate:')) {
        final rateStr = part.substring(9); // Remove 'escalate:'
        return {
          'type': ConsequenceType.escalating,
          'rate': rateStr.isNotEmpty ? EscalationRate.fromString(rateStr) : EscalationRate.neutral,
        };
      }
    }

    return {
      'type': ConsequenceType.fromString(baseConsequence),
      'rate': EscalationRate.neutral,
    };
  }

  @override
  String toString() {
    final baseStr = switch (type) {
      TimeConstraintType.window => 'window($duration',
      TimeConstraintType.deadline => 'due($dueDate',
      TimeConstraintType.recurring => 'every($interval',
    };

    final buffer = StringBuffer(baseStr);
    
    if (gracePeriod != null) {
      buffer.write(':$gracePeriod');
    }
    
    buffer.write(',${consequenceType.value}');
    
    if (escalationRate != EscalationRate.neutral) {
      buffer.write(',escalate:${escalationRate.symbol}');
    }
    
    if (type == TimeConstraintType.recurring && stack) {
      buffer.write(',stack');
    }
    
    buffer.write(')');
    return buffer.toString();
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! TimeConstraint) return false;
    return type == other.type &&
           duration == other.duration &&
           gracePeriod == other.gracePeriod &&
           consequenceType == other.consequenceType &&
           escalationRate == other.escalationRate &&
           dueDate == other.dueDate &&
           interval == other.interval &&
           stack == other.stack;
  }

  @override
  int get hashCode => Object.hash(
    type, duration, gracePeriod, consequenceType, 
    escalationRate, dueDate, interval, stack
  );

  /// Create a copy with modified properties
  TimeConstraint copyWith({
    TimeConstraintType? type,
    GianttDuration? duration,
    GianttDuration? gracePeriod,
    ConsequenceType? consequenceType,
    EscalationRate? escalationRate,
    String? dueDate,
    GianttDuration? interval,
    bool? stack,
  }) {
    return TimeConstraint(
      type: type ?? this.type,
      duration: duration ?? this.duration,
      gracePeriod: gracePeriod ?? this.gracePeriod,
      consequenceType: consequenceType ?? this.consequenceType,
      escalationRate: escalationRate ?? this.escalationRate,
      dueDate: dueDate ?? this.dueDate,
      interval: interval ?? this.interval,
      stack: stack ?? this.stack,
    );
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/chart.dart
// =====================================================================

import 'package:meta/meta.dart';

/// Represents chart metadata and view settings
@immutable
class Chart {
  const Chart({
    required this.name,
    this.description = '',
    this.color,
    this.isVisible = true,
  });

  /// Chart name
  final String name;
  
  /// Optional description
  final String description;
  
  /// Optional color for visualization
  final String? color;
  
  /// Whether this chart is currently visible
  final bool isVisible;

  /// Create a copy with modified properties
  Chart copyWith({
    String? name,
    String? description,
    String? color,
    bool? isVisible,
  }) {
    return Chart(
      name: name ?? this.name,
      description: description ?? this.description,
      color: color ?? this.color,
      isVisible: isVisible ?? this.isVisible,
    );
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! Chart) return false;
    return name == other.name &&
           description == other.description &&
           color == other.color &&
           isVisible == other.isVisible;
  }

  @override
  int get hashCode => Object.hash(name, description, color, isVisible);

  @override
  String toString() => 'Chart(name: $name, visible: $isVisible)';
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/priority.dart
// =====================================================================

import 'package:meta/meta.dart';

/// Priority level of a Giantt item with corresponding symbols
enum GianttPriority {
  lowest(',,,', 'LOWEST'),
  low('...', 'LOW'),
  neutral('', 'NEUTRAL'),
  unsure('?', 'UNSURE'),
  medium('!', 'MEDIUM'),
  high('!!', 'HIGH'),
  critical('!!!', 'CRITICAL');

  const GianttPriority(this.symbol, this.name);

  /// The symbol representing this priority level
  final String symbol;
  
  /// The string name for this priority
  final String name;

  /// Create a GianttPriority from its symbol
  static GianttPriority fromSymbol(String symbol) {
    for (final priority in GianttPriority.values) {
      if (priority.symbol == symbol) {
        return priority;
      }
    }
    throw ArgumentError('Invalid priority symbol: $symbol');
  }

  /// Create a GianttPriority from its name
  static GianttPriority fromName(String name) {
    for (final priority in GianttPriority.values) {
      if (priority.name == name) {
        return priority;
      }
    }
    throw ArgumentError('Invalid priority name: $name');
  }

  @override
  String toString() => symbol;
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/models/relation.dart
// =====================================================================

import 'package:meta/meta.dart';

/// Type of relation between Giantt items with corresponding symbols
enum RelationType {
  requires('⊢', 'REQUIRES'),
  anyof('⋲', 'ANYOF'),
  supercharges('≫', 'SUPERCHARGES'),
  indicates('∴', 'INDICATES'),
  together('∪', 'TOGETHER'),
  conflicts('⊟', 'CONFLICTS'),
  blocks('►', 'BLOCKS'),
  sufficient('≻', 'SUFFICIENT');

  const RelationType(this.symbol, this.name);

  /// The Unicode symbol representing this relation type
  final String symbol;
  
  /// The string name for this relation type
  final String name;

  /// Create a RelationType from its symbol
  static RelationType fromSymbol(String symbol) {
    for (final relationType in RelationType.values) {
      if (relationType.symbol == symbol) {
        return relationType;
      }
    }
    throw ArgumentError('Invalid relation symbol: $symbol');
  }

  /// Create a RelationType from its name
  static RelationType fromName(String name) {
    for (final relationType in RelationType.values) {
      if (relationType.name == name) {
        return relationType;
      }
    }
    throw ArgumentError('Invalid relation name: $name');
  }

  @override
  String toString() => symbol;
}

/// Represents a relation between two Giantt items
@immutable
class Relation {
  const Relation({
    required this.type,
    required this.targetIds,
  });

  /// The type of relation
  final RelationType type;
  
  /// The IDs of the target items
  final List<String> targetIds;

  /// Create a copy with modified properties
  Relation copyWith({
    RelationType? type,
    List<String>? targetIds,
  }) {
    return Relation(
      type: type ?? this.type,
      targetIds: targetIds ?? this.targetIds,
    );
  }

  @override
  bool operator ==(Object other) {
    if (identical(this, other)) return true;
    if (other is! Relation) return false;
    return type == other.type && 
           targetIds.length == other.targetIds.length &&
           targetIds.every((id) => other.targetIds.contains(id));
  }

  @override
  int get hashCode => Object.hash(type, Object.hashAll(targetIds));

  @override
  String toString() => '${type.symbol}[${targetIds.join(',')}]';
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/parser/giantt_parser.dart
// =====================================================================

import 'dart:convert';
import '../models/giantt_item.dart';
import '../models/status.dart';
import '../models/priority.dart';
import '../models/duration.dart';
import '../models/relation.dart';
import '../models/time_constraint.dart';
import '../models/graph_exceptions.dart';

/// Main parser for Giantt file format
class GianttParser {
  /// Parse a line into a GianttItem
  static GianttItem fromString(String line, {bool occlude = false}) {
    line = line.strip();
    
    if (line.isEmpty || line.startsWith('#')) {
      throw GianttParseException('Empty or comment line', line);
    }

    try {
      // Parse the pre-title section
      final titleStart = line.indexOf('"');
      if (titleStart == -1) {
        throw GianttParseException('No opening quote found for title', line);
      }
      
      final preTitle = line.substring(0, titleStart).trim();
      final (status, idPriority, duration) = _parsePreTitleSection(preTitle);
      
      // Parse the title (JSON-escaped)
      final titleEnd = _findClosingQuote(line, titleStart);
      if (titleEnd == -1) {
        throw GianttParseException('No closing quote found for title', line);
      }
      
      final titleJson = line.substring(titleStart, titleEnd + 1);
      final title = jsonDecode(titleJson) as String;
      final postTitle = line.substring(titleEnd + 1).trim();
      
      // Extract ID and priority from id+priority string
      final (id, priority) = _parseIdPriority(idPriority);
      
      // Parse duration
      final parsedDuration = _parseDurationSafely(duration);
      
      // Parse post-title section
      final (charts, tags, relations, timeConstraints, userComment, autoComment) = 
          _parsePostTitleSection(postTitle);
      
      return GianttItem(
        id: id,
        title: title,
        status: status,
        priority: priority,
        duration: parsedDuration,
        charts: charts,
        tags: tags,
        relations: relations,
        timeConstraints: timeConstraints,
        userComment: userComment,
        autoComment: autoComment,
        occlude: occlude,
      );
    } catch (e) {
      if (e is GianttParseException) rethrow;
      throw GianttParseException('Parse error: $e', line);
    }
  }

  /// Convert a GianttItem to its string representation
  static String itemToString(GianttItem item) {
    return item.toFileString();
  }

  /// Parse the pre-title section into status, id+priority, and duration
  static (GianttStatus, String, String) _parsePreTitleSection(String preTitle) {
    // Pattern: status id+priority duration
    final pattern = RegExp(r'^([○◑⊘●])\s+([^\s]+)\s+([^\s"]+)');
    final match = pattern.firstMatch(preTitle);
    
    if (match == null) {
      throw GianttParseException('Invalid pre-title format', preTitle);
    }
    
    final statusSymbol = match.group(1)!;
    final idPriority = match.group(2)!;
    final duration = match.group(3)!.trim();
    
    try {
      final status = GianttStatus.fromSymbol(statusSymbol);
      return (status, idPriority, duration);
    } catch (e) {
      throw GianttParseException('Invalid status symbol: $statusSymbol', preTitle);
    }
  }

  /// Parse ID and priority from combined string
  static (String, GianttPriority) _parseIdPriority(String idPriority) {
    // Priority symbols in order of length (longest first to avoid partial matches)
    const prioritySymbols = ['!!!', '!!', '!', '?', '...', ',,,'];
    
    for (final symbol in prioritySymbols) {
      if (idPriority.endsWith(symbol)) {
        final id = idPriority.substring(0, idPriority.length - symbol.length);
        final priority = GianttPriority.fromSymbol(symbol);
        return (id, priority);
      }
    }
    
    // No priority symbol found, default to neutral
    return (idPriority, GianttPriority.neutral);
  }

  /// Find the closing quote, handling escaped quotes
  static int _findClosingQuote(String line, int startPos) {
    var pos = startPos + 1;
    while (pos < line.length) {
      if (line[pos] == '"') {
        // Check if it's escaped
        var backslashCount = 0;
        var checkPos = pos - 1;
        while (checkPos >= 0 && line[checkPos] == '\\') {
          backslashCount++;
          checkPos--;
        }
        // If even number of backslashes (including 0), quote is not escaped
        if (backslashCount % 2 == 0) {
          return pos;
        }
      }
      pos++;
    }
    return -1;
  }

  /// Parse the post-title section
  static (List<String>, List<String>, Map<String, List<String>>, List<TimeConstraint>, String?, String?) 
      _parsePostTitleSection(String postTitle) {
    
    // Parse charts first
    final chartsPattern = RegExp(r'^\s*(\{[^}]*\})\s*(.*)$');
    final chartsMatch = chartsPattern.firstMatch(postTitle);
    
    if (chartsMatch == null) {
      throw GianttParseException('Invalid charts format', postTitle);
    }
    
    final chartsStr = chartsMatch.group(1)!;
    final remainder = chartsMatch.group(2)!;
    
    // Parse charts
    final charts = _parseCharts(chartsStr);
    
    // Parse comments from the entire remainder first
    final (userComment, autoComment) = _parseComments(remainder);
    
    // Split remainder by >>> to separate tags from relations
    final parts = remainder.split('>>>');
    final tagsStr = parts[0].trim();
    final relationsAndConstraints = parts.length > 1 ? parts[1].trim() : '';
    
    // Parse tags (remove comments from tags string)
    final cleanTagsStr = _removeComments(tagsStr);
    final tags = _parseTags(cleanTagsStr);
    
    // Split relations section by @@@ to separate relations from time constraints
    final constraintParts = relationsAndConstraints.split('@@@');
    final relationsStr = constraintParts[0].trim();
    final timeConstraintStr = constraintParts.length > 1 ? constraintParts[1].trim() : null;
    
    // Parse relations (remove comments from relations string)
    final cleanRelationsStr = _removeComments(relationsStr);
    final relations = _parseRelations(cleanRelationsStr);
    
    // Parse time constraints (remove comments from constraint string)
    final cleanTimeConstraintStr = timeConstraintStr != null ? _removeComments(timeConstraintStr) : null;
    final timeConstraints = <TimeConstraint>[];
    if (cleanTimeConstraintStr != null && cleanTimeConstraintStr.isNotEmpty) {
      _parseTimeConstraints(cleanTimeConstraintStr, timeConstraints);
    }
    
    return (charts, tags, relations, timeConstraints, userComment, autoComment);
  }

  /// Parse charts from string like {"Chart1","Chart2"}
  static List<String> _parseCharts(String chartsStr) {
    if (chartsStr == '{}') return [];
    
    // Remove outer braces
    final inner = chartsStr.substring(1, chartsStr.length - 1);
    if (inner.trim().isEmpty) return [];
    
    // Split by comma and clean up
    return inner.split(',')
        .map((c) => c.trim().replaceAll('"', ''))
        .where((c) => c.isNotEmpty)
        .toList();
  }

  /// Parse tags from comma-separated string
  static List<String> _parseTags(String tagsStr) {
    if (tagsStr.isEmpty) return [];
    
    return tagsStr.split(',')
        .map((t) => t.trim())
        .where((t) => t.isNotEmpty)
        .toList();
  }

  /// Parse relations from string with symbols and brackets
  static Map<String, List<String>> _parseRelations(String relationsStr) {
    final relations = <String, List<String>>{};
    
    if (relationsStr.isEmpty) return relations;
    
    // Map symbols to relation type names
    final symbolToType = <String, String>{
      '⊢': 'REQUIRES',
      '⋲': 'ANYOF', 
      '≫': 'SUPERCHARGES',
      '∴': 'INDICATES',
      '∪': 'TOGETHER',
      '⊟': 'CONFLICTS',
      '►': 'BLOCKS',
      '≻': 'SUFFICIENT',
    };
    
    for (final entry in symbolToType.entries) {
      final symbol = entry.key;
      final relType = entry.value;
      
      final pattern = RegExp('${RegExp.escape(symbol)}\\[([^\\]]+)\\]');
      final matches = pattern.allMatches(relationsStr);
      
      for (final match in matches) {
        final targetsStr = match.group(1)!;
        final targets = targetsStr.split(',')
            .map((t) => t.trim())
            .where((t) => t.isNotEmpty)
            .toList();
        
        if (targets.isNotEmpty) {
          relations[relType] = targets;
        }
      }
    }
    
    return relations;
  }

  /// Parse comments from the relations and constraints section
  static (String?, String?) _parseComments(String text) {
    String? userComment;
    String? autoComment;
    
    // Look for auto comment first (triple ###) to avoid conflicts
    final autoCommentMatch = RegExp(r'###\s*(.*)$').firstMatch(text);
    if (autoCommentMatch != null) {
      autoComment = autoCommentMatch.group(1)?.trim();
      // Remove the auto comment from text to avoid conflicts with user comment parsing
      text = text.replaceFirst(RegExp(r'###.*$'), '').trim();
    }
    
    // Look for user comment (single # but not ###)
    final userCommentMatch = RegExp(r'#(?!##)\s*(.*)$').firstMatch(text);
    if (userCommentMatch != null) {
      userComment = userCommentMatch.group(1)?.trim();
    }
    
    return (userComment, autoComment);
  }

  /// Parse duration safely, wrapping errors in GianttParseException
  static GianttDuration _parseDurationSafely(String duration) {
    try {
      return GianttDuration.parse(duration);
    } catch (e) {
      throw GianttParseException('Invalid duration format: $duration', duration);
    }
  }

  /// Remove comments from a string
  static String _removeComments(String text) {
    // Remove auto comments (###)
    text = text.replaceFirst(RegExp(r'###.*$'), '').trim();
    // Remove user comments (#)
    text = text.replaceFirst(RegExp(r'#(?!##).*$'), '').trim();
    return text;
  }

}

/// Extension to add strip method to String
extension StringExtensions on String {
  String strip() => trim();
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/parser/parse_exceptions.dart
// =====================================================================

import '../models/graph_exceptions.dart';

/// Exception thrown when parsing a specific section fails
class SectionParseException extends GianttParseException {
  const SectionParseException(this.section, String message, [String? input])
      : super('$section: $message', input);

  /// The section that failed to parse
  final String section;
}

/// Exception thrown when a required field is missing
class MissingFieldException extends GianttParseException {
  const MissingFieldException(this.fieldName, [String? input])
      : super('Missing required field: $fieldName', input);

  /// The name of the missing field
  final String fieldName;
}

/// Exception thrown when a field has an invalid format
class InvalidFormatException extends GianttParseException {
  const InvalidFormatException(this.fieldName, this.expectedFormat, [String? input])
      : super('Invalid format for $fieldName. Expected: $expectedFormat', input);

  /// The name of the field with invalid format
  final String fieldName;
  
  /// Description of the expected format
  final String expectedFormat;
}

/// Exception thrown when an unknown symbol is encountered
class UnknownSymbolException extends GianttParseException {
  const UnknownSymbolException(this.symbol, this.context, [String? input])
      : super('Unknown symbol "$symbol" in $context', input);

  /// The unknown symbol
  final String symbol;
  
  /// The context where the symbol was found
  final String context;
}

/// Exception thrown when quotes are not properly balanced
class UnbalancedQuotesException extends GianttParseException {
  const UnbalancedQuotesException([String? input])
      : super('Unbalanced quotes in title', input);
}

/// Exception thrown when JSON parsing fails
class JsonParseException extends GianttParseException {
  const JsonParseException(this.jsonError, [String? input])
      : super('JSON parse error: $jsonError', input);

  /// The underlying JSON error
  final String jsonError;
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/file_header_generator.dart
// =====================================================================

import '../models/relation.dart';

/// Generates file headers and banners for Giantt files
class FileHeaderGenerator {
  /// Create a banner with text centered in a box of hash characters
  static String createBanner(String text, {int paddingH = 5, int paddingV = 1}) {
    final lines = text.split('\n');
    final maxLength = lines.map((line) => line.length).reduce((a, b) => a > b ? a : b);
    final bannerLen = maxLength + 2 * paddingH;
    final topBottomBorder = '#' * (bannerLen + 2);

    final buffer = StringBuffer();
    buffer.writeln(topBottomBorder);

    // Add vertical padding lines
    final emptyLine = '#${' ' * bannerLen}#';
    for (int i = 0; i < paddingV; i++) {
      buffer.writeln(emptyLine);
    }

    // Add text lines, centered
    for (final line in lines) {
      final padding = maxLength - line.length;
      final leftPadding = padding ~/ 2;
      final rightPadding = padding - leftPadding;
      buffer.writeln('#${' ' * paddingH}${' ' * leftPadding}$line${' ' * rightPadding}${' ' * paddingH}#');
    }

    // Add vertical padding lines
    for (int i = 0; i < paddingV; i++) {
      buffer.writeln(emptyLine);
    }

    buffer.writeln(topBottomBorder);
    return buffer.toString();
  }

  /// Generate header for items file
  static String generateItemsFileHeader() {
    return createBanner(
      'Giantt Items\n'
      'This file contains all include Giantt items in topological\n'
      'order according to the REQUIRES (${RelationType.requires.symbol}) relation.\n'
      'You can use #include directives at the top of this file\n'
      'to include other Giantt item files.\n'
      'Edit this file manually at your own risk.'
    );
  }

  /// Generate header for occluded items file
  static String generateOccludedItemsFileHeader() {
    return createBanner(
      'Giantt Occluded Items\n'
      'This file contains all occluded Giantt items in topological\n'
      'order according to the REQUIRES (${RelationType.requires.symbol}) relation.\n'
      'Edit this file manually at your own risk.'
    );
  }

  /// Generate header for logs file
  static String generateLogsFileHeader() {
    return createBanner(
      'Giantt Logs\n'
      'This file contains log entries in JSONL format.\n'
      'Each line is a JSON object representing a log entry.\n'
      'Edit this file manually at your own risk.'
    );
  }

  /// Generate header for occluded logs file
  static String generateOccludedLogsFileHeader() {
    return createBanner(
      'Giantt Occluded Logs\n'
      'This file contains occluded log entries in JSONL format.\n'
      'Each line is a JSON object representing a log entry.\n'
      'Edit this file manually at your own risk.'
    );
  }

  /// Generate header for metadata file
  static String generateMetadataFileHeader() {
    return createBanner(
      'Giantt Metadata\n'
      'This file contains metadata for the Giantt workspace.\n'
      'Edit this file manually at your own risk.'
    );
  }

  /// Generate a custom header with specified title and description
  static String generateCustomHeader(String title, String description) {
    return createBanner('$title\n$description');
  }

  /// Generate initialization timestamp comment
  static String generateTimestampComment() {
    final now = DateTime.now();
    return '# Generated on ${now.toIso8601String()}\n';
  }

  /// Generate include directive
  static String generateIncludeDirective(String includePath) {
    return '#include $includePath\n';
  }

  /// Generate file header with timestamp
  static String generateFileHeaderWithTimestamp(String headerType) {
    final header = switch (headerType.toLowerCase()) {
      'items' => generateItemsFileHeader(),
      'occluded_items' => generateOccludedItemsFileHeader(),
      'logs' => generateLogsFileHeader(),
      'occluded_logs' => generateOccludedLogsFileHeader(),
      'metadata' => generateMetadataFileHeader(),
      _ => generateCustomHeader('Giantt File', 'Generated file'),
    };
    
    return header + '\n' + generateTimestampComment() + '\n';
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/path_resolver.dart
// =====================================================================

import 'dart:io';

/// Resolves file paths for Giantt files with mobile-friendly fallbacks
class PathResolver {
  /// Get the default Giantt directory path
  static String getDefaultGianttDirectory({bool dev = false}) {
    if (dev) {
      // Use local directory in dev mode
      return '${Directory.current.path}${Platform.pathSeparator}.giantt';
    } else {
      // Use home directory in normal mode, with mobile-friendly fallback
      try {
        final homeDir = _getHomeDirectory();
        return '$homeDir${Platform.pathSeparator}.giantt';
      } catch (e) {
        // Fallback to current directory if home directory is not accessible
        return '${Directory.current.path}${Platform.pathSeparator}.giantt';
      }
    }
  }

  /// Get the home directory with mobile platform considerations
  static String _getHomeDirectory() {
    // Try environment variables first
    final home = Platform.environment['HOME'] ?? 
                 Platform.environment['USERPROFILE'] ?? 
                 Platform.environment['HOMEPATH'];
    
    if (home != null && home.isNotEmpty) {
      return home;
    }

    // Platform-specific fallbacks
    if (Platform.isWindows) {
      final userProfile = Platform.environment['USERPROFILE'];
      if (userProfile != null) return userProfile;
      
      final homeDrive = Platform.environment['HOMEDRIVE'] ?? 'C:';
      final homePath = Platform.environment['HOMEPATH'] ?? '\\Users\\Default';
      return '$homeDrive$homePath';
    }

    if (Platform.isLinux || Platform.isMacOS) {
      // Try common Unix home directory patterns
      final user = Platform.environment['USER'] ?? Platform.environment['USERNAME'];
      if (user != null) {
        final homeDir = '/home/$user';
        if (Directory(homeDir).existsSync()) {
          return homeDir;
        }
      }
    }

    // Mobile platform fallbacks
    if (Platform.isAndroid || Platform.isIOS) {
      // On mobile, we might not have access to a traditional home directory
      // Fall back to the app's documents directory or current directory
      return Directory.current.path;
    }

    // Last resort fallback
    return Directory.current.path;
  }

  /// Get the default path for a Giantt file
  static String getDefaultGianttPath(String filename, {bool occlude = false, bool dev = false}) {
    final baseDir = getDefaultGianttDirectory(dev: dev);
    final subDir = occlude ? 'occlude' : 'include';
    return '$baseDir${Platform.pathSeparator}$subDir${Platform.pathSeparator}$filename';
  }

  /// Check if a Giantt workspace exists at the given path
  static bool gianttWorkspaceExists(String basePath) {
    final includeDir = Directory('$basePath${Platform.pathSeparator}include');
    final occludeDir = Directory('$basePath${Platform.pathSeparator}occlude');
    
    return includeDir.existsSync() && occludeDir.existsSync();
  }

  /// Find the nearest Giantt workspace by walking up the directory tree
  static String? findNearestGianttWorkspace([String? startPath]) {
    startPath ??= Directory.current.path;
    
    var currentDir = Directory(startPath);
    
    while (true) {
      final gianttDir = '${currentDir.path}${Platform.pathSeparator}.giantt';
      if (gianttWorkspaceExists(gianttDir)) {
        return gianttDir;
      }
      
      final parentDir = currentDir.parent;
      if (parentDir.path == currentDir.path) {
        // Reached root directory
        break;
      }
      currentDir = parentDir;
    }
    
    return null;
  }

  /// Get the active Giantt workspace path (local dev > nearest > home)
  static String getActiveGianttWorkspace() {
    // First check for local dev workspace
    final localDevPath = getDefaultGianttDirectory(dev: true);
    if (gianttWorkspaceExists(localDevPath)) {
      return localDevPath;
    }
    
    // Then check for nearest workspace
    final nearestPath = findNearestGianttWorkspace();
    if (nearestPath != null) {
      return nearestPath;
    }
    
    // Finally fall back to home directory
    return getDefaultGianttDirectory(dev: false);
  }

  /// Resolve a relative path against a base path
  static String resolvePath(String basePath, String relativePath) {
    if (isAbsolutePath(relativePath)) {
      return relativePath;
    }
    
    // Handle parent directory references
    final baseParts = basePath.split(Platform.pathSeparator);
    final relativeParts = relativePath.split('/'); // Always use forward slash for relative paths
    
    final resolvedParts = List<String>.from(baseParts);
    
    for (final part in relativeParts) {
      if (part == '..') {
        if (resolvedParts.isNotEmpty) {
          resolvedParts.removeLast();
        }
      } else if (part != '.' && part.isNotEmpty) {
        resolvedParts.add(part);
      }
    }
    
    return resolvedParts.join(Platform.pathSeparator);
  }

  /// Check if a path is absolute
  static bool isAbsolutePath(String path) {
    if (Platform.isWindows) {
      // Windows: C:\ or \\server\share
      return path.length >= 2 && 
             ((path[1] == ':') || (path.startsWith(r'\\')));
    } else {
      // Unix-like: starts with /
      return path.startsWith('/');
    }
  }

  /// Normalize a path for the current platform
  static String normalizePath(String path) {
    // Replace forward slashes with platform separator
    if (Platform.pathSeparator != '/') {
      path = path.replaceAll('/', Platform.pathSeparator);
    }
    
    // Remove duplicate separators
    final separator = Platform.pathSeparator;
    while (path.contains('$separator$separator')) {
      path = path.replaceAll('$separator$separator', separator);
    }
    
    return path;
  }

  /// Get a safe filename by removing invalid characters
  static String getSafeFilename(String filename) {
    // Remove or replace characters that are invalid on various platforms
    final invalidChars = RegExp(r'[<>:"/\\|?*\x00-\x1f]');
    return filename.replaceAll(invalidChars, '_');
  }

  /// Create directory structure if it doesn't exist
  static void ensureDirectoryExists(String dirPath) {
    final directory = Directory(dirPath);
    if (!directory.existsSync()) {
      try {
        directory.createSync(recursive: true);
      } catch (e) {
        throw Exception('Failed to create directory $dirPath: $e');
      }
    }
  }

  /// Get the relative path from one directory to another
  static String getRelativePath(String from, String to) {
    final fromParts = from.split(Platform.pathSeparator);
    final toParts = to.split(Platform.pathSeparator);
    
    // Find common prefix
    int commonLength = 0;
    final minLength = fromParts.length < toParts.length ? fromParts.length : toParts.length;
    
    for (int i = 0; i < minLength; i++) {
      if (fromParts[i] == toParts[i]) {
        commonLength++;
      } else {
        break;
      }
    }
    
    // Build relative path
    final relativeParts = <String>[];
    
    // Add .. for each remaining part in from
    for (int i = commonLength; i < fromParts.length; i++) {
      relativeParts.add('..');
    }
    
    // Add remaining parts from to
    for (int i = commonLength; i < toParts.length; i++) {
      relativeParts.add(toParts[i]);
    }
    
    return relativeParts.isEmpty ? '.' : relativeParts.join('/');
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/file_repository.dart
// =====================================================================

import 'dart:io';
import '../models/giantt_item.dart';
import '../models/log_entry.dart';
import '../models/graph_exceptions.dart';
import '../graph/giantt_graph.dart';
import '../parser/giantt_parser.dart';
import '../logging/log_collection.dart';
import 'atomic_file_writer.dart';
import 'backup_manager.dart';
import 'file_header_generator.dart';
import 'path_resolver.dart';

/// Repository for file I/O operations with include support
class FileRepository {

  /// Parse include directives from a file
  /// 
  /// Include directives should be at the top of the file in the format:
  /// #include path/to/file.txt
  static List<String> parseIncludeDirectives(String filepath) {
    final includes = <String>[];
    
    try {
      final file = File(filepath);
      if (!file.existsSync()) {
        return includes;
      }
      
      final lines = file.readAsLinesSync();
      for (final line in lines) {
        final trimmed = line.trim();
        if (trimmed.isEmpty || !trimmed.startsWith('#include ')) {
          break; // Only process directives at the top
        }
        final includePath = trimmed.substring(9).trim(); // Remove '#include ' prefix
        includes.add(includePath);
      }
    } catch (e) {
      throw GraphException('Error reading include directives from $filepath: $e');
    }
    
    return includes;
  }

  /// Load a graph from a file, processing include directives
  /// 
  /// Args:
  ///   filepath: Path to the file to load
  ///   loadedFiles: Set of files already loaded (to prevent circular includes)
  ///   
  /// Returns:
  ///   GianttGraph object
  static GianttGraph loadGraphFromFile(String filepath, [Set<String>? loadedFiles]) {
    loadedFiles ??= <String>{};
    
    // Prevent circular includes
    if (loadedFiles.contains(filepath)) {
      throw GraphException('Circular include detected for $filepath');
    }
    
    loadedFiles.add(filepath);
    
    final file = File(filepath);
    if (!file.existsSync()) {
      throw GraphException('File not found: $filepath');
    }
    
    // Process include directives
    final includes = parseIncludeDirectives(filepath);
    
    // Create the graph
    final graph = GianttGraph();
    
    // Load included files first
    for (final includePath in includes) {
      // Handle relative paths
      String resolvedPath;
      if (_isAbsolutePath(includePath)) {
        resolvedPath = includePath;
      } else {
        final baseDir = _getDirectoryPath(filepath);
        resolvedPath = _joinPaths(baseDir, includePath);
      }
      
      try {
        final includeGraph = loadGraphFromFile(resolvedPath, Set.from(loadedFiles));
        // Merge the included graph
        for (final item in includeGraph.items.values) {
          graph.addItem(item);
        }
      } catch (e) {
        throw GraphException('Error loading include $resolvedPath: $e');
      }
    }
    
    // Now load the main file content
    final lines = file.readAsLinesSync();
    final isOccludeFile = filepath.contains('occlude');
    
    for (final line in lines) {
      final trimmed = line.trim();
      if (trimmed.isNotEmpty && !trimmed.startsWith('#')) {
        try {
          final item = GianttParser.fromString(trimmed, occlude: isOccludeFile);
          graph.addItem(item);
        } catch (e) {
          // Skip invalid lines with warning
          print('Warning: Skipping invalid line in $filepath: $e');
        }
      }
    }
    
    return graph;
  }

  /// Load a graph from main and occluded files, processing includes
  static GianttGraph loadGraph(String filepath, String occludeFilepath) {
    final loadedFiles = <String>{};
    final mainGraph = loadGraphFromFile(filepath, loadedFiles);
    final occludeGraph = loadGraphFromFile(occludeFilepath, Set.from(loadedFiles));
    
    // Merge graphs
    for (final item in occludeGraph.items.values) {
      mainGraph.addItem(item);
    }
    
    return mainGraph;
  }

  /// Load logs from main and occluded files
  static LogCollection loadLogs(String filepath, String occludeFilepath) {
    final logs = LogCollection();
    
    // Load include logs
    logs.addEntries(_loadLogsFromFile(filepath, occlude: false));
    
    // Load occlude logs
    logs.addEntries(_loadLogsFromFile(occludeFilepath, occlude: true));
    
    return logs;
  }

  /// Save a graph to files with atomic operations and proper headers
  static void saveGraph(String filepath, String occludeFilepath, GianttGraph graph) {
    try {
      // First perform topological sort to validate the graph
      final sortedItems = graph.topologicalSort();

      // Prepare file contents
      final includeContent = StringBuffer();
      final occludeContent = StringBuffer();

      // Add headers
      includeContent.writeln(FileHeaderGenerator.generateItemsFileHeader());
      includeContent.writeln();
      
      occludeContent.writeln(FileHeaderGenerator.generateOccludedItemsFileHeader());
      occludeContent.writeln();

      // Add items to appropriate files
      for (final item in sortedItems) {
        final itemString = item.toFileString();
        if (item.occlude) {
          occludeContent.writeln(itemString);
        } else {
          includeContent.writeln(itemString);
        }
      }

      // Write both files atomically
      final fileContents = {
        filepath: includeContent.toString(),
        occludeFilepath: occludeContent.toString(),
      };

      AtomicFileWriter.writeFiles(fileContents);

    } catch (e) {
      if (e is GraphException) rethrow;
      throw GraphException('Failed to save graph: $e');
    }
  }

  /// Initialize a Giantt workspace with proper directory structure
  static void initializeWorkspace(String basePath, {bool dev = false}) {
    try {
      // Create directory structure
      final includeDir = '$basePath${Platform.pathSeparator}include';
      final occludeDir = '$basePath${Platform.pathSeparator}occlude';
      
      PathResolver.ensureDirectoryExists(includeDir);
      PathResolver.ensureDirectoryExists(occludeDir);

      // Create initial files with headers
      final files = {
        '$includeDir${Platform.pathSeparator}items.txt': 
            FileHeaderGenerator.generateItemsFileHeader(),
        '$includeDir${Platform.pathSeparator}logs.jsonl': 
            FileHeaderGenerator.generateLogsFileHeader(),
        '$includeDir${Platform.pathSeparator}metadata.json': 
            FileHeaderGenerator.generateMetadataFileHeader() + '\n{}\n',
        '$occludeDir${Platform.pathSeparator}items.txt': 
            FileHeaderGenerator.generateOccludedItemsFileHeader(),
        '$occludeDir${Platform.pathSeparator}logs.jsonl': 
            FileHeaderGenerator.generateOccludedLogsFileHeader(),
        '$occludeDir${Platform.pathSeparator}metadata.json': 
            FileHeaderGenerator.generateMetadataFileHeader() + '\n{}\n',
      };

      // Only create files that don't already exist
      final filesToCreate = <String, String>{};
      for (final entry in files.entries) {
        if (!File(entry.key).existsSync()) {
          filesToCreate[entry.key] = entry.value;
        }
      }

      if (filesToCreate.isNotEmpty) {
        AtomicFileWriter.writeFiles(filesToCreate, createBackup: false);
      }

    } catch (e) {
      throw GraphException('Failed to initialize workspace: $e');
    }
  }

  /// Check if a workspace is properly initialized
  static bool isWorkspaceInitialized(String basePath) {
    return PathResolver.gianttWorkspaceExists(basePath);
  }

  /// Get the default file paths for a workspace
  static Map<String, String> getDefaultFilePaths([String? basePath]) {
    basePath ??= PathResolver.getActiveGianttWorkspace();
    
    return {
      'items': '$basePath${Platform.pathSeparator}include${Platform.pathSeparator}items.txt',
      'occlude_items': '$basePath${Platform.pathSeparator}occlude${Platform.pathSeparator}items.txt',
      'logs': '$basePath${Platform.pathSeparator}include${Platform.pathSeparator}logs.jsonl',
      'occlude_logs': '$basePath${Platform.pathSeparator}occlude${Platform.pathSeparator}logs.jsonl',
      'metadata': '$basePath${Platform.pathSeparator}include${Platform.pathSeparator}metadata.json',
      'occlude_metadata': '$basePath${Platform.pathSeparator}occlude${Platform.pathSeparator}metadata.json',
    };
  }

  /// Validate that all required files exist and are accessible
  static void validateWorkspace(String basePath) {
    final paths = getDefaultFilePaths(basePath);
    
    for (final entry in paths.entries) {
      final file = File(entry.value);
      if (!file.existsSync()) {
        throw GraphException('Required file missing: ${entry.value}');
      }
      
      // Check if file is readable
      try {
        file.readAsStringSync();
      } catch (e) {
        throw GraphException('Cannot read file ${entry.value}: $e');
      }
    }
  }

  /// Check if a path is absolute
  static bool _isAbsolutePath(String path) {
    return path.startsWith('/') || (path.length > 1 && path[1] == ':'); // Unix or Windows
  }

  /// Get the directory part of a file path
  static String _getDirectoryPath(String filepath) {
    final lastSeparator = filepath.lastIndexOf(Platform.pathSeparator);
    if (lastSeparator == -1) {
      return '.'; // Current directory
    }
    return filepath.substring(0, lastSeparator);
  }

  /// Join two path components
  static String _joinPaths(String dir, String filename) {
    if (dir.endsWith(Platform.pathSeparator)) {
      return dir + filename;
    }
    return dir + Platform.pathSeparator + filename;
  }

  /// Get the include structure of a file
  static void showIncludeStructure(String filepath, {bool recursive = false, int depth = 0, Set<String>? visited}) {
    visited ??= <String>{};
    
    final indent = '  ' * depth;
    
    if (visited.contains(filepath)) {
      print('${indent}└─ $filepath (circular include, skipping)');
      return;
    }
    
    visited.add(filepath);
    
    if (!File(filepath).existsSync()) {
      print('${indent}└─ $filepath (file not found)');
      return;
    }
    
    print('${indent}└─ $filepath');
    
    if (recursive) {
      final includes = parseIncludeDirectives(filepath);
      for (final includePath in includes) {
        // Handle relative paths
        String resolvedPath;
        if (_isAbsolutePath(includePath)) {
          resolvedPath = includePath;
        } else {
          final baseDir = _getDirectoryPath(filepath);
          resolvedPath = _joinPaths(baseDir, includePath);
        }
        
        showIncludeStructure(resolvedPath, recursive: true, depth: depth + 1, visited: Set.from(visited));
      }
    }
  }

  /// Load logs from a single file
  static List<LogEntry> _loadLogsFromFile(String filepath, {required bool occlude}) {
    final logs = <LogEntry>[];
    
    try {
      final file = File(filepath);
      if (!file.existsSync()) {
        return logs;
      }

      final lines = file.readAsLinesSync();
      for (final line in lines) {
        final trimmed = line.trim();
        if (trimmed.isNotEmpty && !trimmed.startsWith('#')) {
          try {
            final log = LogEntry.fromJsonLine(trimmed, occlude: occlude);
            logs.add(log);
          } catch (e) {
            // Skip invalid lines with warning
            print('Warning: Skipping invalid log line in $filepath: $e');
          }
        }
      }
    } catch (e) {
      throw GraphException('Error loading logs from $filepath: $e');
    }
    
    return logs;
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/backup_manager.dart
// =====================================================================

import 'dart:io';
import '../models/graph_exceptions.dart';

/// Manages backup files with configurable retention and cleanup
class BackupManager {
  /// Default number of backups to keep
  static const int defaultRetentionCount = 3;

  /// Create a backup of a file with incremental naming
  static String createBackup(String filepath, {int? retentionCount}) {
    retentionCount ??= defaultRetentionCount;
    
    final file = File(filepath);
    if (!file.existsSync()) {
      throw GraphException('Cannot backup non-existent file: $filepath');
    }

    final backupPath = _getNextBackupPath(filepath);
    
    try {
      file.copySync(backupPath);
      _cleanupOldBackups(filepath, retentionCount);
      return backupPath;
    } catch (e) {
      throw GraphException('Failed to create backup: $e');
    }
  }

  /// Get the next available backup path with incremental numbering
  static String _getNextBackupPath(String filepath) {
    int backupNum = 1;
    String backupPath;
    
    do {
      backupPath = '$filepath.$backupNum.backup';
      backupNum++;
    } while (File(backupPath).existsSync());
    
    return backupPath;
  }

  /// Get the most recent backup path for a file
  static String? getMostRecentBackup(String filepath) {
    final directory = Directory(_getDirectoryPath(filepath));
    if (!directory.existsSync()) return null;

    final filename = _getFileName(filepath);
    final backupPattern = RegExp(r'^' + RegExp.escape(filename) + r'\.(\d+)\.backup$');
    
    int highestNum = 0;
    String? mostRecentBackup;
    
    try {
      for (final entity in directory.listSync()) {
        if (entity is File) {
          final match = backupPattern.firstMatch(_getFileName(entity.path));
          if (match != null) {
            final num = int.parse(match.group(1)!);
            if (num > highestNum) {
              highestNum = num;
              mostRecentBackup = entity.path;
            }
          }
        }
      }
    } catch (e) {
      // Ignore permission errors on mobile platforms
      return null;
    }
    
    return mostRecentBackup;
  }

  /// Clean up old backups, keeping only the most recent ones
  static void _cleanupOldBackups(String filepath, int retentionCount) {
    final directory = Directory(_getDirectoryPath(filepath));
    if (!directory.existsSync()) return;

    final filename = _getFileName(filepath);
    final backupPattern = RegExp(r'^' + RegExp.escape(filename) + r'\.(\d+)\.backup$');
    
    final backups = <int, String>{};
    
    try {
      for (final entity in directory.listSync()) {
        if (entity is File) {
          final match = backupPattern.firstMatch(_getFileName(entity.path));
          if (match != null) {
            final num = int.parse(match.group(1)!);
            backups[num] = entity.path;
          }
        }
      }
    } catch (e) {
      // Ignore permission errors on mobile platforms
      return;
    }

    // Sort backup numbers in descending order (newest first)
    final sortedNums = backups.keys.toList()..sort((a, b) => b.compareTo(a));
    
    // Delete backups beyond retention count
    for (int i = retentionCount; i < sortedNums.length; i++) {
      final backupPath = backups[sortedNums[i]]!;
      try {
        File(backupPath).deleteSync();
      } catch (e) {
        // Ignore deletion errors (may be permission issues on mobile)
      }
    }
  }

  /// Check if the most recent backup is identical to the current file
  static bool isIdenticalToMostRecentBackup(String filepath) {
    final mostRecentBackup = getMostRecentBackup(filepath);
    if (mostRecentBackup == null) return false;

    try {
      final currentContent = File(filepath).readAsStringSync();
      final backupContent = File(mostRecentBackup).readAsStringSync();
      return currentContent == backupContent;
    } catch (e) {
      return false;
    }
  }

  /// Remove the most recent backup if it's identical to the current file
  static void removeDuplicateBackup(String filepath) {
    if (isIdenticalToMostRecentBackup(filepath)) {
      final mostRecentBackup = getMostRecentBackup(filepath);
      if (mostRecentBackup != null) {
        try {
          File(mostRecentBackup).deleteSync();
        } catch (e) {
          // Ignore deletion errors
        }
      }
    }
  }

  /// Get all backup files for a given file
  static List<String> getAllBackups(String filepath) {
    final directory = Directory(_getDirectoryPath(filepath));
    if (!directory.existsSync()) return [];

    final filename = _getFileName(filepath);
    final backupPattern = RegExp(r'^' + RegExp.escape(filename) + r'\.(\d+)\.backup$');
    
    final backups = <int, String>{};
    
    try {
      for (final entity in directory.listSync()) {
        if (entity is File) {
          final match = backupPattern.firstMatch(_getFileName(entity.path));
          if (match != null) {
            final num = int.parse(match.group(1)!);
            backups[num] = entity.path;
          }
        }
      }
    } catch (e) {
      return [];
    }

    // Sort by backup number (oldest first)
    final sortedNums = backups.keys.toList()..sort();
    return sortedNums.map((num) => backups[num]!).toList();
  }

  /// Clean up all backups for multiple files, keeping specified retention
  static void cleanupAllBackups(List<String> filepaths, {int? retentionCount}) {
    retentionCount ??= defaultRetentionCount;
    
    for (final filepath in filepaths) {
      try {
        _cleanupOldBackups(filepath, retentionCount);
      } catch (e) {
        // Continue with other files if one fails
      }
    }
  }

  /// Get directory path from file path
  static String _getDirectoryPath(String filepath) {
    final lastSeparator = filepath.lastIndexOf(Platform.pathSeparator);
    if (lastSeparator == -1) {
      return '.'; // Current directory
    }
    return filepath.substring(0, lastSeparator);
  }

  /// Get filename from file path
  static String _getFileName(String filepath) {
    final lastSeparator = filepath.lastIndexOf(Platform.pathSeparator);
    if (lastSeparator == -1) {
      return filepath;
    }
    return filepath.substring(lastSeparator + 1);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/dual_file_manager.dart
// =====================================================================

import 'dart:io';
import '../models/giantt_item.dart';
import '../models/log_entry.dart';
import '../models/graph_exceptions.dart';
import '../graph/giantt_graph.dart';
import '../parser/giantt_parser.dart';
import '../logging/log_collection.dart';
import '../logging/log_occluder.dart';
import 'atomic_file_writer.dart';
import 'backup_manager.dart';
import 'file_header_generator.dart';
import 'path_resolver.dart';

/// Manages dual-file operations for include/occlude system
class DualFileManager {
  /// Load a graph from main and occluded files, processing includes
  static GianttGraph loadGraph(String filepath, String occludeFilepath) {
    final loadedFiles = <String>{};
    final mainGraph = FileRepository.loadGraphFromFile(filepath, loadedFiles);
    final occludeGraph = FileRepository.loadGraphFromFile(occludeFilepath, Set.from(loadedFiles));
    
    // Merge graphs
    for (final item in occludeGraph.items.values) {
      mainGraph.addItem(item);
    }
    
    return mainGraph;
  }

  /// Save a graph to files with atomic operations and proper headers
  static void saveGraph(String filepath, String occludeFilepath, GianttGraph graph) {
    try {
      // First perform topological sort to validate the graph
      final sortedItems = graph.topologicalSort();

      // Prepare file contents
      final includeContent = StringBuffer();
      final occludeContent = StringBuffer();

      // Add headers
      includeContent.writeln(FileHeaderGenerator.generateItemsFileHeader());
      includeContent.writeln();
      
      occludeContent.writeln(FileHeaderGenerator.generateOccludedItemsFileHeader());
      occludeContent.writeln();

      // Add items to appropriate files
      for (final item in sortedItems) {
        final itemString = item.toFileString();
        if (item.occlude) {
          occludeContent.writeln(itemString);
        } else {
          includeContent.writeln(itemString);
        }
      }

      // Write both files atomically
      final fileContents = {
        filepath: includeContent.toString(),
        occludeFilepath: occludeContent.toString(),
      };

      AtomicFileWriter.writeFiles(fileContents);

    } catch (e) {
      if (e is GraphException) rethrow;
      throw GraphException('Failed to save graph: $e');
    }
  }

  /// Occlude items by ID with optional dry-run
  static OccludeResult occludeItems(
    GianttGraph graph, 
    List<String> itemIds, 
    {bool dryRun = false}
  ) {
    final toOcclude = <String>[];
    final notFound = <String>[];

    // Find items to occlude
    for (final id in itemIds) {
      final item = graph.items[id];
      if (item != null && !item.occlude) {
        toOcclude.add(id);
      } else if (item == null) {
        notFound.add(id);
      }
    }

    if (dryRun) {
      return OccludeResult(
        occludedCount: toOcclude.length,
        occludedItems: toOcclude,
        notFoundItems: notFound,
        dryRun: true,
      );
    }

    // Actually occlude the items
    for (final id in toOcclude) {
      final item = graph.items[id]!;
      final occludedItem = item.copyWith(occlude: true);
      graph.addItem(occludedItem);
    }

    return OccludeResult(
      occludedCount: toOcclude.length,
      occludedItems: toOcclude,
      notFoundItems: notFound,
      dryRun: false,
    );
  }

  /// Occlude items by tags with optional dry-run
  static OccludeResult occludeItemsByTags(
    GianttGraph graph, 
    List<String> tags, 
    {bool dryRun = false}
  ) {
    final toOcclude = <String>[];

    // Find items with matching tags
    for (final item in graph.includedItems.values) {
      if (tags.any((tag) => item.tags.contains(tag))) {
        toOcclude.add(item.id);
      }
    }

    return occludeItems(graph, toOcclude, dryRun: dryRun);
  }

  /// Include (un-occlude) items by ID with optional dry-run
  static OccludeResult includeItems(
    GianttGraph graph, 
    List<String> itemIds, 
    {bool dryRun = false}
  ) {
    final toInclude = <String>[];
    final notFound = <String>[];

    // Find items to include
    for (final id in itemIds) {
      final item = graph.items[id];
      if (item != null && item.occlude) {
        toInclude.add(id);
      } else if (item == null) {
        notFound.add(id);
      }
    }

    if (dryRun) {
      return OccludeResult(
        occludedCount: toInclude.length,
        occludedItems: toInclude,
        notFoundItems: notFound,
        dryRun: true,
      );
    }

    // Actually include the items
    for (final id in toInclude) {
      final item = graph.items[id]!;
      final includedItem = item.copyWith(occlude: false);
      graph.addItem(includedItem);
    }

    return OccludeResult(
      occludedCount: toInclude.length,
      occludedItems: toInclude,
      notFoundItems: notFound,
      dryRun: false,
    );
  }

  /// Load logs from main and occluded files
  static LogCollection loadLogs(String filepath, String occludeFilepath) {
    final logs = LogCollection();
    
    // Load include logs
    logs.addEntries(_loadLogsFromFile(filepath, occlude: false));
    
    // Load occlude logs
    logs.addEntries(_loadLogsFromFile(occludeFilepath, occlude: true));
    
    return logs;
  }

  /// Save logs to files with atomic operations
  static void saveLogs(String filepath, String occludeFilepath, LogCollection logs) {
    try {
      // Prepare file contents
      final includeContent = StringBuffer();
      final occludeContent = StringBuffer();

      // Add headers
      includeContent.writeln(FileHeaderGenerator.generateLogsFileHeader());
      occludeContent.writeln(FileHeaderGenerator.generateOccludedLogsFileHeader());

      // Add logs to appropriate files
      for (final log in logs.entries) {
        final logLine = log.toJsonLine();
        if (log.occlude) {
          occludeContent.writeln(logLine);
        } else {
          includeContent.writeln(logLine);
        }
      }

      // Write both files atomically
      final fileContents = {
        filepath: includeContent.toString(),
        occludeFilepath: occludeContent.toString(),
      };

      AtomicFileWriter.writeFiles(fileContents);

    } catch (e) {
      throw GraphException('Failed to save logs: $e');
    }
  }

  /// Occlude logs by session tags with optional dry-run
  static LogOccludeResult occludeLogs(
    LogCollection logs, 
    List<String> sessionTags, 
    List<String> tags,
    {bool dryRun = false}
  ) {
    final toOcclude = <LogEntry>[];

    for (final log in logs.includedEntries) {
      bool shouldOcclude = false;

      // Check session tags
      if (sessionTags.contains(log.session)) {
        shouldOcclude = true;
      }

      // Check other tags
      if (tags.any((tag) => log.tags.contains(tag))) {
        shouldOcclude = true;
      }

      if (shouldOcclude) {
        toOcclude.add(log);
      }
    }

    if (dryRun) {
      return LogOccludeResult(
        occludedCount: toOcclude.length,
        occludedLogs: toOcclude,
        dryRun: true,
      );
    }

    // Actually occlude the logs
    for (final log in toOcclude) {
      final occludedLog = log.copyWith(occlude: true);
      logs.replaceEntry(log, occludedLog);
    }

    return LogOccludeResult(
      occludedCount: toOcclude.length,
      occludedLogs: toOcclude,
      dryRun: false,
    );
  }

  /// Load logs from a single file
  static List<LogEntry> _loadLogsFromFile(String filepath, {required bool occlude}) {
    final logs = <LogEntry>[];
    
    try {
      final file = File(filepath);
      if (!file.existsSync()) {
        return logs;
      }

      final lines = file.readAsLinesSync();
      for (final line in lines) {
        final trimmed = line.trim();
        if (trimmed.isNotEmpty && !trimmed.startsWith('#')) {
          try {
            final log = LogEntry.fromJsonLine(trimmed, occlude: occlude);
            logs.add(log);
          } catch (e) {
            // Skip invalid lines with warning
            print('Warning: Skipping invalid log line in $filepath: $e');
          }
        }
      }
    } catch (e) {
      throw GraphException('Error loading logs from $filepath: $e');
    }
    
    return logs;
  }
}

/// Result of an occlude operation
class OccludeResult {
  const OccludeResult({
    required this.occludedCount,
    required this.occludedItems,
    required this.notFoundItems,
    required this.dryRun,
  });

  final int occludedCount;
  final List<String> occludedItems;
  final List<String> notFoundItems;
  final bool dryRun;

  bool get hasNotFound => notFoundItems.isNotEmpty;
  bool get hasOccluded => occludedCount > 0;
}

/// Result of a log occlude operation
class LogOccludeResult {
  const LogOccludeResult({
    required this.occludedCount,
    required this.occludedLogs,
    required this.dryRun,
  });

  final int occludedCount;
  final List<LogEntry> occludedLogs;
  final bool dryRun;

  bool get hasOccluded => occludedCount > 0;
}



// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/include_resolver.dart
// =====================================================================



// =====================================================================
// FILE: packages/giantt_core/lib/src/storage/atomic_file_writer.dart
// =====================================================================

import 'dart:io';
import '../models/graph_exceptions.dart';
import 'backup_manager.dart';

/// Provides atomic file write operations with backup and rollback support
class AtomicFileWriter {
  /// Write content to a file atomically using a temporary file
  static void writeFile(String filepath, String content, {bool createBackup = true}) {
    final file = File(filepath);
    final tempPath = '$filepath.tmp';
    final tempFile = File(tempPath);

    try {
      // Create backup if file exists and backup is requested
      if (createBackup && file.existsSync()) {
        BackupManager.createBackup(filepath);
      }

      // Ensure parent directory exists
      final parentDir = file.parent;
      if (!parentDir.existsSync()) {
        parentDir.createSync(recursive: true);
      }

      // Write to temporary file first
      tempFile.writeAsStringSync(content);

      // Atomic move from temp to final location
      // On mobile platforms, this may not be truly atomic, but it's the best we can do
      tempFile.renameSync(filepath);

      // Remove duplicate backup if content is identical
      if (createBackup) {
        BackupManager.removeDuplicateBackup(filepath);
      }

    } catch (e) {
      // Clean up temp file if it exists
      try {
        if (tempFile.existsSync()) {
          tempFile.deleteSync();
        }
      } catch (_) {
        // Ignore cleanup errors
      }
      
      throw GraphException('Failed to write file atomically: $e');
    }
  }

  /// Write multiple files atomically (all succeed or all fail)
  static void writeFiles(Map<String, String> fileContents, {bool createBackup = true}) {
    final tempFiles = <String, String>{};
    final backupPaths = <String, String>{};

    try {
      // Phase 1: Create backups and write to temp files
      for (final entry in fileContents.entries) {
        final filepath = entry.key;
        final content = entry.value;
        final tempPath = '$filepath.tmp';

        // Create backup if file exists
        if (createBackup && File(filepath).existsSync()) {
          backupPaths[filepath] = BackupManager.createBackup(filepath);
        }

        // Ensure parent directory exists
        final parentDir = File(filepath).parent;
        if (!parentDir.existsSync()) {
          parentDir.createSync(recursive: true);
        }

        // Write to temp file
        File(tempPath).writeAsStringSync(content);
        tempFiles[filepath] = tempPath;
      }

      // Phase 2: Atomic moves (all or nothing)
      for (final entry in tempFiles.entries) {
        final filepath = entry.key;
        final tempPath = entry.value;
        File(tempPath).renameSync(filepath);
      }

      // Phase 3: Clean up duplicate backups
      if (createBackup) {
        for (final filepath in fileContents.keys) {
          BackupManager.removeDuplicateBackup(filepath);
        }
      }

    } catch (e) {
      // Rollback: Clean up temp files and restore from backups if needed
      _rollbackTransaction(tempFiles, backupPaths);
      throw GraphException('Failed to write files atomically: $e');
    }
  }

  /// Rollback a failed transaction
  static void _rollbackTransaction(Map<String, String> tempFiles, Map<String, String> backupPaths) {
    // Clean up temp files
    for (final tempPath in tempFiles.values) {
      try {
        final tempFile = File(tempPath);
        if (tempFile.existsSync()) {
          tempFile.deleteSync();
        }
      } catch (_) {
        // Ignore cleanup errors
      }
    }

    // Restore from backups if any files were partially written
    for (final entry in backupPaths.entries) {
      final filepath = entry.key;
      final backupPath = entry.value;
      
      try {
        if (File(backupPath).existsSync()) {
          File(backupPath).copySync(filepath);
        }
      } catch (_) {
        // Best effort restore
      }
    }
  }

  /// Check if a file write operation would be safe (enough disk space, permissions, etc.)
  static bool canWriteFile(String filepath, String content) {
    try {
      final file = File(filepath);
      final parentDir = file.parent;

      // Check if parent directory exists or can be created
      if (!parentDir.existsSync()) {
        try {
          parentDir.createSync(recursive: true);
        } catch (e) {
          return false;
        }
      }

      // Try to write a small test file
      final testPath = '$filepath.test';
      final testFile = File(testPath);
      
      try {
        testFile.writeAsStringSync('test');
        testFile.deleteSync();
        return true;
      } catch (e) {
        return false;
      }

    } catch (e) {
      return false;
    }
  }

  /// Get available disk space (returns null if cannot determine)
  static int? getAvailableDiskSpace(String filepath) {
    try {
      // This is platform-specific and may not work on all mobile platforms
      final file = File(filepath);
      final stat = file.statSync();
      // Note: Dart doesn't provide direct access to disk space info
      // This would need platform-specific implementation for accurate results
      return null;
    } catch (e) {
      return null;
    }
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/init_command.dart
// =====================================================================

import 'dart:io';
import 'command_interface.dart';
import '../storage/path_resolver.dart';
import '../storage/file_header_generator.dart';

/// Arguments for init command
class InitArgs {
  const InitArgs({
    this.force = false,
    this.homeMode = false,
  });

  final bool force;
  final bool homeMode;
}

/// Initialize a new giantt workspace
class InitCommand extends CliCommand<InitArgs> {
  const InitCommand();

  @override
  String get name => 'init';

  @override
  String get description => 'Initialize a new giantt workspace';

  @override
  String get usage => 'init [--force] [--home]';

  @override
  InitArgs parseArgs(List<String> args) {
    bool force = false;
    bool homeMode = false;

    for (final arg in args) {
      switch (arg) {
        case '--force':
        case '-f':
          force = true;
          break;
        case '--home':
        case '-h':
          homeMode = true;
          break;
        default:
          throw ArgumentError('Unknown argument: $arg');
      }
    }

    return InitArgs(force: force, homeMode: homeMode);
  }

  @override
  Future<CommandResult<InitArgs>> execute(CommandContext context) async {
    try {
      final workspacePath = context.workspacePath;
      final workspaceDir = Directory(workspacePath);

      // Check if workspace already exists
      if (workspaceDir.existsSync() && !context.dryRun) {
        final itemsFile = File('${workspacePath}/items.txt');
        if (itemsFile.existsSync()) {
          return CommandResult.failure(
            'Workspace already exists at $workspacePath. Use --force to reinitialize.'
          );
        }
      }

      if (context.dryRun) {
        return CommandResult.message(
          'Would initialize workspace at $workspacePath'
        );
      }

      // Create directory structure
      await _createDirectoryStructure(workspacePath);

      // Create initial files
      await _createInitialFiles(workspacePath);

      return CommandResult.success(
        InitArgs(),
        'Initialized giantt workspace at $workspacePath'
      );

    } catch (e) {
      return CommandResult.failure('Failed to initialize workspace: $e');
    }
  }

  /// Create the directory structure for a giantt workspace
  Future<void> _createDirectoryStructure(String workspacePath) async {
    final directories = [
      workspacePath,
      '$workspacePath/include',
      '$workspacePath/occlude',
    ];

    for (final dir in directories) {
      await Directory(dir).create(recursive: true);
    }
  }

  /// Create initial files with headers
  Future<void> _createInitialFiles(String workspacePath) async {
    // Create items.txt
    final itemsFile = File('$workspacePath/items.txt');
    await itemsFile.writeAsString(
      '${FileHeaderGenerator.generateItemsFileHeader()}\n\n'
    );

    // Create occlude/items.txt
    final occludeItemsFile = File('$workspacePath/occlude/items.txt');
    await occludeItemsFile.writeAsString(
      '${FileHeaderGenerator.generateOccludedItemsFileHeader()}\n\n'
    );

    // Create logs.txt
    final logsFile = File('$workspacePath/logs.txt');
    await logsFile.writeAsString(
      '${FileHeaderGenerator.generateLogsFileHeader()}\n\n'
    );

    // Create occlude/logs.txt
    final occludeLogsFile = File('$workspacePath/occlude/logs.txt');
    await occludeLogsFile.writeAsString(
      '${FileHeaderGenerator.generateOccludedLogsFileHeader()}\n\n'
    );
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<void>> initializeWorkspace(
    String workspacePath, {
    bool force = false,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    final command = InitCommand();
    final result = await command.execute(context);
    
    return CommandResult<void>(
      success: result.success,
      message: result.message,
      error: result.error,
    );
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/includes_command.dart
// =====================================================================

import 'dart:io';
import 'command_interface.dart';
import '../storage/path_resolver.dart';

/// Arguments for includes command
class IncludesArgs {
  const IncludesArgs({
    this.verbose = false,
  });

  final bool verbose;
}

/// Show include structure visualization
class IncludesCommand extends CliCommand<IncludesArgs> {
  const IncludesCommand();

  @override
  String get name => 'includes';

  @override
  String get description => 'Show include structure visualization';

  @override
  String get usage => 'includes [--verbose]';

  @override
  IncludesArgs parseArgs(List<String> args) {
    bool verbose = false;

    for (final arg in args) {
      switch (arg) {
        case '--verbose':
        case '-v':
          verbose = true;
          break;
        default:
          throw ArgumentError('Unknown argument: $arg');
      }
    }

    return IncludesArgs(verbose: verbose);
  }

  @override
  Future<CommandResult<IncludesArgs>> execute(CommandContext context) async {
    try {
      final args = IncludesArgs(); // This will be set by parseArgs in CLI usage

      // Check if workspace exists
      if (!PathResolver.gianttWorkspaceExists(context.workspacePath)) {
        return CommandResult.failure('No giantt workspace found at ${context.workspacePath}');
      }

      // Build include structure starting from main files
      final includeStructure = _buildIncludeStructure(context.itemsPath);
      final occludeStructure = _buildIncludeStructure(context.occludeItemsPath);

      final buffer = StringBuffer();
      
      // Show include file structure
      if (includeStructure.isNotEmpty) {
        buffer.writeln('Include file structure:');
        _formatIncludeTree(buffer, includeStructure, '', <String>{});
        buffer.writeln();
      }

      // Show occlude file structure
      if (occludeStructure.isNotEmpty) {
        buffer.writeln('Occlude file structure:');
        _formatIncludeTree(buffer, occludeStructure, '', <String>{});
        buffer.writeln();
      }

      if (includeStructure.isEmpty && occludeStructure.isEmpty) {
        buffer.writeln('No include directives found in workspace files.');
      }

      // Show additional details if verbose
      if (args.verbose || context.verbose) {
        buffer.writeln('Workspace files:');
        final files = [
          context.itemsPath,
          context.occludeItemsPath,
          context.logsPath,
          context.occludeLogsPath,
        ];

        for (final filepath in files) {
          final file = File(filepath);
          if (file.existsSync()) {
            final includes = _parseIncludeDirectives(filepath);
            buffer.writeln('  $filepath (${includes.length} includes)');
            for (final include in includes) {
              buffer.writeln('    → $include');
            }
          } else {
            buffer.writeln('  $filepath (missing)');
          }
        }
      }

      return CommandResult.success(args, buffer.toString());

    } catch (e) {
      return CommandResult.failure('Failed to show include structure: $e');
    }
  }

  /// Build include structure tree
  Map<String, List<String>> _buildIncludeStructure(String rootFile) {
    final structure = <String, List<String>>{};
    final visited = <String>{};

    void buildTree(String filepath) {
      if (visited.contains(filepath)) return;
      visited.add(filepath);

      final includes = _parseIncludeDirectives(filepath);
      if (includes.isNotEmpty) {
        structure[filepath] = includes;
        for (final include in includes) {
          final resolvedPath = PathResolver.resolvePath(
            PathResolver._getDirectoryPath(filepath),
            include,
          );
          buildTree(resolvedPath);
        }
      }
    }

    buildTree(rootFile);
    return structure;
  }

  /// Parse include directives from a file
  List<String> _parseIncludeDirectives(String filepath) {
    final includes = <String>[];
    
    try {
      final file = File(filepath);
      if (!file.existsSync()) return includes;

      final lines = file.readAsLinesSync();
      
      for (final line in lines) {
        final trimmed = line.trim();
        
        // Stop parsing includes when we hit non-directive content
        if (trimmed.isNotEmpty && !trimmed.startsWith('#')) {
          break;
        }
        
        // Parse include directive
        if (trimmed.startsWith('#include ')) {
          final includePath = trimmed.substring(9).trim();
          includes.add(includePath);
        }
      }
    } catch (e) {
      // Ignore file read errors
    }
    
    return includes;
  }

  /// Format include tree with proper indentation and cycle detection
  void _formatIncludeTree(
    StringBuffer buffer,
    Map<String, List<String>> structure,
    String indent,
    Set<String> currentPath,
  ) {
    for (final entry in structure.entries) {
      final filepath = entry.key;
      final includes = entry.value;
      
      buffer.writeln('${indent}└─ $filepath');
      
      if (currentPath.contains(filepath)) {
        buffer.writeln('$indent  └─ (circular include, skipping)');
        continue;
      }
      
      final newPath = Set<String>.from(currentPath)..add(filepath);
      final newIndent = '$indent  ';
      
      for (final include in includes) {
        final resolvedPath = PathResolver.resolvePath(
          PathResolver._getDirectoryPath(filepath),
          include,
        );
        
        if (structure.containsKey(resolvedPath)) {
          _formatIncludeTree(buffer, {resolvedPath: structure[resolvedPath]!}, newIndent, newPath);
        } else {
          buffer.writeln('${newIndent}└─ $resolvedPath');
        }
      }
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<Map<String, dynamic>>> getIncludeStructure(
    String workspacePath,
  ) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Check if workspace exists
    if (!PathResolver.gianttWorkspaceExists(workspacePath)) {
      return CommandResult.failure('No giantt workspace found at $workspacePath');
    }

    final command = IncludesCommand();
    
    // Build include structures
    final includeStructure = command._buildIncludeStructure(context.itemsPath);
    final occludeStructure = command._buildIncludeStructure(context.occludeItemsPath);

    // Get file details
    final files = [
      context.itemsPath,
      context.occludeItemsPath,
      context.logsPath,
      context.occludeLogsPath,
    ];

    final fileDetails = <String, dynamic>{};
    for (final filepath in files) {
      final file = File(filepath);
      if (file.existsSync()) {
        final includes = command._parseIncludeDirectives(filepath);
        fileDetails[filepath] = {
          'exists': true,
          'includes': includes,
        };
      } else {
        fileDetails[filepath] = {
          'exists': false,
          'includes': <String>[],
        };
      }
    }

    final results = {
      'include_structure': includeStructure,
      'occlude_structure': occludeStructure,
      'file_details': fileDetails,
    };

    return CommandResult.success(results, 'Include structure analyzed');
  }
}

/// Extension to access private method
extension PathResolverExtension on PathResolver {
  static String _getDirectoryPath(String filepath) {
    final lastSeparator = filepath.lastIndexOf(Platform.pathSeparator);
    if (lastSeparator == -1) {
      return '.'; // Current directory
    }
    return filepath.substring(0, lastSeparator);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/occlude_command.dart
// =====================================================================

import 'command_interface.dart';
import '../storage/dual_file_manager.dart';
import '../logging/log_occluder.dart';

/// Arguments for occlude command
class OccludeArgs {
  const OccludeArgs({
    this.itemIds = const [],
    this.tags = const [],
    this.sessionTags = const [],
    this.occludeItems = true,
    this.occludeLogs = false,
    this.dryRun = false,
    this.verbose = false,
  });

  final List<String> itemIds;
  final List<String> tags;
  final List<String> sessionTags;
  final bool occludeItems;
  final bool occludeLogs;
  final bool dryRun;
  final bool verbose;
}

/// Occlude items and/or logs with dry-run support
class OccludeCommand extends CliCommand<OccludeArgs> {
  const OccludeCommand();

  @override
  String get name => 'occlude';

  @override
  String get description => 'Occlude items and/or logs with dry-run support';

  @override
  String get usage => 'occlude [items|logs] [--ids=id1,id2] [--tags=tag1,tag2] [--sessions=s1,s2] [--dry-run] [--verbose]';

  @override
  OccludeArgs parseArgs(List<String> args) {
    List<String> itemIds = [];
    List<String> tags = [];
    List<String> sessionTags = [];
    bool occludeItems = true;
    bool occludeLogs = false;
    bool dryRun = false;
    bool verbose = false;

    for (final arg in args) {
      if (arg == 'items') {
        occludeItems = true;
        occludeLogs = false;
      } else if (arg == 'logs') {
        occludeItems = false;
        occludeLogs = true;
      } else if (arg.startsWith('--ids=')) {
        final idsStr = arg.substring(6);
        itemIds = idsStr.split(',').map((id) => id.trim()).where((id) => id.isNotEmpty).toList();
      } else if (arg.startsWith('--tags=')) {
        final tagsStr = arg.substring(7);
        tags = tagsStr.split(',').map((tag) => tag.trim()).where((tag) => tag.isNotEmpty).toList();
      } else if (arg.startsWith('--sessions=')) {
        final sessionsStr = arg.substring(11);
        sessionTags = sessionsStr.split(',').map((s) => s.trim()).where((s) => s.isNotEmpty).toList();
      } else if (arg == '--dry-run' || arg == '-n') {
        dryRun = true;
      } else if (arg == '--verbose' || arg == '-v') {
        verbose = true;
      } else {
        throw ArgumentError('Unknown argument: $arg');
      }
    }

    return OccludeArgs(
      itemIds: itemIds,
      tags: tags,
      sessionTags: sessionTags,
      occludeItems: occludeItems,
      occludeLogs: occludeLogs,
      dryRun: dryRun,
      verbose: verbose,
    );
  }

  @override
  Future<CommandResult<OccludeArgs>> execute(CommandContext context) async {
    try {
      final args = OccludeArgs(); // This will be set by parseArgs in CLI usage
      final results = <String>[];

      if (args.occludeItems) {
        // Load graph
        context.graph ??= DualFileManager.loadGraph(
          context.itemsPath,
          context.occludeItemsPath,
        );

        OccludeResult? itemResult;

        if (args.itemIds.isNotEmpty) {
          // Occlude by IDs
          itemResult = DualFileManager.occludeItems(
            context.graph!,
            args.itemIds,
            dryRun: context.dryRun || args.dryRun,
          );
        } else if (args.tags.isNotEmpty) {
          // Occlude by tags
          itemResult = DualFileManager.occludeItemsByTags(
            context.graph!,
            args.tags,
            dryRun: context.dryRun || args.dryRun,
          );
        }

        if (itemResult != null) {
          if (itemResult.hasOccluded) {
            final action = itemResult.dryRun ? 'Would occlude' : 'Occluded';
            results.add('$action ${itemResult.occludedCount} items');
            
            if (args.verbose || context.verbose) {
              results.add('  Items: ${itemResult.occludedItems.join(', ')}');
            }
          }

          if (itemResult.hasNotFound) {
            results.add('Items not found: ${itemResult.notFoundItems.join(', ')}');
          }

          // Save graph if not dry run
          if (!itemResult.dryRun) {
            DualFileManager.saveGraph(
              context.itemsPath,
              context.occludeItemsPath,
              context.graph!,
            );
          }
        } else {
          results.add('No items specified for occlusion');
        }
      }

      if (args.occludeLogs) {
        // Load logs
        context.logs ??= DualFileManager.loadLogs(
          context.logsPath,
          context.occludeLogsPath,
        );

        LogOccludeResult? logResult;

        if (args.sessionTags.isNotEmpty || args.tags.isNotEmpty) {
          // Occlude logs by session tags and/or regular tags
          logResult = LogOccluder.occludeBySession(
            context.logs!,
            args.sessionTags,
            dryRun: context.dryRun || args.dryRun,
          );

          // Also occlude by regular tags if specified
          if (args.tags.isNotEmpty) {
            final tagResult = LogOccluder.occludeByTags(
              context.logs!,
              args.tags,
              dryRun: context.dryRun || args.dryRun,
            );
            
            // Combine results (simplified)
            if (logResult.hasOccluded || tagResult.hasOccluded) {
              logResult = LogOccludeResult(
                occludedCount: logResult.occludedCount + tagResult.occludedCount,
                occludedLogs: [...logResult.occludedLogs, ...tagResult.occludedLogs],
                dryRun: logResult.dryRun,
              );
            }
          }
        }

        if (logResult != null) {
          if (logResult.hasOccluded) {
            final action = logResult.dryRun ? 'Would occlude' : 'Occluded';
            results.add('$action ${logResult.occludedCount} log entries');
            
            if (args.verbose || context.verbose) {
              results.add('  Sessions: ${logResult.occludedLogs.map((l) => l.session).toSet().join(', ')}');
            }
          }

          // Save logs if not dry run
          if (!logResult.dryRun) {
            DualFileManager.saveLogs(
              context.logsPath,
              context.occludeLogsPath,
              context.logs!,
            );
          }
        } else {
          results.add('No logs specified for occlusion');
        }
      }

      if (results.isEmpty) {
        return CommandResult.message('Nothing to occlude. Specify items or logs with appropriate filters.');
      }

      return CommandResult.success(args, results.join('\n'));

    } catch (e) {
      return CommandResult.failure('Failed to occlude: $e');
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<Map<String, dynamic>>> occludeContent(
    String workspacePath, {
    List<String> itemIds = const [],
    List<String> tags = const [],
    List<String> sessionTags = const [],
    bool occludeItems = true,
    bool occludeLogs = false,
    bool dryRun = false,
  }) async {
    final context = CommandContext(workspacePath: workspacePath, dryRun: dryRun);
    final results = <String, dynamic>{};

    if (occludeItems) {
      // Load graph
      context.graph = DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      OccludeResult? itemResult;

      if (itemIds.isNotEmpty) {
        itemResult = DualFileManager.occludeItems(
          context.graph!,
          itemIds,
          dryRun: dryRun,
        );
      } else if (tags.isNotEmpty) {
        itemResult = DualFileManager.occludeItemsByTags(
          context.graph!,
          tags,
          dryRun: dryRun,
        );
      }

      if (itemResult != null) {
        results['items'] = {
          'occluded_count': itemResult.occludedCount,
          'occluded_items': itemResult.occludedItems,
          'not_found_items': itemResult.notFoundItems,
          'dry_run': itemResult.dryRun,
        };

        // Save graph if not dry run
        if (!itemResult.dryRun) {
          DualFileManager.saveGraph(
            context.itemsPath,
            context.occludeItemsPath,
            context.graph!,
          );
        }
      }
    }

    if (occludeLogs) {
      // Load logs
      context.logs = DualFileManager.loadLogs(
        context.logsPath,
        context.occludeLogsPath,
      );

      if (sessionTags.isNotEmpty || tags.isNotEmpty) {
        final logResult = LogOccluder.occludeBySession(
          context.logs!,
          sessionTags,
          dryRun: dryRun,
        );

        results['logs'] = {
          'occluded_count': logResult.occludedCount,
          'occluded_sessions': logResult.occludedLogs.map((l) => l.session).toSet().toList(),
          'dry_run': logResult.dryRun,
        };

        // Save logs if not dry run
        if (!logResult.dryRun) {
          DualFileManager.saveLogs(
            context.logsPath,
            context.occludeLogsPath,
            context.logs!,
          );
        }
      }
    }

    final message = dryRun ? 'Analyzed occlusion candidates' : 'Occlusion completed';
    return CommandResult.success(results, message);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/show_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../storage/dual_file_manager.dart';

/// Arguments for show command
class ShowArgs {
  const ShowArgs({
    this.itemId,
    this.substring,
    this.includeOccluded = false,
    this.format = ShowFormat.detailed,
  });

  final String? itemId;
  final String? substring;
  final bool includeOccluded;
  final ShowFormat format;
}

/// Output format for show command
enum ShowFormat {
  detailed,
  brief,
  raw,
}

/// Show items from the graph
class ShowCommand extends CliCommand<ShowArgs> {
  const ShowCommand();

  @override
  String get name => 'show';

  @override
  String get description => 'Show items from the graph';

  @override
  String get usage => 'show [<id_or_substring>] [--occluded] [--brief] [--raw]';

  @override
  ShowArgs parseArgs(List<String> args) {
    String? itemId;
    String? substring;
    bool includeOccluded = false;
    ShowFormat format = ShowFormat.detailed;

    for (int i = 0; i < args.length; i++) {
      final arg = args[i];
      
      if (arg == '--occluded') {
        includeOccluded = true;
      } else if (arg == '--brief') {
        format = ShowFormat.brief;
      } else if (arg == '--raw') {
        format = ShowFormat.raw;
      } else if (!arg.startsWith('--')) {
        // First non-flag argument is the ID or substring
        if (itemId == null && substring == null) {
          // Try to determine if it's an exact ID or substring
          itemId = arg;
          substring = arg;
        }
      }
    }

    return ShowArgs(
      itemId: itemId,
      substring: substring,
      includeOccluded: includeOccluded,
      format: format,
    );
  }

  @override
  Future<CommandResult<ShowArgs>> execute(CommandContext context) async {
    try {
      // Load graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = ShowArgs(); // This will be set by parseArgs in CLI usage
      
      List<GianttItem> itemsToShow = [];

      if (args.itemId != null) {
        // Show specific item or search by substring
        try {
          final item = context.graph!.findBySubstring(args.itemId!);
          itemsToShow = [item];
        } catch (e) {
          // If not found by substring, try exact ID match
          final item = context.graph!.items[args.itemId];
          if (item != null) {
            itemsToShow = [item];
          } else {
            return CommandResult.failure('No item found with ID or substring "${args.itemId}"');
          }
        }
      } else {
        // Show all items
        itemsToShow = context.graph!.items.values.toList();
      }

      // Filter by occlusion status
      if (!args.includeOccluded) {
        itemsToShow = itemsToShow.where((item) => !item.occlude).toList();
      }

      if (itemsToShow.isEmpty) {
        return CommandResult.message('No items to show');
      }

      // Format output
      final output = _formatItems(itemsToShow, args.format);

      return CommandResult.success(args, output);

    } catch (e) {
      return CommandResult.failure('Failed to show items: $e');
    }
  }

  String _formatItems(List<GianttItem> items, ShowFormat format) {
    switch (format) {
      case ShowFormat.raw:
        return items.map((item) => item.toFileString()).join('\n');
      
      case ShowFormat.brief:
        return items.map((item) => 
          '${item.status.symbol} ${item.id}${item.priority.symbol} - ${item.title}'
        ).join('\n');
      
      case ShowFormat.detailed:
        return items.map((item) => _formatItemDetailed(item)).join('\n\n');
    }
  }

  String _formatItemDetailed(GianttItem item) {
    final buffer = StringBuffer();
    
    buffer.writeln('ID: ${item.id}');
    buffer.writeln('Title: ${item.title}');
    buffer.writeln('Status: ${item.status.name} (${item.status.symbol})');
    buffer.writeln('Priority: ${item.priority.name} (${item.priority.symbol})');
    buffer.writeln('Duration: ${item.duration}');
    
    if (item.charts.isNotEmpty) {
      buffer.writeln('Charts: ${item.charts.join(', ')}');
    }
    
    if (item.tags.isNotEmpty) {
      buffer.writeln('Tags: ${item.tags.join(', ')}');
    }
    
    if (item.relations.isNotEmpty) {
      buffer.writeln('Relations:');
      for (final entry in item.relations.entries) {
        buffer.writeln('  ${entry.key}: ${entry.value.join(', ')}');
      }
    }
    
    if (item.timeConstraints.isNotEmpty) {
      buffer.writeln('Time Constraints: ${item.timeConstraints.length}');
    }
    
    if (item.userComment != null) {
      buffer.writeln('Comment: ${item.userComment}');
    }
    
    if (item.occlude) {
      buffer.writeln('Status: OCCLUDED');
    }
    
    return buffer.toString();
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<List<GianttItem>>> getItems(
    String workspacePath, {
    String? itemId,
    String? substring,
    bool includeOccluded = false,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    List<GianttItem> items = [];

    if (itemId != null || substring != null) {
      final searchTerm = itemId ?? substring!;
      try {
        final item = context.graph!.findBySubstring(searchTerm);
        items = [item];
      } catch (e) {
        final item = context.graph!.items[searchTerm];
        if (item != null) {
          items = [item];
        } else {
          return CommandResult.failure('No item found with ID or substring "$searchTerm"');
        }
      }
    } else {
      items = context.graph!.items.values.toList();
    }

    // Filter by occlusion status
    if (!includeOccluded) {
      items = items.where((item) => !item.occlude).toList();
    }

    return CommandResult.success(items);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/command_interface.dart
// =====================================================================

import 'dart:io';
import 'package:args/args.dart';
import '../models/giantt_item.dart';
import '../models/log_entry.dart';
import '../graph/giantt_graph.dart';
import '../logging/log_collection.dart';

/// Base interface for all CLI commands
abstract class Command {
  String get name;
  String get description;
  ArgParser get argParser;
  
  Future<void> execute(ArgResults args);
}

/// Result of a command execution
class CommandResult<T> {
  const CommandResult({
    required this.success,
    this.data,
    this.message,
    this.error,
  });

  final bool success;
  final T? data;
  final String? message;
  final String? error;

  factory CommandResult.success(T data, [String? message]) {
    return CommandResult(success: true, data: data, message: message);
  }

  factory CommandResult.failure(String error) {
    return CommandResult(success: false, error: error);
  }

  factory CommandResult.message(String message) {
    return CommandResult(success: true, message: message);
  }
}

/// Exception thrown when a command fails
class CommandException implements Exception {
  const CommandException(this.message, [this.exitCode = 1]);
  
  final String message;
  final int exitCode;
  
  @override
  String toString() => message;
}

/// Utility functions for CLI commands
class CommandUtils {
  /// Get the default path for Giantt files
  static String getDefaultGianttPath({String filename = 'items.txt', bool occlude = false}) {
    final filepath = '${occlude ? 'occlude' : 'include'}/$filename';
    
    // First check for local .giantt directory
    final localPath = Directory('.giantt/$filepath');
    if (localPath.existsSync()) {
      return localPath.path;
    }
    
    // Fall back to home directory
    final homeDir = Platform.environment['HOME'] ?? Platform.environment['USERPROFILE'];
    if (homeDir != null) {
      final homePath = Directory('$homeDir/.giantt/$filepath');
      if (homePath.existsSync()) {
        return homePath.path;
      }
    }
    
    // If neither exists, throw an error
    throw CommandException(
      'No Giantt $filepath found. Please run \'giantt init\' or \'giantt init --dev\' first.'
    );
  }
  
  /// Confirm an action with the user
  static bool confirm(String message, {bool defaultValue = false}) {
    stdout.write('$message ${defaultValue ? '[Y/n]' : '[y/N]'}: ');
    final input = stdin.readLineSync()?.trim().toLowerCase() ?? '';
    
    if (input.isEmpty) return defaultValue;
    return input == 'y' || input == 'yes';
  }
  
  /// Print an error message to stderr
  static void printError(String message) {
    stderr.writeln('Error: $message');
  }
  
  /// Print a warning message
  static void printWarning(String message) {
    stderr.writeln('Warning: $message');
  }
  
  /// Print a success message
  static void printSuccess(String message) {
    print(message);
  }
}

/// Context for command execution
class CommandContext {
  CommandContext({
    required this.workspacePath,
    this.graph,
    this.logs,
    this.dryRun = false,
    this.verbose = false,
  });

  final String workspacePath;
  GianttGraph? graph;
  LogCollection? logs;
  final bool dryRun;
  final bool verbose;

  String get itemsPath => '$workspacePath/items.txt';
  String get occludeItemsPath => '$workspacePath/occlude/items.txt';
  String get logsPath => '$workspacePath/logs.txt';
  String get occludeLogsPath => '$workspacePath/occlude/logs.txt';
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/sort_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../models/graph_exceptions.dart';
import '../storage/dual_file_manager.dart';

/// Arguments for sort command
class SortArgs {
  const SortArgs({
    this.dryRun = false,
    this.verbose = false,
  });

  final bool dryRun;
  final bool verbose;
}

/// Sort items using topological sort with cycle detection
class SortCommand extends CliCommand<SortArgs> {
  const SortCommand();

  @override
  String get name => 'sort';

  @override
  String get description => 'Sort items using topological sort with cycle detection';

  @override
  String get usage => 'sort [--dry-run] [--verbose]';

  @override
  SortArgs parseArgs(List<String> args) {
    bool dryRun = false;
    bool verbose = false;

    for (final arg in args) {
      switch (arg) {
        case '--dry-run':
        case '-n':
          dryRun = true;
          break;
        case '--verbose':
        case '-v':
          verbose = true;
          break;
        default:
          throw ArgumentError('Unknown argument: $arg');
      }
    }

    return SortArgs(dryRun: dryRun, verbose: verbose);
  }

  @override
  Future<CommandResult<SortArgs>> execute(CommandContext context) async {
    try {
      // Load graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = SortArgs(); // This will be set by parseArgs in CLI usage

      // Perform topological sort
      List<GianttItem> sortedItems;
      try {
        sortedItems = context.graph!.topologicalSort();
      } catch (e) {
        if (e is CycleDetectedException) {
          return CommandResult.failure(
            'Cannot sort: ${e.toString()}\n'
            'Please resolve the cycle before sorting.'
          );
        }
        rethrow;
      }

      if (context.dryRun || args.dryRun) {
        final buffer = StringBuffer();
        buffer.writeln('Would sort ${sortedItems.length} items:');
        for (int i = 0; i < sortedItems.length; i++) {
          final item = sortedItems[i];
          buffer.writeln('${i + 1}. ${item.status.symbol} ${item.id} - ${item.title}');
        }
        return CommandResult.message(buffer.toString());
      }

      // Save the sorted graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );

      final message = args.verbose 
        ? 'Sorted ${sortedItems.length} items successfully:\n${_formatSortedItems(sortedItems)}'
        : 'Sorted ${sortedItems.length} items successfully';

      return CommandResult.success(args, message);

    } catch (e) {
      return CommandResult.failure('Failed to sort items: $e');
    }
  }

  String _formatSortedItems(List<GianttItem> items) {
    final buffer = StringBuffer();
    for (int i = 0; i < items.length; i++) {
      final item = items[i];
      buffer.writeln('${i + 1}. ${item.status.symbol} ${item.id} - ${item.title}');
    }
    return buffer.toString();
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<List<GianttItem>>> sortItems(
    String workspacePath, {
    bool dryRun = false,
  }) async {
    final context = CommandContext(workspacePath: workspacePath, dryRun: dryRun);
    
    // Load graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    // Perform topological sort
    List<GianttItem> sortedItems;
    try {
      sortedItems = context.graph!.topologicalSort();
    } catch (e) {
      if (e is CycleDetectedException) {
        return CommandResult.failure('Cannot sort: ${e.toString()}');
      }
      return CommandResult.failure('Failed to sort: $e');
    }

    if (!dryRun) {
      // Save the sorted graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );
    }

    return CommandResult.success(sortedItems, 'Sorted ${sortedItems.length} items successfully');
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/insert_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../models/status.dart';
import '../models/priority.dart';
import '../models/duration.dart';
import '../storage/dual_file_manager.dart';

/// Arguments for insert command
class InsertArgs {
  const InsertArgs({
    required this.newId,
    required this.title,
    required this.beforeId,
    required this.afterId,
    this.status = GianttStatus.notStarted,
    this.priority = GianttPriority.neutral,
    this.duration,
  });

  final String newId;
  final String title;
  final String beforeId;
  final String afterId;
  final GianttStatus status;
  final GianttPriority priority;
  final GianttDuration? duration;
}

/// Insert a new item between two existing items in the dependency chain
class InsertCommand extends CliCommand<InsertArgs> {
  const InsertCommand();

  @override
  String get name => 'insert';

  @override
  String get description => 'Insert a new item between two existing items in the dependency chain';

  @override
  String get usage => 'insert <new_id> "<title>" <before_id> <after_id> [options]';

  @override
  InsertArgs parseArgs(List<String> args) {
    if (args.length < 4) {
      throw ArgumentError('insert requires new_id, title, before_id, and after_id');
    }

    final newId = args[0];
    final title = args[1];
    final beforeId = args[2];
    final afterId = args[3];

    // Parse optional arguments
    GianttStatus status = GianttStatus.notStarted;
    GianttPriority priority = GianttPriority.neutral;
    GianttDuration? duration;

    for (int i = 4; i < args.length; i++) {
      final arg = args[i];
      
      if (arg.startsWith('--status=')) {
        final statusStr = arg.substring(9);
        status = GianttStatus.fromSymbol(statusStr);
      } else if (arg.startsWith('--priority=')) {
        final priorityStr = arg.substring(11);
        priority = GianttPriority.fromSymbol(priorityStr);
      } else if (arg.startsWith('--duration=')) {
        final durationStr = arg.substring(11);
        duration = GianttDuration.parse(durationStr);
      }
    }

    return InsertArgs(
      newId: newId,
      title: title,
      beforeId: beforeId,
      afterId: afterId,
      status: status,
      priority: priority,
      duration: duration,
    );
  }

  @override
  Future<CommandResult<InsertArgs>> execute(CommandContext context) async {
    try {
      // Load graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = InsertArgs(
        newId: 'temp',
        title: 'temp',
        beforeId: 'temp',
        afterId: 'temp',
      ); // This will be set by parseArgs in CLI usage

      // Check if new item ID already exists
      if (context.graph!.items.containsKey(args.newId)) {
        return CommandResult.failure('Item with ID "${args.newId}" already exists');
      }

      // Check if before and after items exist
      if (!context.graph!.items.containsKey(args.beforeId)) {
        return CommandResult.failure('Before item "${args.beforeId}" not found');
      }
      if (!context.graph!.items.containsKey(args.afterId)) {
        return CommandResult.failure('After item "${args.afterId}" not found');
      }

      // Create new item
      final newItem = GianttItem(
        id: args.newId,
        title: args.title,
        status: args.status,
        priority: args.priority,
        duration: args.duration ?? GianttDuration.zero(),
        charts: [],
        tags: [],
        relations: {},
        timeConstraints: const [],
        userComment: null,
        autoComment: null,
        occlude: false,
      );

      if (context.dryRun) {
        return CommandResult.message(
          'Would insert item "${args.newId}" between "${args.beforeId}" and "${args.afterId}":\n'
          '${newItem.toFileString()}'
        );
      }

      // Insert the item between the two existing items
      context.graph!.insertBetween(newItem, args.beforeId, args.afterId);

      // Save graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );

      return CommandResult.success(
        args,
        'Inserted item "${args.newId}" between "${args.beforeId}" and "${args.afterId}"'
      );

    } catch (e) {
      return CommandResult.failure('Failed to insert item: $e');
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<GianttItem>> insertItem(
    String workspacePath,
    String newId,
    String title,
    String beforeId,
    String afterId, {
    GianttStatus status = GianttStatus.notStarted,
    GianttPriority priority = GianttPriority.neutral,
    GianttDuration? duration,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    // Check if new item ID already exists
    if (context.graph!.items.containsKey(newId)) {
      return CommandResult.failure('Item with ID "$newId" already exists');
    }

    // Check if before and after items exist
    if (!context.graph!.items.containsKey(beforeId)) {
      return CommandResult.failure('Before item "$beforeId" not found');
    }
    if (!context.graph!.items.containsKey(afterId)) {
      return CommandResult.failure('After item "$afterId" not found');
    }

    // Create new item
    final newItem = GianttItem(
      id: newId,
      title: title,
      status: status,
      priority: priority,
      duration: duration ?? GianttDuration.zero(),
      charts: [],
      tags: [],
      relations: {},
      timeConstraints: const [],
      userComment: null,
      autoComment: null,
      occlude: false,
    );

    // Insert the item between the two existing items
    context.graph!.insertBetween(newItem, beforeId, afterId);

    // Save graph
    DualFileManager.saveGraph(
      context.itemsPath,
      context.occludeItemsPath,
      context.graph!,
    );

    return CommandResult.success(
      newItem,
      'Inserted item "$newId" between "$beforeId" and "$afterId"'
    );
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/remove_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../storage/dual_file_manager.dart';

/// Arguments for remove command
class RemoveArgs {
  const RemoveArgs({
    required this.itemId,
    this.force = false,
  });

  final String itemId;
  final bool force;
}

/// Remove an item from the graph
class RemoveCommand extends CliCommand<RemoveArgs> {
  const RemoveCommand();

  @override
  String get name => 'remove';

  @override
  String get description => 'Remove an item from the graph';

  @override
  String get usage => 'remove <id> [--force]';

  @override
  RemoveArgs parseArgs(List<String> args) {
    if (args.isEmpty) {
      throw ArgumentError('remove requires an item ID');
    }

    final itemId = args[0];
    bool force = false;

    for (int i = 1; i < args.length; i++) {
      final arg = args[i];
      if (arg == '--force' || arg == '-f') {
        force = true;
      }
    }

    return RemoveArgs(itemId: itemId, force: force);
  }

  @override
  Future<CommandResult<RemoveArgs>> execute(CommandContext context) async {
    try {
      // Load existing graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = RemoveArgs(itemId: 'temp'); // This will be set by parseArgs in CLI usage

      // Find the item
      final itemToRemove = context.graph!.items[args.itemId];
      if (itemToRemove == null) {
        return CommandResult.failure('Item with ID "${args.itemId}" not found');
      }

      // Check for dependencies unless force is used
      if (!args.force) {
        final dependentItems = _findDependentItems(context.graph!, args.itemId);
        if (dependentItems.isNotEmpty) {
          final dependentIds = dependentItems.map((item) => item.id).join(', ');
          return CommandResult.failure(
            'Cannot remove item "${args.itemId}" because it is required by: $dependentIds. '
            'Use --force to remove anyway.'
          );
        }
      }

      if (context.dryRun) {
        return CommandResult.message(
          'Would remove item "${args.itemId}" and clean up ${_countRelationReferences(context.graph!, args.itemId)} relation references'
        );
      }

      // Remove the item
      context.graph!.items.remove(args.itemId);

      // Clean up relations that reference this item
      _cleanupRelationReferences(context.graph!, args.itemId);

      // Save graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );

      return CommandResult.success(
        args,
        'Removed item "${args.itemId}" successfully'
      );

    } catch (e) {
      return CommandResult.failure('Failed to remove item: $e');
    }
  }

  /// Find items that depend on the given item
  List<GianttItem> _findDependentItems(GianttGraph graph, String itemId) {
    final dependentItems = <GianttItem>[];

    for (final item in graph.items.values) {
      // Check if this item requires the item we want to remove
      final requires = item.relations['REQUIRES'] ?? [];
      if (requires.contains(itemId)) {
        dependentItems.add(item);
      }

      // Check if this item has any-of relation with the item we want to remove
      final anyOf = item.relations['ANYOF'] ?? [];
      if (anyOf.contains(itemId)) {
        dependentItems.add(item);
      }
    }

    return dependentItems;
  }

  /// Count how many relation references would be cleaned up
  int _countRelationReferences(GianttGraph graph, String itemId) {
    int count = 0;

    for (final item in graph.items.values) {
      for (final relations in item.relations.values) {
        if (relations.contains(itemId)) {
          count++;
        }
      }
    }

    return count;
  }

  /// Clean up all relation references to the removed item
  void _cleanupRelationReferences(GianttGraph graph, String itemId) {
    for (final item in graph.items.values) {
      bool modified = false;
      final newRelations = <String, List<String>>{};

      for (final entry in item.relations.entries) {
        final relType = entry.key;
        final targets = entry.value;
        final cleanedTargets = targets.where((target) => target != itemId).toList();
        
        if (cleanedTargets.length != targets.length) {
          modified = true;
        }
        
        if (cleanedTargets.isNotEmpty) {
          newRelations[relType] = cleanedTargets;
        }
      }

      if (modified) {
        final updatedItem = item.copyWith(relations: newRelations);
        graph.addItem(updatedItem);
      }
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<void>> removeItem(
    String workspacePath,
    String itemId, {
    bool force = false,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load existing graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    // Find the item
    final itemToRemove = context.graph!.items[itemId];
    if (itemToRemove == null) {
      return CommandResult.failure('Item with ID "$itemId" not found');
    }

    final command = RemoveCommand();

    // Check for dependencies unless force is used
    if (!force) {
      final dependentItems = command._findDependentItems(context.graph!, itemId);
      if (dependentItems.isNotEmpty) {
        final dependentIds = dependentItems.map((item) => item.id).join(', ');
        return CommandResult.failure(
          'Cannot remove item "$itemId" because it is required by: $dependentIds. '
          'Use force=true to remove anyway.'
        );
      }
    }

    // Remove the item
    context.graph!.items.remove(itemId);

    // Clean up relations that reference this item
    command._cleanupRelationReferences(context.graph!, itemId);

    // Save graph
    DualFileManager.saveGraph(
      context.itemsPath,
      context.occludeItemsPath,
      context.graph!,
    );

    return CommandResult.success(null, 'Removed item "$itemId" successfully');
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/clean_command.dart
// =====================================================================

import 'command_interface.dart';
import '../storage/backup_manager.dart';
import '../storage/path_resolver.dart';

/// Arguments for clean command
class CleanArgs {
  const CleanArgs({
    this.retentionCount = 3,
    this.dryRun = false,
    this.verbose = false,
  });

  final int retentionCount;
  final bool dryRun;
  final bool verbose;
}

/// Clean up backup files with configurable retention
class CleanCommand extends CliCommand<CleanArgs> {
  const CleanCommand();

  @override
  String get name => 'clean';

  @override
  String get description => 'Clean up backup files with configurable retention';

  @override
  String get usage => 'clean [--keep=N] [--dry-run] [--verbose]';

  @override
  CleanArgs parseArgs(List<String> args) {
    int retentionCount = 3;
    bool dryRun = false;
    bool verbose = false;

    for (final arg in args) {
      if (arg.startsWith('--keep=')) {
        final countStr = arg.substring(7);
        retentionCount = int.tryParse(countStr) ?? 3;
      } else if (arg == '--dry-run' || arg == '-n') {
        dryRun = true;
      } else if (arg == '--verbose' || arg == '-v') {
        verbose = true;
      } else {
        throw ArgumentError('Unknown argument: $arg');
      }
    }

    return CleanArgs(
      retentionCount: retentionCount,
      dryRun: dryRun,
      verbose: verbose,
    );
  }

  @override
  Future<CommandResult<CleanArgs>> execute(CommandContext context) async {
    try {
      final args = CleanArgs(); // This will be set by parseArgs in CLI usage

      // Check if workspace exists
      if (!PathResolver.gianttWorkspaceExists(context.workspacePath)) {
        return CommandResult.failure('No giantt workspace found at ${context.workspacePath}');
      }

      // Files to clean backups for
      final filesToClean = [
        context.itemsPath,
        context.occludeItemsPath,
        context.logsPath,
        context.occludeLogsPath,
      ];

      final results = <String>[];
      int totalBackupsFound = 0;
      int totalBackupsToRemove = 0;

      // Analyze what would be cleaned
      for (final filepath in filesToClean) {
        final allBackups = BackupManager.getAllBackups(filepath);
        totalBackupsFound += allBackups.length;
        
        if (allBackups.length > args.retentionCount) {
          final toRemove = allBackups.length - args.retentionCount;
          totalBackupsToRemove += toRemove;
          
          if (args.verbose || context.verbose) {
            results.add('$filepath: ${allBackups.length} backups, would remove $toRemove');
          }
        } else if (args.verbose || context.verbose) {
          results.add('$filepath: ${allBackups.length} backups, none to remove');
        }
      }

      if (context.dryRun || args.dryRun) {
        final message = StringBuffer();
        message.writeln('Would clean $totalBackupsToRemove of $totalBackupsFound backup files (keeping ${args.retentionCount} most recent)');
        if (results.isNotEmpty) {
          message.writeln('Details:');
          message.writeln(results.join('\n'));
        }
        return CommandResult.message(message.toString());
      }

      // Actually clean the backups
      BackupManager.cleanupAllBackups(filesToClean, retentionCount: args.retentionCount);

      final message = args.verbose || context.verbose
        ? 'Cleaned $totalBackupsToRemove of $totalBackupsFound backup files:\n${results.join('\n')}'
        : 'Cleaned $totalBackupsToRemove of $totalBackupsFound backup files (kept ${args.retentionCount} most recent)';

      return CommandResult.success(args, message);

    } catch (e) {
      return CommandResult.failure('Failed to clean backup files: $e');
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<Map<String, int>>> cleanBackups(
    String workspacePath, {
    int retentionCount = 3,
    bool dryRun = false,
  }) async {
    final context = CommandContext(workspacePath: workspacePath, dryRun: dryRun);
    
    // Check if workspace exists
    if (!PathResolver.gianttWorkspaceExists(workspacePath)) {
      return CommandResult.failure('No giantt workspace found at $workspacePath');
    }

    // Files to clean backups for
    final filesToClean = [
      context.itemsPath,
      context.occludeItemsPath,
      context.logsPath,
      context.occludeLogsPath,
    ];

    int totalBackupsFound = 0;
    int totalBackupsToRemove = 0;
    final fileStats = <String, int>{};

    // Analyze what would be cleaned
    for (final filepath in filesToClean) {
      final allBackups = BackupManager.getAllBackups(filepath);
      totalBackupsFound += allBackups.length;
      
      if (allBackups.length > retentionCount) {
        final toRemove = allBackups.length - retentionCount;
        totalBackupsToRemove += toRemove;
        fileStats[filepath] = toRemove;
      } else {
        fileStats[filepath] = 0;
      }
    }

    if (!dryRun) {
      // Actually clean the backups
      BackupManager.cleanupAllBackups(filesToClean, retentionCount: retentionCount);
    }

    final results = {
      'total_found': totalBackupsFound,
      'total_removed': totalBackupsToRemove,
      'retention_count': retentionCount,
      'file_stats': fileStats,
    };

    final message = dryRun 
      ? 'Would clean $totalBackupsToRemove of $totalBackupsFound backup files'
      : 'Cleaned $totalBackupsToRemove of $totalBackupsFound backup files';

    return CommandResult.success(results, message);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/set_status_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../models/status.dart';
import '../storage/dual_file_manager.dart';

/// Arguments for set-status command
class SetStatusArgs {
  const SetStatusArgs({
    required this.itemId,
    required this.status,
  });

  final String itemId;
  final GianttStatus status;
}

/// Set the status of an item
class SetStatusCommand extends CliCommand<SetStatusArgs> {
  const SetStatusCommand();

  @override
  String get name => 'set-status';

  @override
  String get description => 'Set the status of an item';

  @override
  String get usage => 'set-status <id> <status>';

  @override
  SetStatusArgs parseArgs(List<String> args) {
    if (args.length < 2) {
      throw ArgumentError('set-status requires item ID and status');
    }

    final itemId = args[0];
    final statusStr = args[1];
    
    GianttStatus status;
    try {
      // Try parsing as symbol first
      status = GianttStatus.fromSymbol(statusStr);
    } catch (e) {
      try {
        // Try parsing as name
        status = GianttStatus.fromName(statusStr.toUpperCase());
      } catch (e) {
        throw ArgumentError('Invalid status: $statusStr. Valid statuses: ○ (not_started), ◑ (in_progress), ⊘ (blocked), ● (completed)');
      }
    }

    return SetStatusArgs(itemId: itemId, status: status);
  }

  @override
  Future<CommandResult<SetStatusArgs>> execute(CommandContext context) async {
    try {
      // Load existing graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = SetStatusArgs(itemId: 'temp', status: GianttStatus.notStarted); // This will be set by parseArgs in CLI usage

      // Find the item
      final existingItem = context.graph!.items[args.itemId];
      if (existingItem == null) {
        return CommandResult.failure('Item with ID "${args.itemId}" not found');
      }

      if (context.dryRun) {
        return CommandResult.message(
          'Would set status of "${args.itemId}" from ${existingItem.status.symbol} to ${args.status.symbol}'
        );
      }

      // Update status
      final updatedItem = existingItem.copyWith(status: args.status);
      context.graph!.addItem(updatedItem);

      // Save graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );

      return CommandResult.success(
        args,
        'Set status of "${args.itemId}" to ${args.status.name} (${args.status.symbol})'
      );

    } catch (e) {
      return CommandResult.failure('Failed to set status: $e');
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<GianttItem>> setItemStatus(
    String workspacePath,
    String itemId,
    GianttStatus status,
  ) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load existing graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    // Find the item
    final existingItem = context.graph!.items[itemId];
    if (existingItem == null) {
      return CommandResult.failure('Item with ID "$itemId" not found');
    }

    // Update status
    final updatedItem = existingItem.copyWith(status: status);
    context.graph!.addItem(updatedItem);

    // Save graph
    DualFileManager.saveGraph(
      context.itemsPath,
      context.occludeItemsPath,
      context.graph!,
    );

    return CommandResult.success(
      updatedItem,
      'Set status of "$itemId" to ${status.name} (${status.symbol})'
    );
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/add_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../models/status.dart';
import '../models/priority.dart';
import '../models/duration.dart';
import '../models/time_constraint.dart';
import '../storage/dual_file_manager.dart';
import '../parser/giantt_parser.dart';
import '../graph/giantt_graph.dart';
import '../models/graph_exceptions.dart';

/// Arguments for add command
class AddArgs {
  const AddArgs({
    required this.id,
    required this.title,
    this.status = GianttStatus.notStarted,
    this.priority = GianttPriority.neutral,
    this.duration,
    this.charts = const [],
    this.tags = const [],
    this.relations = const {},
    this.timeConstraints = const [],
  });

  final String id;
  final String title;
  final GianttStatus status;
  final GianttPriority priority;
  final GianttDuration? duration;
  final List<String> charts;
  final List<String> tags;
  final Map<String, List<String>> relations;
  final List<TimeConstraint> timeConstraints;
}

/// Add a new item to the graph
class AddCommand extends CliCommand<AddArgs> {
  const AddCommand();

  @override
  String get name => 'add';

  @override
  String get description => 'Add a new item to the graph';

  @override
  String get usage => 'add <id> "<title>" [options]';

  @override
  AddArgs parseArgs(List<String> args) {
    if (args.length < 2) {
      throw ArgumentError('add requires at least id and title');
    }

    final id = args[0];
    final title = args[1];
    
    // Parse optional arguments
    GianttStatus status = GianttStatus.notStarted;
    GianttPriority priority = GianttPriority.neutral;
    GianttDuration? duration;
    List<String> charts = [];
    List<String> tags = [];
    Map<String, List<String>> relations = {};
    List<TimeConstraint> timeConstraints = [];

    for (int i = 2; i < args.length; i++) {
      final arg = args[i];
      
      if (arg.startsWith('--status=')) {
        final statusStr = arg.substring(9);
        status = GianttStatus.fromSymbol(statusStr);
      } else if (arg.startsWith('--priority=')) {
        final priorityStr = arg.substring(11);
        priority = GianttPriority.fromSymbol(priorityStr);
      } else if (arg.startsWith('--duration=')) {
        final durationStr = arg.substring(11);
        duration = GianttDuration.parse(durationStr);
      } else if (arg.startsWith('--charts=')) {
        final chartsStr = arg.substring(9);
        charts = chartsStr.split(',').map((c) => c.trim()).toList();
      } else if (arg.startsWith('--tags=')) {
        final tagsStr = arg.substring(7);
        tags = tagsStr.split(',').map((t) => t.trim()).toList();
      } else if (arg.startsWith('--requires=')) {
        final requiresStr = arg.substring(11);
        relations['REQUIRES'] = requiresStr.split(',').map((r) => r.trim()).toList();
      } else if (arg.startsWith('--blocks=')) {
        final blocksStr = arg.substring(9);
        relations['BLOCKS'] = blocksStr.split(',').map((b) => b.trim()).toList();
      } else if (arg.startsWith('--constraints=')) {
        final constraintsStr = arg.substring(14);
        for (final constraintStr in constraintsStr.split(' ')) {
          if (constraintStr.trim().isNotEmpty) {
            timeConstraints.add(TimeConstraint.parse(constraintStr.trim()));
          }
        }
      }
    }

    return AddArgs(
      id: id,
      title: title,
      status: status,
      priority: priority,
      duration: duration,
      charts: charts,
      tags: tags,
      relations: relations,
      timeConstraints: timeConstraints,
    );
  }

  @override
  Future<CommandResult<AddArgs>> execute(CommandContext context) async {
    try {
      // Load existing graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = AddArgs(
        id: 'temp_id', // This will be set by parseArgs in CLI usage
        title: 'temp_title',
      );

      // Validate ID is unique and doesn't conflict with titles (matching Python logic)
      if (context.graph!.items.containsKey(args.id)) {
        final existingItem = context.graph!.items[args.id]!;
        return CommandResult.failure('Item with ID "${args.id}" already exists\nExisting item: ${existingItem.id} - ${existingItem.title}');
      }
      
      // Check if ID conflicts with any existing item titles
      for (final item in context.graph!.items.values) {
        if (args.id.toLowerCase() == item.title.toLowerCase()) {
          return CommandResult.failure('Item ID "${args.id}" conflicts with title of another item\nConflicting item: ${item.id} - ${item.title}');
        }
        if (item.title.toLowerCase().contains(args.id.toLowerCase())) {
          return CommandResult.failure('Item ID "${args.id}" conflicts with title of another item\nConflicting item: ${item.id} - ${item.title}');
        }
      }
      
      // Check if title conflicts with any existing item titles
      for (final item in context.graph!.items.values) {
        if (args.title.toLowerCase() == item.title.toLowerCase()) {
          return CommandResult.failure('Title "${args.title}" conflicts with title of another item\nConflicting item: ${item.id} - ${item.title}');
        }
        if (item.title.toLowerCase().contains(args.title.toLowerCase()) || 
            args.title.toLowerCase().contains(item.title.toLowerCase())) {
          return CommandResult.failure('Title "${args.title}" conflicts with title of another item\nConflicting item: ${item.id} - ${item.title}');
        }
      }
      
      // Validate that all referenced items exist
      for (final entry in args.relations.entries) {
        for (final targetId in entry.value) {
          if (!context.graph!.items.containsKey(targetId)) {
            return CommandResult.failure('Referenced item "$targetId" does not exist');
          }
        }
      }

      // Create new item
      final newItem = GianttItem(
        id: args.id,
        title: args.title,
        status: args.status,
        priority: args.priority,
        duration: args.duration ?? GianttDuration.zero(),
        charts: args.charts,
        tags: args.tags,
        relations: args.relations,
        timeConstraints: args.timeConstraints,
        userComment: null,
        autoComment: null,
        occlude: false,
      );

      if (context.dryRun) {
        return CommandResult.message(
          'Would add item: ${newItem.toFileString()}'
        );
      }

      // Add to graph
      context.graph!.addItem(newItem);

      // Check for cycles after adding the item
      try {
        context.graph!.topologicalSort();
      } on CycleDetectedException catch (e) {
        // Remove the item we just added since it creates a cycle
        context.graph!.items.remove(args.id);
        return CommandResult.failure('Adding this item would create a dependency cycle: ${e.cycleItems.join(' -> ')}');
      }

      // Save graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );

      return CommandResult.success(
        args,
        'Added item "${args.id}" successfully'
      );

    } catch (e) {
      return CommandResult.failure('Failed to add item: $e');
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<GianttItem>> addItem(
    String workspacePath,
    String id,
    String title, {
    GianttStatus status = GianttStatus.notStarted,
    GianttPriority priority = GianttPriority.neutral,
    GianttDuration? duration,
    List<String> charts = const [],
    List<String> tags = const [],
    Map<String, List<String>> relations = const {},
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load existing graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    // Check if item already exists
    if (context.graph!.items.containsKey(id)) {
      return CommandResult.failure('Item with ID "$id" already exists');
    }

    // Create new item
    final newItem = GianttItem(
      id: id,
      title: title,
      status: status,
      priority: priority,
      duration: duration ?? GianttDuration.zero(),
      charts: charts,
      tags: tags,
      relations: relations,
      timeConstraints: const [],
      userComment: null,
      autoComment: null,
      occlude: false,
    );

    // Add to graph
    context.graph!.addItem(newItem);

    // Save graph
    DualFileManager.saveGraph(
      context.itemsPath,
      context.occludeItemsPath,
      context.graph!,
    );

    return CommandResult.success(newItem, 'Added item "$id" successfully');
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/touch_command.dart
// =====================================================================

import 'dart:io';
import 'command_interface.dart';
import '../storage/dual_file_manager.dart';
import '../storage/path_resolver.dart';

/// Arguments for touch command
class TouchArgs {
  const TouchArgs({
    this.validate = true,
    this.verbose = false,
  });

  final bool validate;
  final bool verbose;
}

/// Reload files and check consistency
class TouchCommand extends CliCommand<TouchArgs> {
  const TouchCommand();

  @override
  String get name => 'touch';

  @override
  String get description => 'Reload files and check consistency';

  @override
  String get usage => 'touch [--no-validate] [--verbose]';

  @override
  TouchArgs parseArgs(List<String> args) {
    bool validate = true;
    bool verbose = false;

    for (final arg in args) {
      switch (arg) {
        case '--no-validate':
          validate = false;
          break;
        case '--verbose':
        case '-v':
          verbose = true;
          break;
        default:
          throw ArgumentError('Unknown argument: $arg');
      }
    }

    return TouchArgs(validate: validate, verbose: verbose);
  }

  @override
  Future<CommandResult<TouchArgs>> execute(CommandContext context) async {
    try {
      final args = TouchArgs(); // This will be set by parseArgs in CLI usage
      final results = <String>[];

      // Check if workspace exists
      if (!PathResolver.gianttWorkspaceExists(context.workspacePath)) {
        return CommandResult.failure('No giantt workspace found at ${context.workspacePath}');
      }

      // Check file existence and accessibility
      final filesToCheck = [
        context.itemsPath,
        context.occludeItemsPath,
        context.logsPath,
        context.occludeLogsPath,
      ];

      for (final filepath in filesToCheck) {
        final file = File(filepath);
        if (file.existsSync()) {
          try {
            // Try to read the file to check accessibility
            final content = file.readAsStringSync();
            final lines = content.split('\n').length;
            results.add('✓ $filepath ($lines lines)');
          } catch (e) {
            results.add('✗ $filepath (read error: $e)');
          }
        } else {
          results.add('? $filepath (missing)');
        }
      }

      // Reload graph to check parsing
      if (args.validate) {
        try {
          final graph = DualFileManager.loadGraph(
            context.itemsPath,
            context.occludeItemsPath,
          );
          
          final itemCount = graph.items.length;
          final includedCount = graph.includedItems.length;
          final occludedCount = graph.occludedItems.length;
          
          results.add('✓ Graph loaded: $itemCount items ($includedCount included, $occludedCount occluded)');

          // Try topological sort to validate graph structure
          try {
            final sortedItems = graph.topologicalSort();
            results.add('✓ Graph structure valid (${sortedItems.length} items sorted)');
          } catch (e) {
            results.add('✗ Graph structure invalid: $e');
          }

          // Load logs
          try {
            final logs = DualFileManager.loadLogs(
              context.logsPath,
              context.occludeLogsPath,
            );
            final logCount = logs.length;
            final includedLogCount = logs.includedEntries.length;
            final occludedLogCount = logs.occludedEntries.length;
            
            results.add('✓ Logs loaded: $logCount entries ($includedLogCount included, $occludedLogCount occluded)');
          } catch (e) {
            results.add('✗ Log loading failed: $e');
          }

        } catch (e) {
          results.add('✗ Graph loading failed: $e');
        }
      }

      final message = args.verbose 
        ? 'File consistency check completed:\n${results.join('\n')}'
        : 'File consistency check completed (${results.where((r) => r.startsWith('✓')).length}/${results.length} checks passed)';

      return CommandResult.success(args, message);

    } catch (e) {
      return CommandResult.failure('Failed to check file consistency: $e');
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<Map<String, dynamic>>> checkConsistency(
    String workspacePath, {
    bool validate = true,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    final results = <String, dynamic>{
      'workspace_exists': PathResolver.gianttWorkspaceExists(workspacePath),
      'files': <String, dynamic>{},
      'graph_valid': false,
      'logs_valid': false,
    };

    // Check files
    final filesToCheck = [
      context.itemsPath,
      context.occludeItemsPath,
      context.logsPath,
      context.occludeLogsPath,
    ];

    for (final filepath in filesToCheck) {
      final file = File(filepath);
      final filename = filepath.split('/').last;
      
      if (file.existsSync()) {
        try {
          final content = file.readAsStringSync();
          results['files'][filename] = {
            'exists': true,
            'readable': true,
            'lines': content.split('\n').length,
          };
        } catch (e) {
          results['files'][filename] = {
            'exists': true,
            'readable': false,
            'error': e.toString(),
          };
        }
      } else {
        results['files'][filename] = {
          'exists': false,
          'readable': false,
        };
      }
    }

    if (validate) {
      // Check graph
      try {
        final graph = DualFileManager.loadGraph(
          context.itemsPath,
          context.occludeItemsPath,
        );
        
        results['graph_valid'] = true;
        results['graph_stats'] = {
          'total_items': graph.items.length,
          'included_items': graph.includedItems.length,
          'occluded_items': graph.occludedItems.length,
        };

        // Check graph structure
        try {
          final sortedItems = graph.topologicalSort();
          results['graph_structure_valid'] = true;
          results['sorted_items_count'] = sortedItems.length;
        } catch (e) {
          results['graph_structure_valid'] = false;
          results['graph_structure_error'] = e.toString();
        }

      } catch (e) {
        results['graph_valid'] = false;
        results['graph_error'] = e.toString();
      }

      // Check logs
      try {
        final logs = DualFileManager.loadLogs(
          context.logsPath,
          context.occludeLogsPath,
        );
        
        results['logs_valid'] = true;
        results['log_stats'] = {
          'total_entries': logs.length,
          'included_entries': logs.includedEntries.length,
          'occluded_entries': logs.occludedEntries.length,
        };

      } catch (e) {
        results['logs_valid'] = false;
        results['logs_error'] = e.toString();
      }
    }

    return CommandResult.success(results, 'Consistency check completed');
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/doctor_command.dart
// =====================================================================

import 'command_interface.dart';
import '../storage/dual_file_manager.dart';
import '../validation/graph_doctor.dart';

/// Arguments for doctor command
class DoctorArgs {
  const DoctorArgs({
    this.autoFix = false,
    this.issueType,
    this.itemId,
    this.verbose = false,
  });

  final bool autoFix;
  final String? issueType;
  final String? itemId;
  final bool verbose;
}

/// Graph health checking with auto-fix capabilities
class DoctorCommand extends CliCommand<DoctorArgs> {
  const DoctorCommand();

  @override
  String get name => 'doctor';

  @override
  String get description => 'Graph health checking with auto-fix capabilities';

  @override
  String get usage => 'doctor [--fix] [--type=issue_type] [--item=item_id] [--verbose]';

  @override
  DoctorArgs parseArgs(List<String> args) {
    bool autoFix = false;
    String? issueType;
    String? itemId;
    bool verbose = false;

    for (final arg in args) {
      if (arg == '--fix') {
        autoFix = true;
      } else if (arg.startsWith('--type=')) {
        issueType = arg.substring(7);
      } else if (arg.startsWith('--item=')) {
        itemId = arg.substring(7);
      } else if (arg == '--verbose' || arg == '-v') {
        verbose = true;
      } else {
        throw ArgumentError('Unknown argument: $arg');
      }
    }

    return DoctorArgs(
      autoFix: autoFix,
      issueType: issueType,
      itemId: itemId,
      verbose: verbose,
    );
  }

  @override
  Future<CommandResult<DoctorArgs>> execute(CommandContext context) async {
    try {
      // Load graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = DoctorArgs(); // This will be set by parseArgs in CLI usage
      final doctor = GraphDoctor(context.graph!);

      // Run diagnosis
      final issues = doctor.fullDiagnosis();
      
      if (issues.isEmpty) {
        return CommandResult.success(args, '✓ No issues found. Graph is healthy!');
      }

      final buffer = StringBuffer();
      buffer.writeln('Found ${issues.length} issue(s):');
      buffer.writeln();

      // Group issues by type
      final issuesByType = <IssueType, List<Issue>>{};
      for (final issue in issues) {
        issuesByType.putIfAbsent(issue.type, () => []).add(issue);
      }

      // Display issues
      for (final entry in issuesByType.entries) {
        final type = entry.key;
        final typeIssues = entry.value;
        
        buffer.writeln('${_getIssueIcon(type)} ${_getIssueTypeName(type)} (${typeIssues.length})');
        
        for (final issue in typeIssues) {
          buffer.writeln('  • ${issue.itemId}: ${issue.message}');
          if (issue.suggestedFix != null && (args.verbose || context.verbose)) {
            buffer.writeln('    Fix: ${issue.suggestedFix}');
          }
        }
        buffer.writeln();
      }

      // Auto-fix if requested
      if (args.autoFix) {
        final issueTypeFilter = args.issueType != null 
          ? IssueType.fromString(args.issueType!)
          : null;
        
        final fixedIssues = doctor.fixIssues(
          issueType: issueTypeFilter,
          itemId: args.itemId,
        );

        if (fixedIssues.isNotEmpty) {
          buffer.writeln('🔧 Fixed ${fixedIssues.length} issue(s):');
          for (final issue in fixedIssues) {
            buffer.writeln('  ✓ ${issue.itemId}: ${issue.message}');
          }
          buffer.writeln();

          // Save the fixed graph
          DualFileManager.saveGraph(
            context.itemsPath,
            context.occludeItemsPath,
            context.graph!,
          );

          buffer.writeln('Graph saved with fixes applied.');
        } else {
          buffer.writeln('No issues could be automatically fixed.');
        }
      } else {
        buffer.writeln('Run with --fix to automatically repair issues where possible.');
      }

      return CommandResult.success(args, buffer.toString());

    } catch (e) {
      return CommandResult.failure('Failed to run health check: $e');
    }
  }

  String _getIssueIcon(IssueType type) {
    switch (type) {
      case IssueType.danglingReference:
        return '🔗';
      case IssueType.orphanedItem:
        return '🏝️';
      case IssueType.incompleteChain:
        return '⛓️';
      case IssueType.chartInconsistency:
        return '📊';
      case IssueType.tagInconsistency:
        return '🏷️';
    }
  }

  String _getIssueTypeName(IssueType type) {
    switch (type) {
      case IssueType.danglingReference:
        return 'Dangling References';
      case IssueType.orphanedItem:
        return 'Orphaned Items';
      case IssueType.incompleteChain:
        return 'Incomplete Chains';
      case IssueType.chartInconsistency:
        return 'Chart Inconsistencies';
      case IssueType.tagInconsistency:
        return 'Tag Inconsistencies';
    }
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<Map<String, dynamic>>> checkHealth(
    String workspacePath, {
    bool autoFix = false,
    String? issueType,
    String? itemId,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    final doctor = GraphDoctor(context.graph!);
    final issues = doctor.fullDiagnosis();

    final results = <String, dynamic>{
      'total_issues': issues.length,
      'issues_by_type': <String, int>{},
      'issues': <Map<String, dynamic>>[],
      'fixed_issues': <Map<String, dynamic>>[],
    };

    // Group issues by type
    final issuesByType = <IssueType, List<Issue>>{};
    for (final issue in issues) {
      issuesByType.putIfAbsent(issue.type, () => []).add(issue);
    }

    // Convert to results format
    for (final entry in issuesByType.entries) {
      results['issues_by_type'][entry.key.value] = entry.value.length;
    }

    for (final issue in issues) {
      results['issues'].add({
        'type': issue.type.value,
        'item_id': issue.itemId,
        'message': issue.message,
        'related_ids': issue.relatedIds,
        'suggested_fix': issue.suggestedFix,
      });
    }

    // Auto-fix if requested
    if (autoFix && issues.isNotEmpty) {
      final issueTypeFilter = issueType != null 
        ? IssueType.fromString(issueType)
        : null;
      
      final fixedIssues = doctor.fixIssues(
        issueType: issueTypeFilter,
        itemId: itemId,
      );

      for (final issue in fixedIssues) {
        results['fixed_issues'].add({
          'type': issue.type.value,
          'item_id': issue.itemId,
          'message': issue.message,
        });
      }

      if (fixedIssues.isNotEmpty) {
        // Save the fixed graph
        DualFileManager.saveGraph(
          context.itemsPath,
          context.occludeItemsPath,
          context.graph!,
        );
        results['graph_saved'] = true;
      }
    }

    final message = issues.isEmpty 
      ? 'No issues found. Graph is healthy!'
      : 'Found ${issues.length} issue(s)${autoFix && results['fixed_issues'].isNotEmpty ? ', fixed ${results['fixed_issues'].length}' : ''}';

    return CommandResult.success(results, message);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/commands/modify_command.dart
// =====================================================================

import 'command_interface.dart';
import '../models/giantt_item.dart';
import '../models/status.dart';
import '../models/priority.dart';
import '../models/duration.dart';
import '../storage/dual_file_manager.dart';
import '../graph/giantt_graph.dart';
import '../models/graph_exceptions.dart';

/// Arguments for modify command
class ModifyArgs {
  const ModifyArgs({
    required this.itemId,
    this.title,
    this.status,
    this.priority,
    this.duration,
    this.addCharts = const [],
    this.removeCharts = const [],
    this.addTags = const [],
    this.removeTags = const [],
    this.addRelations = const {},
    this.removeRelations = const {},
    this.userComment,
  });

  final String itemId;
  final String? title;
  final GianttStatus? status;
  final GianttPriority? priority;
  final GianttDuration? duration;
  final List<String> addCharts;
  final List<String> removeCharts;
  final List<String> addTags;
  final List<String> removeTags;
  final Map<String, List<String>> addRelations;
  final Map<String, List<String>> removeRelations;
  final String? userComment;
}

/// Modify an existing item in the graph
class ModifyCommand extends CliCommand<ModifyArgs> {
  const ModifyCommand();

  @override
  String get name => 'modify';

  @override
  String get description => 'Modify an existing item in the graph';

  @override
  String get usage => 'modify <id> [options]';

  @override
  ModifyArgs parseArgs(List<String> args) {
    if (args.isEmpty) {
      throw ArgumentError('modify requires an item ID');
    }

    final itemId = args[0];
    String? title;
    GianttStatus? status;
    GianttPriority? priority;
    GianttDuration? duration;
    List<String> addCharts = [];
    List<String> removeCharts = [];
    List<String> addTags = [];
    List<String> removeTags = [];
    Map<String, List<String>> addRelations = {};
    Map<String, List<String>> removeRelations = {};
    String? userComment;

    for (int i = 1; i < args.length; i++) {
      final arg = args[i];
      
      if (arg.startsWith('--title=')) {
        title = arg.substring(8);
      } else if (arg.startsWith('--status=')) {
        final statusStr = arg.substring(9);
        status = GianttStatus.fromSymbol(statusStr);
      } else if (arg.startsWith('--priority=')) {
        final priorityStr = arg.substring(11);
        priority = GianttPriority.fromSymbol(priorityStr);
      } else if (arg.startsWith('--duration=')) {
        final durationStr = arg.substring(11);
        duration = GianttDuration.parse(durationStr);
      } else if (arg.startsWith('--add-charts=')) {
        final chartsStr = arg.substring(13);
        addCharts = chartsStr.split(',').map((c) => c.trim()).toList();
      } else if (arg.startsWith('--remove-charts=')) {
        final chartsStr = arg.substring(16);
        removeCharts = chartsStr.split(',').map((c) => c.trim()).toList();
      } else if (arg.startsWith('--add-tags=')) {
        final tagsStr = arg.substring(11);
        addTags = tagsStr.split(',').map((t) => t.trim()).toList();
      } else if (arg.startsWith('--remove-tags=')) {
        final tagsStr = arg.substring(14);
        removeTags = tagsStr.split(',').map((t) => t.trim()).toList();
      } else if (arg.startsWith('--add-requires=')) {
        final requiresStr = arg.substring(15);
        addRelations['REQUIRES'] = requiresStr.split(',').map((r) => r.trim()).toList();
      } else if (arg.startsWith('--remove-requires=')) {
        final requiresStr = arg.substring(18);
        removeRelations['REQUIRES'] = requiresStr.split(',').map((r) => r.trim()).toList();
      } else if (arg.startsWith('--add-blocks=')) {
        final blocksStr = arg.substring(13);
        addRelations['BLOCKS'] = blocksStr.split(',').map((b) => b.trim()).toList();
      } else if (arg.startsWith('--remove-blocks=')) {
        final blocksStr = arg.substring(16);
        removeRelations['BLOCKS'] = blocksStr.split(',').map((b) => b.trim()).toList();
      } else if (arg.startsWith('--comment=')) {
        userComment = arg.substring(10);
      }
    }

    return ModifyArgs(
      itemId: itemId,
      title: title,
      status: status,
      priority: priority,
      duration: duration,
      addCharts: addCharts,
      removeCharts: removeCharts,
      addTags: addTags,
      removeTags: removeTags,
      addRelations: addRelations,
      removeRelations: removeRelations,
      userComment: userComment,
    );
  }

  @override
  Future<CommandResult<ModifyArgs>> execute(CommandContext context) async {
    try {
      // Load existing graph
      context.graph ??= DualFileManager.loadGraph(
        context.itemsPath,
        context.occludeItemsPath,
      );

      final args = ModifyArgs(itemId: 'temp'); // This will be set by parseArgs in CLI usage

      // Find the item
      final existingItem = context.graph!.items[args.itemId];
      if (existingItem == null) {
        return CommandResult.failure('Item with ID "${args.itemId}" not found');
      }

      // Apply modifications
      final modifiedItem = _applyModifications(existingItem, args);

      if (context.dryRun) {
        return CommandResult.message(
          'Would modify item: ${modifiedItem.toFileString()}'
        );
      }

      // Check for cycles before applying changes (especially for relation modifications)
      if (args.addRelations.isNotEmpty || args.removeRelations.isNotEmpty) {
        // Create a temporary graph to test the changes
        final tempGraph = GianttGraph();
        for (final item in context.graph!.items.values) {
          tempGraph.addItem(item);
        }
        tempGraph.addItem(modifiedItem); // Replace with modified version
        
        try {
          tempGraph.topologicalSort();
        } on CycleDetectedException catch (e) {
          return CommandResult.failure('Modifying relations would create a dependency cycle: ${e.cycleItems.join(' -> ')}');
        }
      }

      // Update in graph
      context.graph!.addItem(modifiedItem);

      // Save graph
      DualFileManager.saveGraph(
        context.itemsPath,
        context.occludeItemsPath,
        context.graph!,
      );

      return CommandResult.success(
        args,
        'Modified item "${args.itemId}" successfully'
      );

    } catch (e) {
      return CommandResult.failure('Failed to modify item: $e');
    }
  }

  GianttItem _applyModifications(GianttItem item, ModifyArgs args) {
    // Start with existing item
    var modified = item;

    // Apply basic property changes
    if (args.title != null) {
      modified = modified.copyWith(title: args.title);
    }
    if (args.status != null) {
      modified = modified.copyWith(status: args.status);
    }
    if (args.priority != null) {
      modified = modified.copyWith(priority: args.priority);
    }
    if (args.duration != null) {
      modified = modified.copyWith(duration: args.duration);
    }
    if (args.userComment != null) {
      modified = modified.copyWith(userComment: args.userComment);
    }

    // Apply chart modifications
    var newCharts = List<String>.from(modified.charts);
    newCharts.addAll(args.addCharts);
    newCharts.removeWhere((chart) => args.removeCharts.contains(chart));
    modified = modified.copyWith(charts: newCharts);

    // Apply tag modifications
    var newTags = List<String>.from(modified.tags);
    newTags.addAll(args.addTags);
    newTags.removeWhere((tag) => args.removeTags.contains(tag));
    modified = modified.copyWith(tags: newTags);

    // Apply relation modifications
    var newRelations = Map<String, List<String>>.from(modified.relations);
    
    // Add relations
    for (final entry in args.addRelations.entries) {
      final relType = entry.key;
      final targets = entry.value;
      newRelations[relType] = (newRelations[relType] ?? [])..addAll(targets);
    }
    
    // Remove relations
    for (final entry in args.removeRelations.entries) {
      final relType = entry.key;
      final targets = entry.value;
      if (newRelations.containsKey(relType)) {
        newRelations[relType]!.removeWhere((target) => targets.contains(target));
        if (newRelations[relType]!.isEmpty) {
          newRelations.remove(relType);
        }
      }
    }
    
    modified = modified.copyWith(relations: newRelations);

    return modified;
  }

  /// Static method for programmatic use (Flutter)
  static Future<CommandResult<GianttItem>> modifyItem(
    String workspacePath,
    String itemId, {
    String? title,
    GianttStatus? status,
    GianttPriority? priority,
    GianttDuration? duration,
    List<String> addCharts = const [],
    List<String> removeCharts = const [],
    List<String> addTags = const [],
    List<String> removeTags = const [],
    Map<String, List<String>> addRelations = const {},
    Map<String, List<String>> removeRelations = const {},
    String? userComment,
  }) async {
    final context = CommandContext(workspacePath: workspacePath);
    
    // Load existing graph
    context.graph = DualFileManager.loadGraph(
      context.itemsPath,
      context.occludeItemsPath,
    );

    // Find the item
    final existingItem = context.graph!.items[itemId];
    if (existingItem == null) {
      return CommandResult.failure('Item with ID "$itemId" not found');
    }

    final args = ModifyArgs(
      itemId: itemId,
      title: title,
      status: status,
      priority: priority,
      duration: duration,
      addCharts: addCharts,
      removeCharts: removeCharts,
      addTags: addTags,
      removeTags: removeTags,
      addRelations: addRelations,
      removeRelations: removeRelations,
      userComment: userComment,
    );

    final command = ModifyCommand();
    final modifiedItem = command._applyModifications(existingItem, args);

    // Update in graph
    context.graph!.addItem(modifiedItem);

    // Save graph
    DualFileManager.saveGraph(
      context.itemsPath,
      context.occludeItemsPath,
      context.graph!,
    );

    return CommandResult.success(modifiedItem, 'Modified item "$itemId" successfully');
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/logging/log_collection.dart
// =====================================================================

import '../models/log_entry.dart';

/// A collection of log entries with query capabilities
class LogCollection {
  final List<LogEntry> _entries = [];

  /// Create an empty log collection
  LogCollection();

  /// Create a log collection with initial entries
  LogCollection.fromEntries(List<LogEntry> entries) {
    _entries.addAll(entries);
    _sortEntries();
  }

  /// Get all entries
  List<LogEntry> get entries => List.unmodifiable(_entries);

  /// Get entries that are not occluded
  List<LogEntry> get includedEntries {
    return _entries.where((entry) => !entry.occlude).toList();
  }

  /// Get entries that are occluded
  List<LogEntry> get occludedEntries {
    return _entries.where((entry) => entry.occlude).toList();
  }

  /// Add a single entry
  void addEntry(LogEntry entry) {
    final index = _getInsertionIndex(entry.timestamp);
    _entries.insert(index, entry);
  }

  /// Add multiple entries
  void addEntries(List<LogEntry> entries) {
    _entries.addAll(entries);
    _sortEntries();
  }

  /// Replace an entry with a new one
  void replaceEntry(LogEntry oldEntry, LogEntry newEntry) {
    final index = _entries.indexOf(oldEntry);
    if (index != -1) {
      _entries[index] = newEntry;
      _sortEntries();
    }
  }

  /// Create and add a new entry
  LogEntry createEntry(
    String sessionTag,
    String message, {
    List<String>? additionalTags,
    Map<String, String>? metadata,
    bool occlude = false,
  }) {
    final entry = LogEntry.create(
      sessionTag,
      message,
      additionalTags: additionalTags,
      metadata: metadata,
      occlude: occlude,
    );
    addEntry(entry);
    return entry;
  }

  /// Get all entries with a specific session tag
  List<LogEntry> getBySession(String sessionTag) {
    return _entries.where((entry) => entry.session == sessionTag).toList();
  }

  /// Get entries with specified tags
  List<LogEntry> getByTags(List<String> tags, {bool requireAll = false}) {
    if (requireAll) {
      return _entries.where((entry) => entry.hasAllTags(tags)).toList();
    }
    return _entries.where((entry) => entry.hasAnyTags(tags)).toList();
  }

  /// Get entries within a date range
  List<LogEntry> getByDateRange(DateTime start, [DateTime? end]) {
    final endDate = end ?? DateTime.now().toUtc();
    return _entries.where((entry) {
      return entry.timestamp.isAfter(start) && entry.timestamp.isBefore(endDate);
    }).toList();
  }

  /// Get entries with a specific substring in the message
  List<LogEntry> getBySubstring(String substring) {
    final lowerSubstring = substring.toLowerCase();
    return _entries.where((entry) {
      return entry.message.toLowerCase().contains(lowerSubstring);
    }).toList();
  }

  /// Get the index where a new entry with the given timestamp should be inserted
  int _getInsertionIndex(DateTime timestamp) {
    if (_entries.isEmpty) return 0;
    if (timestamp.isAfter(_entries.last.timestamp)) return _entries.length;
    if (timestamp.isBefore(_entries.first.timestamp)) return 0;

    int low = 0;
    int high = _entries.length - 1;
    
    while (low <= high) {
      final mid = (low + high) ~/ 2;
      final midTimestamp = _entries[mid].timestamp;
      
      if (midTimestamp.isBefore(timestamp)) {
        low = mid + 1;
      } else {
        high = mid - 1;
      }
    }
    
    return low;
  }

  /// Sort entries by timestamp
  void _sortEntries() {
    _entries.sort((a, b) => a.timestamp.compareTo(b.timestamp));
  }

  /// Get the first index after a timestamp (for binary search)
  int getFirstIndexAfterTimestamp(DateTime timestamp) {
    if (_entries.isEmpty) return 0;
    if (timestamp.isAfter(_entries.last.timestamp)) return _entries.length - 1;
    if (timestamp.isBefore(_entries.first.timestamp)) return 0;

    int low = 0;
    int high = _entries.length - 1;
    
    while (low < high) {
      final mid = (low + high) ~/ 2;
      if (_entries[mid].timestamp.isBefore(timestamp)) {
        low = mid + 1;
      } else {
        high = mid;
      }
    }
    
    return low;
  }

  /// Clear all entries
  void clear() {
    _entries.clear();
  }

  /// Get the number of entries
  int get length => _entries.length;

  /// Check if the collection is empty
  bool get isEmpty => _entries.isEmpty;

  /// Check if the collection is not empty
  bool get isNotEmpty => _entries.isNotEmpty;

  /// Iterator support
  Iterator<LogEntry> get iterator => _entries.iterator;
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/logging/log_repository.dart
// =====================================================================

import 'dart:io';
import '../models/log_entry.dart';
import '../models/graph_exceptions.dart';
import 'log_collection.dart';
import '../storage/file_header_generator.dart';
import '../storage/atomic_file_writer.dart';

/// Repository for log file I/O operations
class LogRepository {
  /// Load logs from a single file
  static LogCollection loadFromFile(String filepath, {bool occlude = false}) {
    final logs = <LogEntry>[];
    
    try {
      final file = File(filepath);
      if (!file.existsSync()) {
        return LogCollection.fromEntries(logs);
      }

      final lines = file.readAsLinesSync();
      for (final line in lines) {
        final trimmed = line.trim();
        if (trimmed.isNotEmpty && !trimmed.startsWith('#')) {
          try {
            final log = LogEntry.fromJsonLine(trimmed, occlude: occlude);
            logs.add(log);
          } catch (e) {
            // Skip invalid lines with warning
            print('Warning: Skipping invalid log line in $filepath: $e');
          }
        }
      }
    } catch (e) {
      throw GraphException('Error loading logs from $filepath: $e');
    }
    
    return LogCollection.fromEntries(logs);
  }

  /// Load logs from both include and occlude files
  static LogCollection loadDualFile(String includePath, String occludePath) {
    final includeCollection = loadFromFile(includePath, occlude: false);
    final occludeCollection = loadFromFile(occludePath, occlude: true);
    
    final allLogs = LogCollection();
    allLogs.addEntries(includeCollection.entries);
    allLogs.addEntries(occludeCollection.entries);
    
    return allLogs;
  }

  /// Save logs to a single file
  static void saveToFile(String filepath, LogCollection logs, {bool includeHeader = true}) {
    try {
      final content = StringBuffer();
      
      if (includeHeader) {
        content.writeln(FileHeaderGenerator.generateLogsFileHeader());
        content.writeln();
      }

      for (final log in logs.entries) {
        content.writeln(log.toJsonLine());
      }

      final fileContents = {filepath: content.toString()};
      AtomicFileWriter.writeFiles(fileContents);
    } catch (e) {
      throw GraphException('Failed to save logs to $filepath: $e');
    }
  }

  /// Save logs to both include and occlude files
  static void saveDualFile(String includePath, String occludePath, LogCollection logs) {
    try {
      final includeContent = StringBuffer();
      final occludeContent = StringBuffer();

      // Add headers
      includeContent.writeln(FileHeaderGenerator.generateLogsFileHeader());
      includeContent.writeln();
      
      occludeContent.writeln(FileHeaderGenerator.generateOccludedLogsFileHeader());
      occludeContent.writeln();

      // Add logs to appropriate files
      for (final log in logs.entries) {
        final logLine = log.toJsonLine();
        if (log.occlude) {
          occludeContent.writeln(logLine);
        } else {
          includeContent.writeln(logLine);
        }
      }

      // Write both files atomically
      final fileContents = {
        includePath: includeContent.toString(),
        occludePath: occludeContent.toString(),
      };

      AtomicFileWriter.writeFiles(fileContents);
    } catch (e) {
      throw GraphException('Failed to save logs: $e');
    }
  }

  /// Append a single log entry to a file
  static void appendToFile(String filepath, LogEntry entry) {
    try {
      final file = File(filepath);
      final logLine = entry.toJsonLine();
      
      if (file.existsSync()) {
        file.writeAsStringSync('$logLine\n', mode: FileMode.append);
      } else {
        // Create new file with header
        final content = StringBuffer();
        content.writeln(FileHeaderGenerator.generateLogsFileHeader());
        content.writeln();
        content.writeln(logLine);
        file.writeAsStringSync(content.toString());
      }
    } catch (e) {
      throw GraphException('Failed to append log to $filepath: $e');
    }
  }

  /// Check if a log file exists
  static bool fileExists(String filepath) {
    return File(filepath).existsSync();
  }

  /// Get the size of a log file in bytes
  static int getFileSize(String filepath) {
    final file = File(filepath);
    return file.existsSync() ? file.lengthSync() : 0;
  }

  /// Get the number of log entries in a file (approximate, based on line count)
  static int getEntryCount(String filepath) {
    try {
      final file = File(filepath);
      if (!file.existsSync()) return 0;
      
      final lines = file.readAsLinesSync();
      return lines.where((line) => 
        line.trim().isNotEmpty && !line.trim().startsWith('#')
      ).length;
    } catch (e) {
      return 0;
    }
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/logging/session_manager.dart
// =====================================================================

import '../models/log_entry.dart';
import 'log_collection.dart';

/// Manages session tags and session-based operations
class SessionManager {
  final LogCollection _logs;
  
  SessionManager(this._logs);

  /// Get all unique session tags
  Set<String> getAllSessions() {
    return _logs.entries.map((entry) => entry.session).toSet();
  }

  /// Get all entries for a specific session
  List<LogEntry> getSessionEntries(String sessionTag) {
    return _logs.getBySession(sessionTag);
  }

  /// Get the most recent session tag
  String? getMostRecentSession() {
    if (_logs.isEmpty) return null;
    return _logs.entries.last.session;
  }

  /// Get the oldest session tag
  String? getOldestSession() {
    if (_logs.isEmpty) return null;
    return _logs.entries.first.session;
  }

  /// Get sessions within a date range
  Set<String> getSessionsInDateRange(DateTime start, [DateTime? end]) {
    final entriesInRange = _logs.getByDateRange(start, end);
    return entriesInRange.map((entry) => entry.session).toSet();
  }

  /// Get session statistics
  Map<String, SessionStats> getSessionStats() {
    final stats = <String, SessionStats>{};
    
    for (final entry in _logs.entries) {
      final session = entry.session;
      if (!stats.containsKey(session)) {
        stats[session] = SessionStats(
          sessionTag: session,
          entryCount: 0,
          firstEntry: entry.timestamp,
          lastEntry: entry.timestamp,
          totalTags: <String>{},
        );
      }
      
      final sessionStats = stats[session]!;
      stats[session] = SessionStats(
        sessionTag: session,
        entryCount: sessionStats.entryCount + 1,
        firstEntry: entry.timestamp.isBefore(sessionStats.firstEntry) 
          ? entry.timestamp 
          : sessionStats.firstEntry,
        lastEntry: entry.timestamp.isAfter(sessionStats.lastEntry) 
          ? entry.timestamp 
          : sessionStats.lastEntry,
        totalTags: sessionStats.totalTags..addAll(entry.tags),
      );
    }
    
    return stats;
  }

  /// Generate a new session tag based on current timestamp
  static String generateSessionTag([String? prefix]) {
    final now = DateTime.now().toUtc();
    final timestamp = now.toIso8601String().replaceAll(RegExp(r'[:\-.]'), '');
    return prefix != null ? '${prefix}_$timestamp' : 'session_$timestamp';
  }

  /// Check if a session exists
  bool sessionExists(String sessionTag) {
    return _logs.entries.any((entry) => entry.session == sessionTag);
  }

  /// Get the duration of a session (time between first and last entry)
  Duration? getSessionDuration(String sessionTag) {
    final sessionEntries = getSessionEntries(sessionTag);
    if (sessionEntries.isEmpty) return null;
    if (sessionEntries.length == 1) return Duration.zero;
    
    sessionEntries.sort((a, b) => a.timestamp.compareTo(b.timestamp));
    return sessionEntries.last.timestamp.difference(sessionEntries.first.timestamp);
  }

  /// Get sessions that contain specific tags
  Set<String> getSessionsWithTags(List<String> tags, {bool requireAll = false}) {
    final matchingSessions = <String>{};
    
    for (final session in getAllSessions()) {
      final sessionEntries = getSessionEntries(session);
      final sessionTags = sessionEntries
        .expand((entry) => entry.tags)
        .toSet();
      
      if (requireAll) {
        if (tags.every(sessionTags.contains)) {
          matchingSessions.add(session);
        }
      } else {
        if (tags.any(sessionTags.contains)) {
          matchingSessions.add(session);
        }
      }
    }
    
    return matchingSessions;
  }

  /// Get the most active sessions (by entry count)
  List<String> getMostActiveSessions([int limit = 10]) {
    final stats = getSessionStats();
    final sortedSessions = stats.entries.toList()
      ..sort((a, b) => b.value.entryCount.compareTo(a.value.entryCount));
    
    return sortedSessions
      .take(limit)
      .map((entry) => entry.key)
      .toList();
  }
}

/// Statistics for a session
class SessionStats {
  const SessionStats({
    required this.sessionTag,
    required this.entryCount,
    required this.firstEntry,
    required this.lastEntry,
    required this.totalTags,
  });

  final String sessionTag;
  final int entryCount;
  final DateTime firstEntry;
  final DateTime lastEntry;
  final Set<String> totalTags;

  Duration get duration => lastEntry.difference(firstEntry);
  
  @override
  String toString() {
    return 'SessionStats(session: $sessionTag, entries: $entryCount, '
           'duration: $duration, tags: ${totalTags.length})';
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/logging/log_occluder.dart
// =====================================================================

import '../models/log_entry.dart';
import 'log_collection.dart';

/// Handles log occlusion operations
class LogOccluder {
  /// Occlude logs by session tags with optional dry-run
  static LogOccludeResult occludeBySession(
    LogCollection logs,
    List<String> sessionTags, {
    bool dryRun = false,
  }) {
    final toOcclude = <LogEntry>[];

    for (final log in logs.includedEntries) {
      if (sessionTags.contains(log.session)) {
        toOcclude.add(log);
      }
    }

    if (dryRun) {
      return LogOccludeResult(
        occludedCount: toOcclude.length,
        occludedLogs: toOcclude,
        dryRun: true,
      );
    }

    // Actually occlude the logs
    for (final log in toOcclude) {
      final occludedLog = log.setOcclude(true);
      logs.replaceEntry(log, occludedLog);
    }

    return LogOccludeResult(
      occludedCount: toOcclude.length,
      occludedLogs: toOcclude,
      dryRun: false,
    );
  }

  /// Occlude logs by tags with optional dry-run
  static LogOccludeResult occludeByTags(
    LogCollection logs,
    List<String> tags, {
    bool requireAll = false,
    bool dryRun = false,
  }) {
    final toOcclude = <LogEntry>[];

    for (final log in logs.includedEntries) {
      bool shouldOcclude = false;

      if (requireAll) {
        shouldOcclude = log.hasAllTags(tags);
      } else {
        shouldOcclude = log.hasAnyTags(tags);
      }

      if (shouldOcclude) {
        toOcclude.add(log);
      }
    }

    if (dryRun) {
      return LogOccludeResult(
        occludedCount: toOcclude.length,
        occludedLogs: toOcclude,
        dryRun: true,
      );
    }

    // Actually occlude the logs
    for (final log in toOcclude) {
      final occludedLog = log.setOcclude(true);
      logs.replaceEntry(log, occludedLog);
    }

    return LogOccludeResult(
      occludedCount: toOcclude.length,
      occludedLogs: toOcclude,
      dryRun: false,
    );
  }

  /// Occlude logs by date range with optional dry-run
  static LogOccludeResult occludeByDateRange(
    LogCollection logs,
    DateTime start,
    DateTime end, {
    bool dryRun = false,
  }) {
    final toOcclude = logs.includedEntries.where((log) {
      return log.timestamp.isAfter(start) && log.timestamp.isBefore(end);
    }).toList();

    if (dryRun) {
      return LogOccludeResult(
        occludedCount: toOcclude.length,
        occludedLogs: toOcclude,
        dryRun: true,
      );
    }

    // Actually occlude the logs
    for (final log in toOcclude) {
      final occludedLog = log.setOcclude(true);
      logs.replaceEntry(log, occludedLog);
    }

    return LogOccludeResult(
      occludedCount: toOcclude.length,
      occludedLogs: toOcclude,
      dryRun: false,
    );
  }

  /// Include (un-occlude) logs by session tags with optional dry-run
  static LogOccludeResult includeBySession(
    LogCollection logs,
    List<String> sessionTags, {
    bool dryRun = false,
  }) {
    final toInclude = <LogEntry>[];

    for (final log in logs.occludedEntries) {
      if (sessionTags.contains(log.session)) {
        toInclude.add(log);
      }
    }

    if (dryRun) {
      return LogOccludeResult(
        occludedCount: toInclude.length,
        occludedLogs: toInclude,
        dryRun: true,
      );
    }

    // Actually include the logs
    for (final log in toInclude) {
      final includedLog = log.setOcclude(false);
      logs.replaceEntry(log, includedLog);
    }

    return LogOccludeResult(
      occludedCount: toInclude.length,
      occludedLogs: toInclude,
      dryRun: false,
    );
  }

  /// Include logs by tags with optional dry-run
  static LogOccludeResult includeByTags(
    LogCollection logs,
    List<String> tags, {
    bool requireAll = false,
    bool dryRun = false,
  }) {
    final toInclude = <LogEntry>[];

    for (final log in logs.occludedEntries) {
      bool shouldInclude = false;

      if (requireAll) {
        shouldInclude = log.hasAllTags(tags);
      } else {
        shouldInclude = log.hasAnyTags(tags);
      }

      if (shouldInclude) {
        toInclude.add(log);
      }
    }

    if (dryRun) {
      return LogOccludeResult(
        occludedCount: toInclude.length,
        occludedLogs: toInclude,
        dryRun: true,
      );
    }

    // Actually include the logs
    for (final log in toInclude) {
      final includedLog = log.setOcclude(false);
      logs.replaceEntry(log, includedLog);
    }

    return LogOccludeResult(
      occludedCount: toInclude.length,
      occludedLogs: toInclude,
      dryRun: false,
    );
  }

  /// Get statistics about what would be occluded (dry-run analysis)
  static OcclusionAnalysis analyzeOcclusion(
    LogCollection logs, {
    List<String>? sessionTags,
    List<String>? tags,
    DateTime? startDate,
    DateTime? endDate,
    bool requireAllTags = false,
  }) {
    var candidateEntries = logs.includedEntries;

    // Apply session filter
    if (sessionTags != null && sessionTags.isNotEmpty) {
      candidateEntries = candidateEntries
        .where((log) => sessionTags.contains(log.session))
        .toList();
    }

    // Apply tag filter
    if (tags != null && tags.isNotEmpty) {
      candidateEntries = candidateEntries.where((log) {
        return requireAllTags ? log.hasAllTags(tags) : log.hasAnyTags(tags);
      }).toList();
    }

    // Apply date filter
    if (startDate != null || endDate != null) {
      candidateEntries = candidateEntries.where((log) {
        if (startDate != null && log.timestamp.isBefore(startDate)) return false;
        if (endDate != null && log.timestamp.isAfter(endDate)) return false;
        return true;
      }).toList();
    }

    // Analyze the results
    final sessionCounts = <String, int>{};
    final tagCounts = <String, int>{};
    
    for (final entry in candidateEntries) {
      sessionCounts[entry.session] = (sessionCounts[entry.session] ?? 0) + 1;
      for (final tag in entry.tags) {
        tagCounts[tag] = (tagCounts[tag] ?? 0) + 1;
      }
    }

    return OcclusionAnalysis(
      totalCandidates: candidateEntries.length,
      sessionBreakdown: sessionCounts,
      tagBreakdown: tagCounts,
      dateRange: candidateEntries.isNotEmpty ? DateRange(
        start: candidateEntries.map((e) => e.timestamp).reduce((a, b) => a.isBefore(b) ? a : b),
        end: candidateEntries.map((e) => e.timestamp).reduce((a, b) => a.isAfter(b) ? a : b),
      ) : null,
    );
  }
}

/// Result of a log occlude operation
class LogOccludeResult {
  const LogOccludeResult({
    required this.occludedCount,
    required this.occludedLogs,
    required this.dryRun,
  });

  final int occludedCount;
  final List<LogEntry> occludedLogs;
  final bool dryRun;

  bool get hasOccluded => occludedCount > 0;
  
  @override
  String toString() {
    final action = dryRun ? 'Would occlude' : 'Occluded';
    return '$action $occludedCount log entries';
  }
}

/// Analysis of what would be occluded
class OcclusionAnalysis {
  const OcclusionAnalysis({
    required this.totalCandidates,
    required this.sessionBreakdown,
    required this.tagBreakdown,
    this.dateRange,
  });

  final int totalCandidates;
  final Map<String, int> sessionBreakdown;
  final Map<String, int> tagBreakdown;
  final DateRange? dateRange;

  @override
  String toString() {
    final buffer = StringBuffer();
    buffer.writeln('Occlusion Analysis:');
    buffer.writeln('  Total candidates: $totalCandidates');
    
    if (sessionBreakdown.isNotEmpty) {
      buffer.writeln('  By session:');
      for (final entry in sessionBreakdown.entries) {
        buffer.writeln('    ${entry.key}: ${entry.value}');
      }
    }
    
    if (tagBreakdown.isNotEmpty) {
      buffer.writeln('  By tag:');
      for (final entry in tagBreakdown.entries) {
        buffer.writeln('    ${entry.key}: ${entry.value}');
      }
    }
    
    if (dateRange != null) {
      buffer.writeln('  Date range: ${dateRange!.start} to ${dateRange!.end}');
    }
    
    return buffer.toString();
  }
}

/// Represents a date range
class DateRange {
  const DateRange({required this.start, required this.end});
  
  final DateTime start;
  final DateTime end;
  
  Duration get duration => end.difference(start);
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/logging/log_serializer.dart
// =====================================================================

import 'dart:convert';
import '../models/log_entry.dart';

/// Handles JSONL serialization and deserialization for log entries
class LogSerializer {
  /// Serialize a log entry to JSONL format
  static String serialize(LogEntry entry) {
    return entry.toJsonLine();
  }

  /// Deserialize a JSONL line to a log entry
  static LogEntry deserialize(String jsonLine, {bool occlude = false}) {
    return LogEntry.fromJsonLine(jsonLine, occlude: occlude);
  }

  /// Serialize multiple log entries to JSONL format
  static String serializeMultiple(List<LogEntry> entries) {
    return entries.map(serialize).join('\n');
  }

  /// Deserialize multiple JSONL lines to log entries
  static List<LogEntry> deserializeMultiple(String jsonLines, {bool occlude = false}) {
    final lines = jsonLines.split('\n');
    final entries = <LogEntry>[];
    
    for (final line in lines) {
      final trimmed = line.trim();
      if (trimmed.isNotEmpty && !trimmed.startsWith('#')) {
        try {
          entries.add(deserialize(trimmed, occlude: occlude));
        } catch (e) {
          // Skip invalid lines
          print('Warning: Skipping invalid log line: $e');
        }
      }
    }
    
    return entries;
  }

  /// Validate that a string is valid JSONL format
  static bool isValidJsonLine(String line) {
    try {
      final data = json.decode(line);
      if (data is! Map<String, dynamic>) return false;
      
      // Check required fields
      return data.containsKey('s') &&  // session
             data.containsKey('t') &&  // timestamp
             data.containsKey('m') &&  // message
             data.containsKey('tags'); // tags
    } catch (e) {
      return false;
    }
  }

  /// Extract metadata from a JSONL line without full deserialization
  static Map<String, dynamic>? extractMetadata(String jsonLine) {
    try {
      final data = json.decode(jsonLine) as Map<String, dynamic>;
      return {
        'session': data['s'],
        'timestamp': data['t'],
        'tags': data['tags'],
        'hasMetadata': data.containsKey('meta') && (data['meta'] as Map).isNotEmpty,
      };
    } catch (e) {
      return null;
    }
  }

  /// Convert log entry to a pretty-printed JSON format (for debugging)
  static String toPrettyJson(LogEntry entry) {
    final data = {
      'session': entry.session,
      'timestamp': entry.timestamp.toIso8601String(),
      'message': entry.message,
      'tags': entry.tags.toList()..sort(),
      'metadata': entry.metadata,
      'occlude': entry.occlude,
    };
    
    const encoder = JsonEncoder.withIndent('  ');
    return encoder.convert(data);
  }

  /// Batch serialize entries with optional filtering
  static String serializeBatch(
    List<LogEntry> entries, {
    bool includeOccluded = true,
    Set<String>? sessionFilter,
    Set<String>? tagFilter,
  }) {
    var filteredEntries = entries;
    
    if (!includeOccluded) {
      filteredEntries = filteredEntries.where((e) => !e.occlude).toList();
    }
    
    if (sessionFilter != null) {
      filteredEntries = filteredEntries
        .where((e) => sessionFilter.contains(e.session))
        .toList();
    }
    
    if (tagFilter != null) {
      filteredEntries = filteredEntries
        .where((e) => e.tags.any(tagFilter.contains))
        .toList();
    }
    
    return serializeMultiple(filteredEntries);
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/validation/issue_types.dart
// =====================================================================

import '../models/duration.dart';
import '../graph/giantt_graph.dart';

/// Additional validation issue types for comprehensive graph checking
enum ValidationSeverity {
  warning,
  error,
  info,
}

/// Validation issue categories
class ValidationIssue {
  const ValidationIssue({
    required this.type,
    required this.severity,
    required this.message,
    required this.itemId,
    this.suggestedFix,
    this.relatedItems = const [],
  });

  final String type;
  final ValidationSeverity severity;
  final String message;
  final String itemId;
  final String? suggestedFix;
  final List<String> relatedItems;
}

/// Validation rules for input checking
class ValidationRules {
  /// Check if an ID is valid (no special characters, not empty, etc.)
  static bool isValidId(String id) {
    if (id.isEmpty) return false;
    if (id.contains(' ')) return false;
    if (id.contains('\t')) return false;
    if (id.contains('\n')) return false;
    return true;
  }

  /// Check if a title is valid (not empty, reasonable length)
  static bool isValidTitle(String title) {
    if (title.isEmpty) return false;
    if (title.length > 200) return false; // Reasonable limit
    return true;
  }

  /// Check if a duration string is valid
  static bool isValidDuration(String duration) {
    try {
      GianttDuration.parse(duration);
      return true;
    } catch (e) {
      return false;
    }
  }

  /// Check for potential ID/title conflicts
  static List<ValidationIssue> checkIdTitleConflicts(GianttGraph graph, String newId, String newTitle) {
    final issues = <ValidationIssue>[];
    
    for (final item in graph.items.values) {
      // Check ID conflicts
      if (newId.toLowerCase() == item.title.toLowerCase()) {
        issues.add(ValidationIssue(
          type: 'id_title_conflict',
          severity: ValidationSeverity.error,
          message: 'ID "$newId" conflicts with existing item title',
          itemId: item.id,
          relatedItems: [item.id],
        ));
      }
      
      // Check title conflicts
      if (newTitle.toLowerCase() == item.title.toLowerCase()) {
        issues.add(ValidationIssue(
          type: 'title_conflict',
          severity: ValidationSeverity.error,
          message: 'Title "$newTitle" conflicts with existing item title',
          itemId: item.id,
          relatedItems: [item.id],
        ));
      }
    }
    
    return issues;
  }
}


// =====================================================================
// FILE: packages/giantt_core/lib/src/validation/graph_doctor.dart
// =====================================================================

import '../models/giantt_item.dart';
import '../graph/giantt_graph.dart';

/// Types of issues that can be detected in the graph
enum IssueType {
  danglingReference('dangling_reference'),
  orphanedItem('orphaned_item'),
  incompleteChain('incomplete_chain'),
  chartInconsistency('chart_inconsistency'),
  tagInconsistency('tag_inconsistency');

  const IssueType(this.value);
  final String value;

  static IssueType fromString(String value) {
    for (final type in IssueType.values) {
      if (type.value == value) return type;
    }
    throw ArgumentError('Invalid issue type: $value');
  }
}

/// Represents a specific issue found in the graph
class Issue {
  const Issue({
    required this.type,
    required this.itemId,
    required this.message,
    required this.relatedIds,
    this.suggestedFix,
  });

  final IssueType type;
  final String itemId;
  final String message;
  final List<String> relatedIds;
  final String? suggestedFix;
}

/// Graph health checker with auto-fix capabilities
class GraphDoctor {
  GraphDoctor(this.graph);

  final GianttGraph graph;
  final List<Issue> _issues = [];
  final List<Issue> _fixedIssues = [];

  /// Run a quick check and return number of issues found
  int quickCheck() {
    _issues.clear();
    _checkReferences();
    return _issues.length;
  }

  /// Run all checks and return detailed issues
  List<Issue> fullDiagnosis() {
    _issues.clear();
    _checkReferences();
    _checkChains();
    _checkTimeConstraints();
    _checkIdTitleConflicts();
    // Note: Orphan, chart, and tag checks commented out as they may not be actual issues
    return List.unmodifiable(_issues);
  }

  /// Get all issues of a specific type
  List<Issue> getIssuesByType(IssueType issueType) {
    return _issues.where((issue) => issue.type == issueType).toList();
  }

  /// Fix issues of a specific type or for a specific item
  List<Issue> fixIssues({IssueType? issueType, String? itemId}) {
    // Filter issues to fix
    var issuesToFix = _issues.toList();
    if (issueType != null) {
      issuesToFix = issuesToFix.where((issue) => issue.type == issueType).toList();
    }
    if (itemId != null) {
      issuesToFix = issuesToFix.where((issue) => issue.itemId == itemId).toList();
    }

    final fixed = <Issue>[];
    for (final issue in issuesToFix) {
      if (_fixIssue(issue)) {
        fixed.add(issue);
      }
    }

    // Remove fixed issues from the issues list
    for (final issue in fixed) {
      _issues.remove(issue);
    }

    _fixedIssues.addAll(fixed);
    return fixed;
  }

  /// Fix a specific issue. Returns true if fixed, false otherwise
  bool _fixIssue(Issue issue) {
    switch (issue.type) {
      case IssueType.danglingReference:
        return _fixDanglingReference(issue);
      case IssueType.incompleteChain:
        return _fixIncompleteChain(issue);
      case IssueType.orphanedItem:
      case IssueType.chartInconsistency:
      case IssueType.tagInconsistency:
        // These issues typically require manual intervention
        return false;
    }
  }

  /// Fix a dangling reference issue
  bool _fixDanglingReference(Issue issue) {
    final item = graph.items[issue.itemId];
    if (item == null) return false;

    // Find the relation type and target from the message
    String? relType;
    String? target;

    // Extract relation type from message
    for (final type in ['REQUIRES', 'BLOCKS', 'ANYOF', 'SUFFICIENT', 'SUPERCHARGES', 'INDICATES', 'TOGETHER', 'CONFLICTS']) {
      if (issue.message.toLowerCase().contains(type.toLowerCase())) {
        relType = type;
        break;
      }
    }

    if (relType == null) return false;

    // Extract target ID from message
    final match = RegExp(r"non-existent item '([^']+)'").firstMatch(issue.message);
    if (match == null) return false;
    target = match.group(1);

    // Remove the dangling reference
    final relations = Map<String, List<String>>.from(item.relations);
    if (relations.containsKey(relType) && relations[relType]!.contains(target)) {
      relations[relType]!.remove(target);
      if (relations[relType]!.isEmpty) {
        relations.remove(relType);
      }

      // Update the item
      final updatedItem = item.copyWith(relations: relations);
      graph.addItem(updatedItem);
      return true;
    }

    return false;
  }

  /// Fix an incomplete chain issue
  bool _fixIncompleteChain(Issue issue) {
    if (issue.relatedIds.isEmpty || issue.suggestedFix == null) {
      return false;
    }

    final item = graph.items[issue.itemId];
    final relatedItem = graph.items[issue.relatedIds.first];
    if (item == null || relatedItem == null) return false;

    // Parse the suggested fix: "giantt modify <target> --add <relation> <source>"
    final fixParts = issue.suggestedFix!.split(' ');
    if (fixParts.length < 6) return false;

    final targetId = fixParts[2];
    final relType = fixParts[4].toUpperCase();
    final sourceId = fixParts[5];

    final targetItem = graph.items[targetId];
    if (targetItem == null) return false;

    // Add the relation
    final relations = Map<String, List<String>>.from(targetItem.relations);
    relations.putIfAbsent(relType, () => []);
    
    if (!relations[relType]!.contains(sourceId)) {
      relations[relType]!.add(sourceId);
      
      // Update the item
      final updatedItem = targetItem.copyWith(relations: relations);
      graph.addItem(updatedItem);
      return true;
    }

    return false;
  }

  /// Check for dangling references in relations
  void _checkReferences() {
    for (final item in graph.items.values) {
      for (final entry in item.relations.entries) {
        final relType = entry.key;
        final targets = entry.value;
        
        for (final target in targets) {
          if (!graph.items.containsKey(target)) {
            _issues.add(Issue(
              type: IssueType.danglingReference,
              itemId: item.id,
              message: "References non-existent item '$target' in ${relType.toLowerCase()} relation",
              relatedIds: [target],
              suggestedFix: "Remove reference to '$target' from ${relType.toLowerCase()} relation",
            ));
          }
        }
      }
    }
  }

  /// Check for incomplete dependency chains
  void _checkChains() {
    final blocksMap = <String, Set<String>>{};
    final requiresMap = <String, Set<String>>{};
    final sufficientMap = <String, Set<String>>{};
    final anyofMap = <String, Set<String>>{};

    // Build relation maps
    for (final item in graph.items.values) {
      blocksMap[item.id] = Set<String>.from(item.relations['BLOCKS'] ?? []);
      requiresMap[item.id] = Set<String>.from(item.relations['REQUIRES'] ?? []);
      sufficientMap[item.id] = Set<String>.from(item.relations['SUFFICIENT'] ?? []);
      anyofMap[item.id] = Set<String>.from(item.relations['ANYOF'] ?? []);
    }

    // Check for items that block something but aren't required by it
    for (final entry in blocksMap.entries) {
      final itemId = entry.key;
      final blocksItems = entry.value;
      
      for (final blocked in blocksItems) {
        if (graph.items.containsKey(blocked)) {
          final blockedRequires = requiresMap[blocked] ?? <String>{};
          if (!blockedRequires.contains(itemId)) {
            _issues.add(Issue(
              type: IssueType.incompleteChain,
              itemId: itemId,
              message: "Item blocks '$blocked' but isn't required by it",
              relatedIds: [blocked],
              suggestedFix: "giantt modify $blocked --add requires $itemId",
            ));
          }
        }
      }
    }

    // Check for items that require something but aren't blocked by it
    for (final entry in requiresMap.entries) {
      final itemId = entry.key;
      final requiresItems = entry.value;
      
      for (final required in requiresItems) {
        if (graph.items.containsKey(required)) {
          final requiredBlocks = blocksMap[required] ?? <String>{};
          if (!requiredBlocks.contains(itemId)) {
            _issues.add(Issue(
              type: IssueType.incompleteChain,
              itemId: itemId,
              message: "Item requires '$required' but isn't blocked by it",
              relatedIds: [required],
              suggestedFix: "giantt modify $required --add blocks $itemId",
            ));
          }
        }
      }
    }

    // Check for items that are sufficient for something but aren't in an anyof relation
    for (final entry in sufficientMap.entries) {
      final itemId = entry.key;
      final sufficientItems = entry.value;
      
      for (final sufficient in sufficientItems) {
        if (graph.items.containsKey(sufficient)) {
          final sufficientAnyof = anyofMap[sufficient] ?? <String>{};
          if (!sufficientAnyof.contains(itemId)) {
            _issues.add(Issue(
              type: IssueType.incompleteChain,
              itemId: itemId,
              message: "Item is sufficient for '$sufficient' but doesn't have anyof relation with it",
              relatedIds: [sufficient],
              suggestedFix: "giantt modify $sufficient --add anyof $itemId",
            ));
          }
        }
      }
    }

    // Check for items that have anyof relation but aren't sufficient
    for (final entry in anyofMap.entries) {
      final itemId = entry.key;
      final anyofItems = entry.value;
      
      for (final anyofItem in anyofItems) {
        if (graph.items.containsKey(anyofItem)) {
          final anyofSufficient = sufficientMap[anyofItem] ?? <String>{};
          if (!anyofSufficient.contains(itemId)) {
            _issues.add(Issue(
              type: IssueType.incompleteChain,
              itemId: itemId,
              message: "Item has anyof relation with '$anyofItem' but isn't sufficient for it",
              relatedIds: [anyofItem],
              suggestedFix: "giantt modify $anyofItem --add sufficient $itemId",
            ));
          }
        }
      }
    }
  }

  /// Get all fixed issues
  List<Issue> get fixedIssues => List.unmodifiable(_fixedIssues);

  /// Get current issues
  List<Issue> get issues => List.unmodifiable(_issues);

  /// Check for time constraint violations
  void _checkTimeConstraints() {
    for (final item in graph.items.values) {
      if (item.timeConstraint != null) {
        // Check if time constraint is valid
        final constraint = item.timeConstraint!;
        
        // Add validation for time windows here
        // This would check if the constraint makes sense given dependencies
        // For now, we'll add a placeholder for future time constraint validation
        
        // Example: Check if constraint conflicts with dependencies
        final requiredItems = item.relations['REQUIRES'] ?? [];
        for (final requiredId in requiredItems) {
          final requiredItem = graph.items[requiredId];
          if (requiredItem?.timeConstraint != null) {
            // Could add logic here to check if time constraints are compatible
          }
        }
      }
    }
  }

  /// Check for ID and title conflicts
  void _checkIdTitleConflicts() {
    final itemsList = graph.items.values.toList();
    
    for (int i = 0; i < itemsList.length; i++) {
      final item1 = itemsList[i];
      
      for (int j = i + 1; j < itemsList.length; j++) {
        final item2 = itemsList[j];
        
        // Check for title conflicts
        if (item1.title.toLowerCase() == item2.title.toLowerCase()) {
          _issues.add(Issue(
            type: IssueType.chartInconsistency, // Reusing existing type for now
            itemId: item1.id,
            message: "Title conflicts with item '${item2.id}': '${item2.title}'",
            relatedIds: [item2.id],
            suggestedFix: "Rename one of the items to have a unique title",
          ));
        }
        
        // Check for ID/title cross-conflicts
        if (item1.id.toLowerCase() == item2.title.toLowerCase()) {
          _issues.add(Issue(
            type: IssueType.chartInconsistency, // Reusing existing type for now
            itemId: item1.id,
            message: "ID conflicts with title of item '${item2.id}': '${item2.title}'",
            relatedIds: [item2.id],
            suggestedFix: "Rename the ID or title to avoid conflict",
          ));
        }
      }
    }
  }
}


// =====================================================================
// FILE: lib/main.dart
// =====================================================================

import 'package:flutter/material.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  // This widget is the root of your application.
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Flutter Demo',
      theme: ThemeData(
        // This is the theme of your application.
        //
        // TRY THIS: Try running your application with "flutter run". You'll see
        // the application has a purple toolbar. Then, without quitting the app,
        // try changing the seedColor in the colorScheme below to Colors.green
        // and then invoke "hot reload" (save your changes or press the "hot
        // reload" button in a Flutter-supported IDE, or press "r" if you used
        // the command line to start the app).
        //
        // Notice that the counter didn't reset back to zero; the application
        // state is not lost during the reload. To reset the state, use hot
        // restart instead.
        //
        // This works for code too, not just values: Most code changes can be
        // tested with just a hot reload.
        colorScheme: ColorScheme.fromSeed(seedColor: Colors.deepPurple),
      ),
      home: const MyHomePage(title: 'Flutter Demo Home Page'),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({super.key, required this.title});

  // This widget is the home page of your application. It is stateful, meaning
  // that it has a State object (defined below) that contains fields that affect
  // how it looks.

  // This class is the configuration for the state. It holds the values (in this
  // case the title) provided by the parent (in this case the App widget) and
  // used by the build method of the State. Fields in a Widget subclass are
  // always marked "final".

  final String title;

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {
  int _counter = 0;

  void _incrementCounter() {
    setState(() {
      // This call to setState tells the Flutter framework that something has
      // changed in this State, which causes it to rerun the build method below
      // so that the display can reflect the updated values. If we changed
      // _counter without calling setState(), then the build method would not be
      // called again, and so nothing would appear to happen.
      _counter++;
    });
  }

  @override
  Widget build(BuildContext context) {
    // This method is rerun every time setState is called, for instance as done
    // by the _incrementCounter method above.
    //
    // The Flutter framework has been optimized to make rerunning build methods
    // fast, so that you can just rebuild anything that needs updating rather
    // than having to individually change instances of widgets.
    return Scaffold(
      appBar: AppBar(
        // TRY THIS: Try changing the color here to a specific color (to
        // Colors.amber, perhaps?) and trigger a hot reload to see the AppBar
        // change color while the other colors stay the same.
        backgroundColor: Theme.of(context).colorScheme.inversePrimary,
        // Here we take the value from the MyHomePage object that was created by
        // the App.build method, and use it to set our appbar title.
        title: Text(widget.title),
      ),
      body: Center(
        // Center is a layout widget. It takes a single child and positions it
        // in the middle of the parent.
        child: Column(
          // Column is also a layout widget. It takes a list of children and
          // arranges them vertically. By default, it sizes itself to fit its
          // children horizontally, and tries to be as tall as its parent.
          //
          // Column has various properties to control how it sizes itself and
          // how it positions its children. Here we use mainAxisAlignment to
          // center the children vertically; the main axis here is the vertical
          // axis because Columns are vertical (the cross axis would be
          // horizontal).
          //
          // TRY THIS: Invoke "debug painting" (choose the "Toggle Debug Paint"
          // action in the IDE, or press "p" in the console), to see the
          // wireframe for each widget.
          mainAxisAlignment: MainAxisAlignment.center,
          children: <Widget>[
            const Text('You have pushed the button this many times:'),
            Text(
              '$_counter',
              style: Theme.of(context).textTheme.headlineMedium,
            ),
          ],
        ),
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: _incrementCounter,
        tooltip: 'Increment',
        child: const Icon(Icons.add),
      ), // This trailing comma makes auto-formatting nicer for build methods.
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/test/widget_test.dart
// =====================================================================

// This is a basic Flutter widget test.
//
// To perform an interaction with a widget in your test, use the WidgetTester
// utility in the flutter_test package. For example, you can send tap and scroll
// gestures. You can also use WidgetTester to find child widgets in the widget
// tree, read text, and verify that the values of widget properties are correct.

import 'package:flutter/material.dart';
import 'package:flutter_test/flutter_test.dart';

import 'package:soradyne_app/main.dart';

void main() {
  testWidgets('Counter increments smoke test', (WidgetTester tester) async {
    // Build our app and trigger a frame.
    await tester.pumpWidget(const MyApp());

    // Verify that our counter starts at 0.
    expect(find.text('0'), findsOneWidget);
    expect(find.text('1'), findsNothing);

    // Tap the '+' icon and trigger a frame.
    await tester.tap(find.byIcon(Icons.add));
    await tester.pump();

    // Verify that our counter has incremented.
    expect(find.text('0'), findsNothing);
    expect(find.text('1'), findsOneWidget);
  });
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/main.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import 'screens/activity_selector_screen.dart';
import 'services/album_service.dart';
import 'theme/app_theme.dart';

void main() {
  runApp(const SoradyneApp());
}

class SoradyneApp extends StatelessWidget {
  const SoradyneApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MultiProvider(
      providers: [
        ChangeNotifierProvider(create: (_) => AlbumService()),
      ],
      child: MaterialApp(
        title: 'Soradyne',
        theme: AppTheme.lightTheme,
        darkTheme: AppTheme.darkTheme,
        themeMode: ThemeMode.system,
        home: const ActivitySelectorScreen(),
        debugShowCheckedModeBanner: false,
      ),
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/models/album.dart
// =====================================================================

class Album {
  final String id;
  final String name;
  final int itemCount;
  final DateTime createdAt;

  Album({
    required this.id,
    required this.name,
    required this.itemCount,
    required this.createdAt,
  });

  factory Album.fromJson(Map<String, dynamic> json) {
    return Album(
      id: json['id'],
      name: json['name'],
      itemCount: json['item_count'],
      createdAt: DateTime.now(), // TODO: Parse from API
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/models/media_item.dart
// =====================================================================

enum MediaType { image, video, audio }

class MediaItem {
  final String id;
  final String filename;
  final MediaType mediaType;
  final int size;
  final double rotation;
  final bool hasCrop;
  final int markupCount;
  final List<Comment> comments;

  MediaItem({
    required this.id,
    required this.filename,
    required this.mediaType,
    required this.size,
    this.rotation = 0.0,
    this.hasCrop = false,
    this.markupCount = 0,
    this.comments = const [],
  });

  // Progressive image data will be loaded via FFI at different resolutions
  List<int>? _thumbnailData;
  List<int>? _mediumData;
  List<int>? _highData;
  
  void setThumbnailData(List<int> data) {
    _thumbnailData = data;
  }
  
  void setMediumData(List<int> data) {
    _mediumData = data;
  }
  
  void setHighData(List<int> data) {
    _highData = data;
  }
  
  List<int>? get thumbnailData => _thumbnailData;
  List<int>? get mediumData => _mediumData;
  List<int>? get highData => _highData;
  
  bool get hasThumbnailData => _thumbnailData != null;
  bool get hasMediumData => _mediumData != null;
  bool get hasHighData => _highData != null;
  
  // Legacy compatibility
  List<int>? get imageData => _highData ?? _mediumData ?? _thumbnailData;
  bool get hasImageData => hasHighData || hasMediumData || hasThumbnailData;
  
  void setImageData(List<int> data) {
    // For backward compatibility, set as high resolution
    setHighData(data);
  }
  
  // For videos, we should use thumbnail data for display
  List<int>? get displayData {
    if (mediaType == MediaType.video) {
      return _thumbnailData ?? imageData;
    }
    return imageData;
  }
  
  bool get hasDisplayData {
    if (mediaType == MediaType.video) {
      return _thumbnailData != null || imageData != null;
    }
    return imageData != null;
  }

  factory MediaItem.fromJson(Map<String, dynamic> json, String albumId) {
    MediaType type;
    switch (json['media_type']) {
      case 'video':
        type = MediaType.video;
        break;
      case 'audio':
        type = MediaType.audio;
        break;
      default:
        type = MediaType.image;
    }

    return MediaItem(
      id: json['id'],
      filename: json['filename'],
      mediaType: type,
      size: json['size'],
      rotation: (json['rotation'] ?? 0.0).toDouble(),
      hasCrop: json['has_crop'] ?? false,
      markupCount: json['markup_count'] ?? 0,
      comments: (json['comments'] as List?)
          ?.map((c) => Comment.fromJson(c))
          .toList() ?? [],
    );
  }
}

class Comment {
  final String author;
  final String text;
  final DateTime timestamp;

  Comment({
    required this.author,
    required this.text,
    required this.timestamp,
  });

  factory Comment.fromJson(Map<String, dynamic> json) {
    return Comment(
      author: json['author'],
      text: json['text'],
      timestamp: DateTime.fromMillisecondsSinceEpoch(json['timestamp'] * 1000),
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/screens/activity_selector_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'album_list_screen.dart';

class ActivitySelectorScreen extends StatelessWidget {
  const ActivitySelectorScreen({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: Container(
        decoration: const BoxDecoration(
          gradient: LinearGradient(
            begin: Alignment.topLeft,
            end: Alignment.bottomRight,
            colors: [
              Color(0xFF667EEA),
              Color(0xFF764BA2),
            ],
          ),
        ),
        child: SafeArea(
          child: Padding(
            padding: const EdgeInsets.all(24.0),
            child: Column(
              crossAxisAlignment: CrossAxisAlignment.start,
              children: [
                const SizedBox(height: 40),
                const Text(
                  'Welcome to',
                  style: TextStyle(
                    color: Colors.white70,
                    fontSize: 24,
                    fontWeight: FontWeight.w300,
                  ),
                ),
                const Text(
                  'Soradyne',
                  style: TextStyle(
                    color: Colors.white,
                    fontSize: 48,
                    fontWeight: FontWeight.bold,
                  ),
                ),
                const SizedBox(height: 16),
                const Text(
                  'Choose your activity',
                  style: TextStyle(
                    color: Colors.white70,
                    fontSize: 18,
                  ),
                ),
                const SizedBox(height: 60),
                Expanded(
                  child: GridView.count(
                    crossAxisCount: 2,
                    crossAxisSpacing: 16,
                    mainAxisSpacing: 16,
                    children: [
                      _ActivityCard(
                        title: 'Photo Albums',
                        subtitle: 'Share & collaborate on media',
                        icon: Icons.photo_library_rounded,
                        onTap: () {
                          Navigator.push(
                            context,
                            MaterialPageRoute(
                              builder: (context) => const AlbumListScreen(),
                            ),
                          );
                        },
                      ),
                      _ActivityCard(
                        title: 'Flow Demo',
                        subtitle: 'Real-time data flows',
                        icon: Icons.stream_rounded,
                        onTap: () {
                          // TODO: Navigate to flow demo
                          ScaffoldMessenger.of(context).showSnackBar(
                            const SnackBar(
                              content: Text('Flow demo coming soon!'),
                            ),
                          );
                        },
                      ),
                      _ActivityCard(
                        title: 'Network Demo',
                        subtitle: 'Peer discovery & sync',
                        icon: Icons.network_check_rounded,
                        onTap: () {
                          // TODO: Navigate to network demo
                          ScaffoldMessenger.of(context).showSnackBar(
                            const SnackBar(
                              content: Text('Network demo coming soon!'),
                            ),
                          );
                        },
                      ),
                      _ActivityCard(
                        title: 'Storage Demo',
                        subtitle: 'Block storage system',
                        icon: Icons.storage_rounded,
                        onTap: () {
                          // TODO: Navigate to storage demo
                          ScaffoldMessenger.of(context).showSnackBar(
                            const SnackBar(
                              content: Text('Storage demo coming soon!'),
                            ),
                          );
                        },
                      ),
                    ],
                  ),
                ),
              ],
            ),
          ),
        ),
      ),
    );
  }
}

class _ActivityCard extends StatelessWidget {
  final String title;
  final String subtitle;
  final IconData icon;
  final VoidCallback onTap;

  const _ActivityCard({
    required this.title,
    required this.subtitle,
    required this.icon,
    required this.onTap,
  });

  @override
  Widget build(BuildContext context) {
    return Card(
      elevation: 8,
      shape: RoundedRectangleBorder(
        borderRadius: BorderRadius.circular(20),
      ),
      child: InkWell(
        onTap: onTap,
        borderRadius: BorderRadius.circular(20),
        child: Padding(
          padding: const EdgeInsets.all(20),
          child: Column(
            mainAxisAlignment: MainAxisAlignment.center,
            children: [
              Icon(
                icon,
                size: 48,
                color: Theme.of(context).colorScheme.primary,
              ),
              const SizedBox(height: 16),
              Text(
                title,
                style: const TextStyle(
                  fontSize: 18,
                  fontWeight: FontWeight.bold,
                ),
                textAlign: TextAlign.center,
              ),
              const SizedBox(height: 8),
              Text(
                subtitle,
                style: TextStyle(
                  fontSize: 14,
                  color: Colors.grey[600],
                ),
                textAlign: TextAlign.center,
              ),
            ],
          ),
        ),
      ),
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/screens/album_detail_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import 'package:photo_view/photo_view.dart';
import 'package:photo_view/photo_view_gallery.dart';
import 'package:image_picker/image_picker.dart';
import 'package:desktop_drop/desktop_drop.dart';
import 'dart:io';
import 'dart:typed_data';
import '../services/album_service.dart';
import '../models/album.dart';
import '../models/media_item.dart';
import '../widgets/progressive_image.dart';

class AlbumDetailScreen extends StatefulWidget {
  final Album album;

  const AlbumDetailScreen({super.key, required this.album});

  @override
  State<AlbumDetailScreen> createState() => _AlbumDetailScreenState();
}

class _AlbumDetailScreenState extends State<AlbumDetailScreen> {
  bool _isDragOver = false;

  @override
  void initState() {
    super.initState();
    WidgetsBinding.instance.addPostFrameCallback((_) {
      context.read<AlbumService>().loadAlbumItems(widget.album.id);
    });
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text(widget.album.name),
        actions: [
          IconButton(
            icon: const Icon(Icons.add_a_photo),
            onPressed: () {
              print('Add photo button pressed');
              _pickAndUploadImage();
            },
          ),
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: () => context.read<AlbumService>().loadAlbumItems(widget.album.id),
          ),
        ],
      ),
      body: Consumer<AlbumService>(
        builder: (context, albumService, child) {
          final items = albumService.getAlbumItems(widget.album.id);
          
          print('Album ${widget.album.id} has ${items.length} items');

          if (albumService.isLoading) {
            return const Center(
              child: Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: [
                  CircularProgressIndicator(),
                  SizedBox(height: 16),
                  Text('Loading album contents...'),
                ],
              ),
            );
          }

          if (items.isEmpty) {
            return Center(
              child: DropTarget(
                onDragDone: (detail) {
                  print('Files dropped: ${detail.files.map((f) => f.path).toList()}');
                  _handleDroppedFiles(detail.files.map((f) => File(f.path)).toList());
                },
                onDragEntered: (detail) {
                  print('Drag entered');
                  setState(() {
                    _isDragOver = true;
                  });
                },
                onDragExited: (detail) {
                  print('Drag exited');
                  setState(() {
                    _isDragOver = false;
                  });
                },
                child: AnimatedContainer(
                  duration: const Duration(milliseconds: 200),
                  decoration: BoxDecoration(
                    color: _isDragOver ? Colors.blue.withOpacity(0.1) : Colors.transparent,
                    border: Border.all(
                      color: _isDragOver ? Colors.blue : Colors.grey.shade300,
                      width: 2,
                      style: BorderStyle.solid,
                    ),
                    borderRadius: BorderRadius.circular(12),
                  ),
                  padding: const EdgeInsets.all(40),
                  child: Column(
                    mainAxisAlignment: MainAxisAlignment.center,
                    children: [
                      Icon(
                        _isDragOver ? Icons.cloud_upload : Icons.photo_outlined,
                        size: 64,
                        color: _isDragOver ? Colors.blue : Colors.grey,
                      ),
                      const SizedBox(height: 16),
                      Text(
                        _isDragOver ? 'Drop files here!' : 'No media yet',
                        style: TextStyle(
                          fontSize: 18,
                          color: _isDragOver ? Colors.blue : Colors.grey,
                          fontWeight: _isDragOver ? FontWeight.bold : FontWeight.normal,
                        ),
                      ),
                      const SizedBox(height: 8),
                      Text(
                        _isDragOver 
                          ? 'Release to upload photos, videos, or audio'
                          : 'Supports JPG, PNG, MP4, MOV, MP3, WAV, and more',
                        style: TextStyle(
                          color: _isDragOver ? Colors.blue : Colors.grey,
                        ),
                      ),
                      const SizedBox(height: 24),
                      ElevatedButton.icon(
                        onPressed: () {
                          print('Add Media button pressed');
                          _pickAndUploadImage();
                        },
                        icon: const Icon(Icons.add_a_photo),
                        label: const Text('Add Media'),
                      ),
                    ],
                  ),
                ),
              ),
            );
          }

          return DropTarget(
            onDragDone: (detail) {
              print('Files dropped on grid: ${detail.files.map((f) => f.path).toList()}');
              _handleDroppedFiles(detail.files.map((f) => File(f.path)).toList());
            },
            onDragEntered: (detail) {
              print('Grid drag entered');
              setState(() {
                _isDragOver = true;
              });
            },
            onDragExited: (detail) {
              print('Grid drag exited');
              setState(() {
                _isDragOver = false;
              });
            },
            child: Container(
              decoration: _isDragOver ? BoxDecoration(
                color: Colors.blue.withOpacity(0.1),
                border: Border.all(color: Colors.blue, width: 2),
                borderRadius: BorderRadius.circular(8),
              ) : null,
              child: Padding(
                padding: const EdgeInsets.all(8.0),
                child: GridView.builder(
                  gridDelegate: const SliverGridDelegateWithFixedCrossAxisCount(
                    crossAxisCount: 3,
                    crossAxisSpacing: 4,
                    mainAxisSpacing: 4,
                  ),
                  itemCount: items.length,
                  itemBuilder: (context, index) {
                    final item = items[index];
                    return _MediaThumbnail(
                      item: item,
                      albumId: widget.album.id,
                      onTap: () => _openMediaViewer(context, items, index),
                    );
                  },
                ),
              ),
            ),
          );
        },
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: () {
          print('Floating action button pressed');
          _pickAndUploadImage();
        },
        child: const Icon(Icons.add),
      ),
    );
  }

  void _pickAndUploadImage() async {
    print('_pickAndUploadImage called');
    _showMediaPickerDialog();
  }

  void _showMediaPickerDialog() {
    showModalBottomSheet(
      context: context,
      builder: (context) => SafeArea(
        child: Column(
          mainAxisSize: MainAxisSize.min,
          children: [
            ListTile(
              leading: const Icon(Icons.photo_camera),
              title: const Text('Take Photo'),
              onTap: () {
                Navigator.pop(context);
                _pickMedia(MediaType.image, ImageSource.camera);
              },
            ),
            ListTile(
              leading: const Icon(Icons.photo_library),
              title: const Text('Choose Photos'),
              onTap: () {
                Navigator.pop(context);
                _pickMedia(MediaType.image, ImageSource.gallery);
              },
            ),
            ListTile(
              leading: const Icon(Icons.videocam),
              title: const Text('Record Video'),
              onTap: () {
                Navigator.pop(context);
                _pickMedia(MediaType.video, ImageSource.camera);
              },
            ),
            ListTile(
              leading: const Icon(Icons.video_library),
              title: const Text('Choose Videos'),
              onTap: () {
                Navigator.pop(context);
                _pickMedia(MediaType.video, ImageSource.gallery);
              },
            ),
            ListTile(
              leading: const Icon(Icons.audiotrack),
              title: const Text('Choose Audio'),
              onTap: () {
                Navigator.pop(context);
                _pickAudioFile();
              },
            ),
          ],
        ),
      ),
    );
  }

  void _pickMedia(MediaType mediaType, ImageSource source) async {
    print('_pickMedia called with type: $mediaType, source: $source');
    
    try {
      final picker = ImagePicker();
      XFile? pickedFile;
      
      switch (mediaType) {
        case MediaType.image:
          print('Picking image...');
          pickedFile = await picker.pickImage(
            source: source,
            imageQuality: 85,
          );
          break;
        case MediaType.video:
          print('Picking video...');
          pickedFile = await picker.pickVideo(
            source: source,
            maxDuration: const Duration(minutes: 10), // Reasonable limit
          );
          break;
        case MediaType.audio:
          // Audio picking will be handled separately
          return;
      }
      
      print('Media picker returned: ${pickedFile?.path ?? 'null'}');
      
      if (pickedFile != null && mounted) {
        print('Media picked successfully: ${pickedFile.path}');
        await _uploadPickedFile(pickedFile);
      } else {
        print('No media was picked or widget not mounted');
      }
    } catch (e) {
      print('Error in _pickMedia: $e');
      print('Error type: ${e.runtimeType}');
      print('Stack trace: ${StackTrace.current}');
      
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Error picking media: $e'),
            backgroundColor: Colors.red,
          ),
        );
      }
    }
  }

  void _pickAudioFile() async {
    print('_pickAudioFile called');
    
    try {
      // For audio files, we'll need to use file_picker package
      // For now, show a message that audio support is coming
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          const SnackBar(
            content: Text('Audio file support coming soon! For now, you can drag and drop audio files.'),
            duration: Duration(seconds: 3),
          ),
        );
      }
    } catch (e) {
      print('Error in _pickAudioFile: $e');
      
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Error picking audio: $e'),
            backgroundColor: Colors.red,
          ),
        );
      }
    }
  }

  Future<void> _uploadPickedFile(XFile pickedFile) async {
    // Show loading indicator
    if (mounted) {
      ScaffoldMessenger.of(context).showSnackBar(
        const SnackBar(
          content: Row(
            children: [
              SizedBox(
                width: 20,
                height: 20,
                child: CircularProgressIndicator(strokeWidth: 2),
              ),
              SizedBox(width: 16),
              Text('Uploading media...'),
            ],
          ),
          duration: Duration(seconds: 10),
        ),
      );
    }
    
    final file = File(pickedFile.path);
    print('Attempting to upload file: ${file.path}');
    print('File exists: ${await file.exists()}');
    print('File size: ${await file.length()} bytes');
    
    final success = await context.read<AlbumService>().uploadMedia(widget.album.id, file);
    
    if (mounted) {
      ScaffoldMessenger.of(context).hideCurrentSnackBar();
      
      if (success) {
        // Refresh album items after successful upload
        context.read<AlbumService>().loadAlbumItems(widget.album.id);
        
        ScaffoldMessenger.of(context).showSnackBar(
          const SnackBar(
            content: Text('Media uploaded successfully!'),
            backgroundColor: Colors.green,
          ),
        );
      } else {
        ScaffoldMessenger.of(context).showSnackBar(
          const SnackBar(
            content: Text('Failed to upload media'),
            backgroundColor: Colors.red,
          ),
        );
      }
    }
  }


  void _handleDroppedFiles(List<File> files) async {
    print('Handling ${files.length} dropped files');
    
    for (final file in files) {
      print('Processing dropped file: ${file.path}');
      
      // Show loading indicator
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Row(
              children: [
                const SizedBox(
                  width: 20,
                  height: 20,
                  child: CircularProgressIndicator(strokeWidth: 2),
                ),
                const SizedBox(width: 16),
                Expanded(child: Text('Uploading ${file.path.split('/').last}...')),
              ],
            ),
            duration: const Duration(seconds: 10),
          ),
        );
      }
      
      final success = await context.read<AlbumService>().uploadMedia(widget.album.id, file);
      
      if (mounted) {
        ScaffoldMessenger.of(context).hideCurrentSnackBar();
        
        if (success) {
          // Refresh album items after successful upload
          context.read<AlbumService>().loadAlbumItems(widget.album.id);
          
          ScaffoldMessenger.of(context).showSnackBar(
            SnackBar(
              content: Text('${file.path.split('/').last} uploaded successfully!'),
              backgroundColor: Colors.green,
            ),
          );
        } else {
          ScaffoldMessenger.of(context).showSnackBar(
            SnackBar(
              content: Text('Failed to upload ${file.path.split('/').last}'),
              backgroundColor: Colors.red,
            ),
          );
        }
      }
    }
  }

  void _openMediaViewer(BuildContext context, List<MediaItem> items, int initialIndex) {
    Navigator.push(
      context,
      MaterialPageRoute(
        builder: (context) => MediaViewerScreen(
          items: items,
          initialIndex: initialIndex,
          albumId: widget.album.id,
        ),
      ),
    );
  }
}

class _MediaThumbnail extends StatelessWidget {
  final MediaItem item;
  final String albumId;
  final VoidCallback onTap;

  const _MediaThumbnail({
    required this.item,
    required this.albumId,
    required this.onTap,
  });

  @override
  Widget build(BuildContext context) {
    return GestureDetector(
      onTap: onTap,
      child: Container(
        decoration: BoxDecoration(
          borderRadius: BorderRadius.circular(8),
          border: Border.all(color: Colors.grey.shade300),
        ),
        child: ClipRRect(
          borderRadius: BorderRadius.circular(8),
          child: Stack(
            fit: StackFit.expand,
            children: [
              // Progressive image loading
              ProgressiveImage(
                mediaItem: item,
                albumId: albumId,
                fit: BoxFit.cover,
              ),
              if (item.mediaType == MediaType.video)
                const Center(
                  child: Icon(
                    Icons.play_circle_outline,
                    color: Colors.white,
                    size: 32,
                  ),
                ),
              if (item.mediaType == MediaType.audio)
                const Center(
                  child: Icon(
                    Icons.audiotrack,
                    color: Colors.white,
                    size: 32,
                  ),
                ),
            ],
          ),
        ),
      ),
    );
  }

  IconData _getMediaIcon(MediaType type) {
    switch (type) {
      case MediaType.video:
        return Icons.videocam;
      case MediaType.audio:
        return Icons.audiotrack;
      case MediaType.image:
      default:
        return Icons.image;
    }
  }
}

class MediaViewerScreen extends StatefulWidget {
  final List<MediaItem> items;
  final int initialIndex;
  final String albumId;

  const MediaViewerScreen({
    super.key,
    required this.items,
    required this.initialIndex,
    required this.albumId,
  });

  @override
  State<MediaViewerScreen> createState() => _MediaViewerScreenState();
}

class _MediaViewerScreenState extends State<MediaViewerScreen> {
  late PageController _pageController;
  int _currentIndex = 0;

  @override
  void initState() {
    super.initState();
    _currentIndex = widget.initialIndex;
    _pageController = PageController(initialPage: widget.initialIndex);
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      backgroundColor: Colors.black,
      appBar: AppBar(
        backgroundColor: Colors.black,
        iconTheme: const IconThemeData(color: Colors.white),
        title: Text(
          '${_currentIndex + 1} of ${widget.items.length}',
          style: const TextStyle(color: Colors.white),
        ),
      ),
      body: PhotoViewGallery.builder(
        pageController: _pageController,
        itemCount: widget.items.length,
        builder: (context, index) {
          final item = widget.items[index];
          return PhotoViewGalleryPageOptions(
            imageProvider: item.hasDisplayData
                ? MemoryImage(Uint8List.fromList(item.displayData!))
                : MemoryImage(Uint8List.fromList([0])), // Empty placeholder
            minScale: PhotoViewComputedScale.contained,
            maxScale: PhotoViewComputedScale.covered * 2,
            heroAttributes: PhotoViewHeroAttributes(tag: item.id),
          );
        },
        onPageChanged: (index) {
          setState(() {
            _currentIndex = index;
          });
        },
        scrollPhysics: const BouncingScrollPhysics(),
        backgroundDecoration: const BoxDecoration(color: Colors.black),
      ),
    );
  }

  @override
  void dispose() {
    _pageController.dispose();
    super.dispose();
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/screens/album_list_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import '../services/album_service.dart';
import '../models/album.dart';
import 'album_detail_screen.dart';

class AlbumListScreen extends StatefulWidget {
  const AlbumListScreen({super.key});

  @override
  State<AlbumListScreen> createState() => _AlbumListScreenState();
}

class _AlbumListScreenState extends State<AlbumListScreen> {
  @override
  void initState() {
    super.initState();
    WidgetsBinding.instance.addPostFrameCallback((_) {
      context.read<AlbumService>().loadAlbums();
    });
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Photo Albums'),
        actions: [
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: () => context.read<AlbumService>().loadAlbums(),
          ),
        ],
      ),
      body: Consumer<AlbumService>(
        builder: (context, albumService, child) {
          if (albumService.isLoading) {
            return const Center(
              child: Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: [
                  CircularProgressIndicator(),
                  SizedBox(height: 16),
                  Text('Loading albums...'),
                ],
              ),
            );
          }

          if (albumService.error != null) {
            return Center(
              child: Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: [
                  const Icon(
                    Icons.error_outline,
                    size: 64,
                    color: Colors.red,
                  ),
                  const SizedBox(height: 16),
                  Text(
                    'Error: ${albumService.error}',
                    style: const TextStyle(color: Colors.red),
                    textAlign: TextAlign.center,
                  ),
                  const SizedBox(height: 16),
                  ElevatedButton(
                    onPressed: () => albumService.loadAlbums(),
                    child: const Text('Retry'),
                  ),
                ],
              ),
            );
          }

          if (albumService.albums.isEmpty) {
            return Center(
              child: Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: [
                  const Icon(
                    Icons.photo_library_outlined,
                    size: 64,
                    color: Colors.grey,
                  ),
                  const SizedBox(height: 16),
                  const Text(
                    'No albums yet',
                    style: TextStyle(
                      fontSize: 18,
                      color: Colors.grey,
                    ),
                  ),
                  const SizedBox(height: 8),
                  const Text(
                    'Create your first album to get started',
                    style: TextStyle(
                      color: Colors.grey,
                    ),
                  ),
                  const SizedBox(height: 24),
                  ElevatedButton.icon(
                    onPressed: () => _showCreateAlbumDialog(context),
                    icon: const Icon(Icons.add),
                    label: const Text('Create Album'),
                  ),
                ],
              ),
            );
          }

          return Padding(
            padding: const EdgeInsets.all(16.0),
            child: GridView.builder(
              gridDelegate: const SliverGridDelegateWithFixedCrossAxisCount(
                crossAxisCount: 2,
                crossAxisSpacing: 16,
                mainAxisSpacing: 16,
                childAspectRatio: 0.8,
              ),
              itemCount: albumService.albums.length,
              itemBuilder: (context, index) {
                final album = albumService.albums[index];
                return _AlbumCard(
                  album: album,
                  onTap: () {
                    Navigator.push(
                      context,
                      MaterialPageRoute(
                        builder: (context) => AlbumDetailScreen(album: album),
                      ),
                    );
                  },
                );
              },
            ),
          );
        },
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: () => _showCreateAlbumDialog(context),
        child: const Icon(Icons.add),
      ),
    );
  }

  void _showCreateAlbumDialog(BuildContext context) {
    final controller = TextEditingController();
    
    showDialog(
      context: context,
      builder: (context) => AlertDialog(
        title: const Text('Create Album'),
        content: TextField(
          controller: controller,
          decoration: const InputDecoration(
            labelText: 'Album Name',
            hintText: 'Enter album name',
          ),
          autofocus: true,
        ),
        actions: [
          TextButton(
            onPressed: () => Navigator.pop(context),
            child: const Text('Cancel'),
          ),
          ElevatedButton(
            onPressed: () async {
              if (controller.text.trim().isNotEmpty) {
                Navigator.pop(context);
                final success = await context.read<AlbumService>().createAlbum(controller.text.trim());
                if (success && mounted) {
                  ScaffoldMessenger.of(context).showSnackBar(
                    const SnackBar(content: Text('Album created successfully!')),
                  );
                } else if (mounted) {
                  ScaffoldMessenger.of(context).showSnackBar(
                    const SnackBar(content: Text('Failed to create album')),
                  );
                }
              }
            },
            child: const Text('Create'),
          ),
        ],
      ),
    );
  }
}

class _AlbumCard extends StatelessWidget {
  final Album album;
  final VoidCallback onTap;

  const _AlbumCard({
    required this.album,
    required this.onTap,
  });

  @override
  Widget build(BuildContext context) {
    return Card(
      elevation: 4,
      shape: RoundedRectangleBorder(
        borderRadius: BorderRadius.circular(16),
      ),
      child: InkWell(
        onTap: onTap,
        borderRadius: BorderRadius.circular(16),
        child: Padding(
          padding: const EdgeInsets.all(16),
          child: Column(
            crossAxisAlignment: CrossAxisAlignment.start,
            children: [
              Expanded(
                child: Container(
                  width: double.infinity,
                  decoration: BoxDecoration(
                    color: Theme.of(context).colorScheme.primary.withOpacity(0.1),
                    borderRadius: BorderRadius.circular(12),
                  ),
                  child: Icon(
                    Icons.photo_library_rounded,
                    size: 48,
                    color: Theme.of(context).colorScheme.primary,
                  ),
                ),
              ),
              const SizedBox(height: 12),
              Text(
                album.name,
                style: const TextStyle(
                  fontSize: 16,
                  fontWeight: FontWeight.bold,
                ),
                maxLines: 2,
                overflow: TextOverflow.ellipsis,
              ),
              const SizedBox(height: 4),
              Text(
                '${album.itemCount} items',
                style: TextStyle(
                  fontSize: 14,
                  color: Colors.grey[600],
                ),
              ),
            ],
          ),
        ),
      ),
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/theme/app_theme.dart
// =====================================================================

import 'package:flutter/material.dart';

class AppTheme {
  static const Color primaryColor = Color(0xFF667EEA);
  static const Color secondaryColor = Color(0xFF764BA2);
  static const Color accentColor = Color(0xFF48BB78);
  static const Color errorColor = Color(0xFFF56565);
  
  static ThemeData lightTheme = ThemeData(
    useMaterial3: true,
    colorScheme: ColorScheme.fromSeed(
      seedColor: primaryColor,
      brightness: Brightness.light,
    ),
    appBarTheme: const AppBarTheme(
      backgroundColor: Colors.transparent,
      elevation: 0,
      centerTitle: true,
      titleTextStyle: TextStyle(
        color: Colors.black87,
        fontSize: 20,
        fontWeight: FontWeight.w600,
      ),
    ),
    cardTheme: CardThemeData(
      elevation: 4,
      shape: RoundedRectangleBorder(
        borderRadius: BorderRadius.circular(16),
      ),
    ),
    elevatedButtonTheme: ElevatedButtonThemeData(
      style: ElevatedButton.styleFrom(
        padding: const EdgeInsets.symmetric(horizontal: 24, vertical: 12),
        shape: RoundedRectangleBorder(
          borderRadius: BorderRadius.circular(12),
        ),
      ),
    ),
  );
  
  static ThemeData darkTheme = ThemeData(
    useMaterial3: true,
    colorScheme: ColorScheme.fromSeed(
      seedColor: primaryColor,
      brightness: Brightness.dark,
    ),
    appBarTheme: const AppBarTheme(
      backgroundColor: Colors.transparent,
      elevation: 0,
      centerTitle: true,
      titleTextStyle: TextStyle(
        color: Colors.white,
        fontSize: 20,
        fontWeight: FontWeight.w600,
      ),
    ),
    cardTheme: CardThemeData(
      elevation: 4,
      shape: RoundedRectangleBorder(
        borderRadius: BorderRadius.circular(16),
      ),
    ),
  );
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/ffi/soradyne_bindings.dart
// =====================================================================

import 'dart:ffi';
import 'dart:io';
import 'package:ffi/ffi.dart';
import 'package:path/path.dart' as path;

// Define the C function signatures
typedef SoradyneInitC = Int32 Function();
typedef SoradyneInit = int Function();

typedef SoradyneGetAlbumsC = Pointer<Utf8> Function();
typedef SoradyneGetAlbums = Pointer<Utf8> Function();

typedef SoradyneCreateAlbumC = Pointer<Utf8> Function(Pointer<Utf8>);
typedef SoradyneCreateAlbum = Pointer<Utf8> Function(Pointer<Utf8>);

typedef SoradyneGetAlbumItemsC = Pointer<Utf8> Function(Pointer<Utf8>);
typedef SoradyneGetAlbumItems = Pointer<Utf8> Function(Pointer<Utf8>);

typedef SoradyneUploadMediaC = Int32 Function(Pointer<Utf8>, Pointer<Utf8>);
typedef SoradyneUploadMedia = int Function(Pointer<Utf8>, Pointer<Utf8>);

typedef SoradyneFreeStringC = Void Function(Pointer<Utf8>);
typedef SoradyneFreeString = void Function(Pointer<Utf8>);

typedef SoradyneGetMediaDataC = Int32 Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);
typedef SoradyneGetMediaData = int Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);

typedef SoradyneGetMediaThumbnailC = Int32 Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);
typedef SoradyneGetMediaThumbnail = int Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);

typedef SoradyneGetMediaMediumC = Int32 Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);
typedef SoradyneGetMediaMedium = int Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);

typedef SoradyneGetMediaHighC = Int32 Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);
typedef SoradyneGetMediaHigh = int Function(Pointer<Utf8>, Pointer<Utf8>, Pointer<Pointer<Uint8>>, Pointer<Size>);

typedef SoradyneFreeMediaDataC = Void Function(Pointer<Uint8>, Size);
typedef SoradyneFreeMediaData = void Function(Pointer<Uint8>, int);

typedef SoradyneGetStorageStatusC = Pointer<Utf8> Function();
typedef SoradyneGetStorageStatus = Pointer<Utf8> Function();

typedef SoradyneRefreshStorageC = Int32 Function();
typedef SoradyneRefreshStorage = int Function();

typedef SoradyneCleanupC = Void Function();
typedef SoradyneCleanup = void Function();

class SoradyneBindings {
  late final DynamicLibrary _lib;
  late final SoradyneInit _init;
  late final SoradyneGetAlbums _getAlbums;
  late final SoradyneCreateAlbum _createAlbum;
  late final SoradyneGetAlbumItems _getAlbumItems;
  late final SoradyneUploadMedia _uploadMedia;
  late final SoradyneGetMediaData _getMediaData;
  late final SoradyneGetMediaThumbnail _getMediaThumbnail;
  late final SoradyneGetMediaMedium _getMediaMedium;
  late final SoradyneGetMediaHigh _getMediaHigh;
  late final SoradyneFreeMediaData _freeMediaData;
  late final SoradyneFreeString _freeString;
  late final SoradyneGetStorageStatus? _getStorageStatus;
  late final SoradyneRefreshStorage? _refreshStorage;
  late final SoradyneCleanup _cleanup;

  SoradyneBindings() {
    // Load the dynamic library
    if (Platform.isMacOS) {
      // For macOS, the library should be bundled with the app
      try {
        // Try loading from the app bundle's MacOS directory first
        final executablePath = Platform.resolvedExecutable;
        final appDir = path.dirname(executablePath);
        final dylibPath = path.join(appDir, 'libsoradyne.dylib');
        print('Attempting to load dylib from: $dylibPath');
        _lib = DynamicLibrary.open(dylibPath);
        print('Successfully loaded dylib from: $dylibPath');
      } catch (e) {
        print('Failed to load from app bundle: $e');
        print('Attempting fallback to loading by name...');
        // Fallback to loading by name
        _lib = DynamicLibrary.open('libsoradyne.dylib');
        print('Successfully loaded dylib by name');
      }
    } else if (Platform.isLinux) {
      _lib = DynamicLibrary.open('libsoradyne.so');
    } else if (Platform.isWindows) {
      _lib = DynamicLibrary.open('soradyne.dll');
    } else if (Platform.isAndroid) {
      _lib = DynamicLibrary.open('libsoradyne.so');
    } else if (Platform.isIOS) {
      _lib = DynamicLibrary.process();
    } else {
      throw UnsupportedError('Platform not supported');
    }

    // Bind the functions
    _init = _lib.lookupFunction<SoradyneInitC, SoradyneInit>('soradyne_init');
    _getAlbums = _lib.lookupFunction<SoradyneGetAlbumsC, SoradyneGetAlbums>('soradyne_get_albums');
    _createAlbum = _lib.lookupFunction<SoradyneCreateAlbumC, SoradyneCreateAlbum>('soradyne_create_album');
    _getAlbumItems = _lib.lookupFunction<SoradyneGetAlbumItemsC, SoradyneGetAlbumItems>('soradyne_get_album_items');
    _uploadMedia = _lib.lookupFunction<SoradyneUploadMediaC, SoradyneUploadMedia>('soradyne_upload_media');
    _getMediaData = _lib.lookupFunction<SoradyneGetMediaDataC, SoradyneGetMediaData>('soradyne_get_media_data');
    _getMediaThumbnail = _lib.lookupFunction<SoradyneGetMediaThumbnailC, SoradyneGetMediaThumbnail>('soradyne_get_media_thumbnail');
    _getMediaMedium = _lib.lookupFunction<SoradyneGetMediaMediumC, SoradyneGetMediaMedium>('soradyne_get_media_medium');
    _getMediaHigh = _lib.lookupFunction<SoradyneGetMediaHighC, SoradyneGetMediaHigh>('soradyne_get_media_high');
    _freeMediaData = _lib.lookupFunction<SoradyneFreeMediaDataC, SoradyneFreeMediaData>('soradyne_free_media_data');
    _freeString = _lib.lookupFunction<SoradyneFreeStringC, SoradyneFreeString>('soradyne_free_string');
    
    // Try to bind the new functions, but don't crash if they're missing
    try {
      _getStorageStatus = _lib.lookupFunction<SoradyneGetStorageStatusC, SoradyneGetStorageStatus>('soradyne_get_storage_status');
      _refreshStorage = _lib.lookupFunction<SoradyneRefreshStorageC, SoradyneRefreshStorage>('soradyne_refresh_storage');
    } catch (e) {
      print('Note: Storage status functions not available (using older library)');
      _getStorageStatus = null;
      _refreshStorage = null;
    }
    
    _cleanup = _lib.lookupFunction<SoradyneCleanupC, SoradyneCleanup>('soradyne_cleanup');
  }

  int init() {
    return _init();
  }

  String getAlbums() {
    final ptr = _getAlbums();
    final result = ptr.toDartString();
    _freeString(ptr);
    return result;
  }

  String createAlbum(String name) {
    final namePtr = name.toNativeUtf8();
    final resultPtr = _createAlbum(namePtr);
    final result = resultPtr.toDartString();
    _freeString(resultPtr);
    malloc.free(namePtr);
    return result;
  }

  String getAlbumItems(String albumId) {
    final albumIdPtr = albumId.toNativeUtf8();
    final resultPtr = _getAlbumItems(albumIdPtr);
    final result = resultPtr.toDartString();
    _freeString(resultPtr);
    malloc.free(albumIdPtr);
    return result;
  }

  int uploadMedia(String albumId, String filePath) {
    print('FFI uploadMedia called with albumId: $albumId, filePath: $filePath');
    
    final albumIdPtr = albumId.toNativeUtf8();
    final filePathPtr = filePath.toNativeUtf8();
    
    print('Calling native uploadMedia function...');
    final result = _uploadMedia(albumIdPtr, filePathPtr);
    print('Native uploadMedia returned: $result');
    
    malloc.free(albumIdPtr);
    malloc.free(filePathPtr);
    return result;
  }

  List<int>? getMediaData(String albumId, String mediaId) {
    return _getMediaAtResolution(albumId, mediaId, _getMediaData, 'getMediaData');
  }

  List<int>? getMediaThumbnail(String albumId, String mediaId) {
    return _getMediaAtResolution(albumId, mediaId, _getMediaThumbnail, 'getMediaThumbnail');
  }

  List<int>? getMediaMedium(String albumId, String mediaId) {
    return _getMediaAtResolution(albumId, mediaId, _getMediaMedium, 'getMediaMedium');
  }

  List<int>? getMediaHigh(String albumId, String mediaId) {
    return _getMediaAtResolution(albumId, mediaId, _getMediaHigh, 'getMediaHigh');
  }

  List<int>? _getMediaAtResolution(String albumId, String mediaId, Function nativeFunction, String functionName) {
    print('FFI $functionName called with albumId: $albumId, mediaId: $mediaId');
    
    final albumIdPtr = albumId.toNativeUtf8();
    final mediaIdPtr = mediaId.toNativeUtf8();
    final dataPtrPtr = malloc<Pointer<Uint8>>();
    final sizePtr = malloc<Size>();
    
    try {
      print('Calling native $functionName function...');
      final result = nativeFunction(albumIdPtr, mediaIdPtr, dataPtrPtr, sizePtr);
      print('Native $functionName returned: $result');
      
      if (result == 0) {
        final dataPtr = dataPtrPtr.value;
        final size = sizePtr.value;
        
        if (dataPtr != nullptr && size > 0) {
          print('Retrieved media data: $size bytes');
          final data = dataPtr.asTypedList(size).toList();
          _freeMediaData(dataPtr, size);
          return data;
        }
      }
      
      return null;
    } finally {
      malloc.free(albumIdPtr);
      malloc.free(mediaIdPtr);
      malloc.free(dataPtrPtr);
      malloc.free(sizePtr);
    }
  }

  String getStorageStatus() {
    if (_getStorageStatus == null) {
      return '{"available_devices": 0, "required_threshold": 3, "can_read_data": false, "missing_devices": 3, "device_paths": []}';
    }
    final ptr = _getStorageStatus!();
    final result = ptr.toDartString();
    _freeString(ptr);
    return result;
  }

  int refreshStorage() {
    if (_refreshStorage == null) {
      return -1; // Not available
    }
    return _refreshStorage!();
  }

  void cleanup() {
    _cleanup();
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/services/album_service.dart
// =====================================================================

import 'package:flutter/foundation.dart';
import 'dart:convert';
import 'dart:io';
import '../models/album.dart';
import '../models/media_item.dart';
import '../ffi/soradyne_bindings.dart';

class AlbumService extends ChangeNotifier {
  late final SoradyneBindings _bindings;
  
  List<Album> _albums = [];
  Map<String, List<MediaItem>> _albumItems = {};
  bool _isLoading = false;
  String? _error;
  bool _initialized = false;

  List<Album> get albums => _albums;
  bool get isLoading => _isLoading;
  String? get error => _error;

  AlbumService() {
    _initializeBindings();
  }

  void _initializeBindings() {
    try {
      debugPrint('Creating SoradyneBindings...');
      _bindings = SoradyneBindings();
      debugPrint('SoradyneBindings created successfully, calling init...');
      final result = _bindings.init();
      debugPrint('Init returned: $result');
      if (result == 0) {
        _initialized = true;
        debugPrint('Soradyne FFI initialized successfully');
      } else {
        _error = 'Failed to initialize Soradyne FFI (error code: $result)';
        debugPrint('Failed to initialize Soradyne FFI: $result');
      }
    } catch (e) {
      _error = 'FFI initialization error: $e';
      debugPrint('FFI initialization error: $e');
      debugPrint('Stack trace: ${StackTrace.current}');
    }
  }

  List<MediaItem> getAlbumItems(String albumId) {
    return _albumItems[albumId] ?? [];
  }

  Future<void> loadAlbums() async {
    if (!_initialized) {
      _error = 'Soradyne not initialized';
      notifyListeners();
      return;
    }

    _isLoading = true;
    _error = null;
    notifyListeners();

    try {
      final albumsJson = _bindings.getAlbums();
      final List<dynamic> albumsList = json.decode(albumsJson);
      _albums = albumsList.map((json) => Album.fromJson(json)).toList();
      _error = null;
      debugPrint('Loaded ${_albums.length} albums via FFI');
    } catch (e) {
      _error = 'Error loading albums: $e';
      debugPrint('Error loading albums: $e');
    }

    _isLoading = false;
    notifyListeners();
  }

  Future<void> loadAlbumItems(String albumId) async {
    if (!_initialized) {
      debugPrint('Cannot load album items: FFI not initialized');
      return;
    }

    try {
      debugPrint('Loading items for album: $albumId');
      final itemsJson = _bindings.getAlbumItems(albumId);
      debugPrint('Raw items JSON: $itemsJson');
      
      final List<dynamic> itemsList = json.decode(itemsJson);
      debugPrint('Parsed items list: $itemsList');
      
      final items = itemsList.map((json) => MediaItem.fromJson(json, albumId)).toList();
      
      // Load thumbnail data for each item (for immediate display)
      for (final item in items) {
        debugPrint('Loading thumbnail data for media: ${item.id}');
        final thumbnailData = _bindings.getMediaThumbnail(albumId, item.id);
        if (thumbnailData != null) {
          item.setThumbnailData(thumbnailData);
          debugPrint('Loaded ${thumbnailData.length} bytes thumbnail for media: ${item.id}');
        } else {
          debugPrint('Failed to load thumbnail data for media: ${item.id}');
        }
      }
      
      _albumItems[albumId] = items;
      notifyListeners();
      debugPrint('Loaded ${_albumItems[albumId]?.length ?? 0} items for album $albumId');
    } catch (e) {
      debugPrint('Error loading album items: $e');
      debugPrint('Stack trace: ${StackTrace.current}');
    }
  }

  Future<bool> createAlbum(String name) async {
    if (!_initialized) return false;

    try {
      final resultJson = _bindings.createAlbum(name);
      final result = json.decode(resultJson);
      
      if (result.containsKey('error')) {
        debugPrint('Error creating album: ${result['error']}');
        return false;
      } else {
        await loadAlbums(); // Refresh the list
        debugPrint('Created album: ${result['name']}');
        return true;
      }
    } catch (e) {
      debugPrint('Error creating album: $e');
      return false;
    }
  }

  Future<bool> uploadMedia(String albumId, File file) async {
    if (!_initialized) {
      debugPrint('Cannot upload media: FFI not initialized');
      return false;
    }

    try {
      // Verify file exists and is readable
      if (!await file.exists()) {
        debugPrint('File does not exist: ${file.path}');
        return false;
      }
      
      final fileSize = await file.length();
      debugPrint('Uploading file: ${file.path} (${fileSize} bytes) to album: $albumId');
      
      final result = _bindings.uploadMedia(albumId, file.path);
      debugPrint('FFI upload result: $result');
      
      if (result == 0) {
        debugPrint('Upload successful, refreshing album items...');
        await loadAlbumItems(albumId); // Refresh the album items
        debugPrint('Album items refreshed');
        return true;
      } else {
        debugPrint('FFI upload failed with code: $result');
        return false;
      }
    } catch (e) {
      debugPrint('Exception during media upload: $e');
      return false;
    }
  }

  Future<void> rotateMedia(String albumId, String mediaId, double degrees) async {
    // TODO: Implement rotation via FFI
    debugPrint('Rotation not yet implemented via FFI');
  }

  Future<void> addComment(String albumId, String mediaId, String text) async {
    // TODO: Implement comments via FFI
    debugPrint('Comments not yet implemented via FFI');
  }

  Future<List<int>?> loadMediaAtResolution(String albumId, String mediaId, String resolution) async {
    if (!_initialized) {
      debugPrint('Cannot load media: FFI not initialized');
      return null;
    }

    try {
      debugPrint('Loading $resolution resolution for media: $mediaId');
      
      List<int>? data;
      switch (resolution) {
        case 'thumbnail':
          data = _bindings.getMediaThumbnail(albumId, mediaId);
          break;
        case 'medium':
          data = _bindings.getMediaMedium(albumId, mediaId);
          break;
        case 'high':
          data = _bindings.getMediaHigh(albumId, mediaId);
          break;
        default:
          data = _bindings.getMediaData(albumId, mediaId);
      }
      
      if (data != null) {
        debugPrint('Loaded ${data.length} bytes at $resolution resolution for media: $mediaId');
        
        // Update the media item in the album items cache
        final items = _albumItems[albumId];
        if (items != null) {
          final item = items.firstWhere((item) => item.id == mediaId, orElse: () => items.first);
          switch (resolution) {
            case 'thumbnail':
              item.setThumbnailData(data);
              break;
            case 'medium':
              item.setMediumData(data);
              break;
            case 'high':
              item.setHighData(data);
              break;
          }
          // Don't notify listeners here to avoid setState during build
          // The UI will update when the individual widget receives the data
        }
        
        return data;
      } else {
        debugPrint('Failed to load $resolution resolution for media: $mediaId');
        return null;
      }
    } catch (e) {
      debugPrint('Error loading $resolution resolution for media $mediaId: $e');
      return null;
    }
  }

  @override
  void dispose() {
    if (_initialized) {
      _bindings.cleanup();
    }
    super.dispose();
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/widgets/progressive_image.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import 'dart:typed_data';
import '../models/media_item.dart';
import '../services/album_service.dart';

class ProgressiveImage extends StatefulWidget {
  final MediaItem mediaItem;
  final String albumId;
  final double? width;
  final double? height;
  final BoxFit fit;

  const ProgressiveImage({
    super.key,
    required this.mediaItem,
    required this.albumId,
    this.width,
    this.height,
    this.fit = BoxFit.cover,
  });

  @override
  State<ProgressiveImage> createState() => _ProgressiveImageState();
}

class _ProgressiveImageState extends State<ProgressiveImage> {
  Uint8List? _currentImageData;
  String _currentResolution = 'loading';
  bool _isLoading = true;

  @override
  void initState() {
    super.initState();
    _loadImage();
  }

  void _loadImage() {
    // Real progressive loading: placeholder → thumbnail → medium → high
    _loadProgressively();
  }

  void _loadProgressively() async {
    // Start with placeholder
    setState(() {
      _currentResolution = 'placeholder';
      _isLoading = true;
    });

    // Check if we already have thumbnail data
    if (widget.mediaItem.hasThumbnailData) {
      setState(() {
        _currentImageData = Uint8List.fromList(widget.mediaItem.thumbnailData!);
        _currentResolution = 'thumbnail';
      });
    }

    // Load medium resolution
    await _loadResolution('medium');
    
    // Load high resolution for images (not for videos to save bandwidth)
    if (widget.mediaItem.mediaType != MediaType.video) {
      await _loadResolution('high');
    }
    
    setState(() {
      _isLoading = false;
    });
  }

  Future<void> _loadResolution(String resolution) async {
    if (!mounted) return;
    
    final albumService = context.read<AlbumService>();
    final data = await albumService.loadMediaAtResolution(
      widget.albumId, 
      widget.mediaItem.id, 
      resolution
    );
    
    if (mounted && data != null) {
      // Use post-frame callback to avoid setState during build
      WidgetsBinding.instance.addPostFrameCallback((_) {
        if (mounted) {
          setState(() {
            _currentImageData = Uint8List.fromList(data);
            _currentResolution = resolution;
          });
        }
      });
    }
  }

  @override
  Widget build(BuildContext context) {
    return Stack(
      children: [
        // Main image with smooth transitions
        AnimatedSwitcher(
          duration: const Duration(milliseconds: 300),
          child: Container(
            key: ValueKey(_currentResolution),
            width: widget.width,
            height: widget.height,
            decoration: BoxDecoration(
              color: Colors.grey[300],
              borderRadius: BorderRadius.circular(8),
            ),
            child: _buildImageContent(),
          ),
        ),
        
        // Loading indicator for higher resolutions
        if (_isLoading && _currentResolution != 'placeholder')
          Positioned(
            bottom: 4,
            right: 4,
            child: Container(
              padding: const EdgeInsets.all(4),
              decoration: BoxDecoration(
                color: Colors.black54,
                borderRadius: BorderRadius.circular(4),
              ),
              child: const SizedBox(
                width: 12,
                height: 12,
                child: CircularProgressIndicator(
                  strokeWidth: 2,
                  valueColor: AlwaysStoppedAnimation<Color>(Colors.white),
                ),
              ),
            ),
          ),
        
        // Resolution indicator
        Positioned(
          bottom: 4,
          left: 4,
          child: Container(
            padding: const EdgeInsets.symmetric(horizontal: 6, vertical: 2),
            decoration: BoxDecoration(
              color: Colors.black54,
              borderRadius: BorderRadius.circular(4),
            ),
            child: Text(
              _currentResolution.toUpperCase(),
              style: const TextStyle(
                color: Colors.white,
                fontSize: 10,
                fontWeight: FontWeight.bold,
              ),
            ),
          ),
        ),
      ],
    );
  }

  Widget _buildImageContent() {
    switch (_currentResolution) {
      case 'placeholder':
        return _buildPlaceholder();
      case 'thumbnail':
        return _buildThumbnailPlaceholder();
      case 'thumbnail':
      case 'medium':
      case 'high':
        if (_currentImageData != null) {
          return ClipRRect(
            borderRadius: BorderRadius.circular(8),
            child: Stack(
              fit: StackFit.expand,
              children: [
                Image.memory(
                  _currentImageData!,
                  width: widget.width,
                  height: widget.height,
                  fit: widget.fit,
                  errorBuilder: (context, error, stackTrace) {
                    print('Error loading image data: $error');
                    return _buildErrorWidget();
                  },
                ),
                // Add video play icon overlay for videos
                if (widget.mediaItem.mediaType == MediaType.video)
                  const Center(
                    child: Icon(
                      Icons.play_circle_outline,
                      color: Colors.white,
                      size: 32,
                      shadows: [
                        Shadow(
                          offset: Offset(1, 1),
                          blurRadius: 3,
                          color: Colors.black54,
                        ),
                      ],
                    ),
                  ),
              ],
            ),
          );
        } else {
          return _buildErrorWidget();
        }
      case 'error':
        return _buildErrorWidget();
      default:
        return _buildPlaceholder();
    }
  }

  Widget _buildErrorWidget() {
    return Container(
      width: widget.width,
      height: widget.height,
      decoration: BoxDecoration(
        color: Colors.grey[300],
        borderRadius: BorderRadius.circular(8),
      ),
      child: const Icon(
        Icons.error,
        color: Colors.red,
        size: 32,
      ),
    );
  }

  Widget _buildPlaceholder() {
    return Container(
      width: widget.width,
      height: widget.height,
      decoration: BoxDecoration(
        color: Colors.grey[300],
        borderRadius: BorderRadius.circular(8),
      ),
      child: const Icon(
        Icons.image,
        color: Colors.grey,
        size: 32,
      ),
    );
  }

  Widget _buildThumbnailPlaceholder() {
    return Container(
      width: widget.width,
      height: widget.height,
      decoration: BoxDecoration(
        color: Colors.grey[200],
        borderRadius: BorderRadius.circular(8),
      ),
      child: Stack(
        children: [
          // Simulated low-res blur effect
          Container(
            decoration: BoxDecoration(
              gradient: LinearGradient(
                begin: Alignment.topLeft,
                end: Alignment.bottomRight,
                colors: [
                  Colors.grey[400]!,
                  Colors.grey[300]!,
                  Colors.grey[400]!,
                ],
              ),
              borderRadius: BorderRadius.circular(8),
            ),
          ),
          const Center(
            child: Icon(
              Icons.image,
              color: Colors.white70,
              size: 24,
            ),
          ),
        ],
      ),
    );
  }
}


// =====================================================================
// FILE: apps/soradyne_demo/flutter_app/lib/widgets/storage_status_widget.dart
// =====================================================================

import 'dart:async';
import 'dart:convert';
import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import '../services/album_service.dart';

class StorageStatusWidget extends StatefulWidget {
  @override
  _StorageStatusWidgetState createState() => _StorageStatusWidgetState();
}

class _StorageStatusWidgetState extends State<StorageStatusWidget> {
  Timer? _statusTimer;
  Map<String, dynamic> _status = {};

  @override
  void initState() {
    super.initState();
    _startStatusPolling();
  }

  void _startStatusPolling() {
    _updateStatus();
    _statusTimer = Timer.periodic(Duration(seconds: 2), (_) => _updateStatus());
  }

  void _updateStatus() {
    try {
      final albumService = context.read<AlbumService>();
      if (albumService.bindings != null) {
        final statusJson = albumService.bindings.getStorageStatus();
        setState(() {
          _status = json.decode(statusJson);
        });
      } else {
        print('AlbumService bindings not initialized yet');
      }
    } catch (e) {
      print('Error getting storage status: $e');
      // Set default status on error
      setState(() {
        _status = {
          'available_devices': 0,
          'required_threshold': 3,
          'can_read_data': false,
          'missing_devices': 3,
          'device_paths': [],
        };
      });
    }
  }

  @override
  Widget build(BuildContext context) {
    final canRead = _status['can_read_data'] ?? false;
    final available = _status['available_devices'] ?? 0;
    final required = _status['required_threshold'] ?? 3;
    final missing = _status['missing_devices'] ?? 3;

    return Card(
      color: canRead ? Colors.green[100] : Colors.orange[100],
      child: Padding(
        padding: EdgeInsets.all(16),
        child: Column(
          children: [
            Row(
              children: [
                Icon(
                  canRead ? Icons.sd_card : Icons.warning,
                  color: canRead ? Colors.green : Colors.orange,
                ),
                SizedBox(width: 8),
                Text(
                  canRead 
                    ? 'Storage Ready' 
                    : 'Insert $missing more SD card${missing > 1 ? 's' : ''}',
                  style: TextStyle(fontWeight: FontWeight.bold),
                ),
              ],
            ),
            SizedBox(height: 8),
            Text('$available of $required SD cards detected'),
            if (!canRead)
              ElevatedButton(
                onPressed: () {
                  try {
                    final albumService = context.read<AlbumService>();
                    if (albumService.bindings != null) {
                      albumService.bindings.refreshStorage();
                      _updateStatus();
                    }
                  } catch (e) {
                    print('Error refreshing storage: $e');
                  }
                },
                child: Text('Refresh'),
              ),
          ],
        ),
      ),
    );
  }

  @override
  void dispose() {
    _statusTimer?.cancel();
    super.dispose();
  }
}


// =====================================================================
// FILE: apps/giantt/test/widget_test.dart
// =====================================================================

// This is a basic Flutter widget test.
//
// To perform an interaction with a widget in your test, use the WidgetTester
// utility in the flutter_test package. For example, you can send tap and scroll
// gestures. You can also use WidgetTester to find child widgets in the widget
// tree, read text, and verify that the values of widget properties are correct.

import 'package:flutter/material.dart';
import 'package:flutter_test/flutter_test.dart';

import 'package:giantt/main.dart';

void main() {
  testWidgets('Counter increments smoke test', (WidgetTester tester) async {
    // Build our app and trigger a frame.
    await tester.pumpWidget(const MyApp());

    // Verify that our counter starts at 0.
    expect(find.text('0'), findsOneWidget);
    expect(find.text('1'), findsNothing);

    // Tap the '+' icon and trigger a frame.
    await tester.tap(find.byIcon(Icons.add));
    await tester.pump();

    // Verify that our counter has incremented.
    expect(find.text('0'), findsNothing);
    expect(find.text('1'), findsOneWidget);
  });
}


// =====================================================================
// FILE: apps/giantt/ios/Runner/Runner-Bridging-Header.h
// =====================================================================

#import "GeneratedPluginRegistrant.h"


// =====================================================================
// FILE: apps/giantt/ios/Runner/GeneratedPluginRegistrant.h
// =====================================================================

//
//  Generated file. Do not edit.
//

// clang-format off

#ifndef GeneratedPluginRegistrant_h
#define GeneratedPluginRegistrant_h

#import <Flutter/Flutter.h>

NS_ASSUME_NONNULL_BEGIN

@interface GeneratedPluginRegistrant : NSObject
+ (void)registerWithRegistry:(NSObject<FlutterPluginRegistry>*)registry;
@end

NS_ASSUME_NONNULL_END
#endif /* GeneratedPluginRegistrant_h */


# =====================================================================
# FILE: apps/giantt/ios/Flutter/ephemeral/flutter_lldb_helper.py
# =====================================================================

#
# Generated file, do not edit.
#

import lldb

def handle_new_rx_page(frame: lldb.SBFrame, bp_loc, extra_args, intern_dict):
    """Intercept NOTIFY_DEBUGGER_ABOUT_RX_PAGES and touch the pages."""
    base = frame.register["x0"].GetValueAsAddress()
    page_len = frame.register["x1"].GetValueAsUnsigned()

    # Note: NOTIFY_DEBUGGER_ABOUT_RX_PAGES will check contents of the
    # first page to see if handled it correctly. This makes diagnosing
    # misconfiguration (e.g. missing breakpoint) easier.
    data = bytearray(page_len)
    data[0:8] = b'IHELPED!'

    error = lldb.SBError()
    frame.GetThread().GetProcess().WriteMemory(base, data, error)
    if not error.Success():
        print(f'Failed to write into {base}[+{page_len}]', error)
        return

def __lldb_init_module(debugger: lldb.SBDebugger, _):
    target = debugger.GetDummyTarget()
    # Caveat: must use BreakpointCreateByRegEx here and not
    # BreakpointCreateByName. For some reasons callback function does not
    # get carried over from dummy target for the later.
    bp = target.BreakpointCreateByRegex("^NOTIFY_DEBUGGER_ABOUT_RX_PAGES$")
    bp.SetScriptCallbackFunction('{}.handle_new_rx_page'.format(__name__))
    bp.SetAutoContinue(True)
    print("-- LLDB integration loaded --")


// =====================================================================
// FILE: apps/giantt/linux/runner/my_application.h
// =====================================================================

#ifndef FLUTTER_MY_APPLICATION_H_
#define FLUTTER_MY_APPLICATION_H_

#include <gtk/gtk.h>

G_DECLARE_FINAL_TYPE(MyApplication, my_application, MY, APPLICATION,
                     GtkApplication)

/**
 * my_application_new:
 *
 * Creates a new Flutter-based application.
 *
 * Returns: a new #MyApplication.
 */
MyApplication* my_application_new();

#endif  // FLUTTER_MY_APPLICATION_H_


// =====================================================================
// FILE: apps/giantt/linux/flutter/generated_plugin_registrant.h
// =====================================================================

//
//  Generated file. Do not edit.
//

// clang-format off

#ifndef GENERATED_PLUGIN_REGISTRANT_
#define GENERATED_PLUGIN_REGISTRANT_

#include <flutter_linux/flutter_linux.h>

// Registers Flutter plugins.
void fl_register_plugins(FlPluginRegistry* registry);

#endif  // GENERATED_PLUGIN_REGISTRANT_


// =====================================================================
// FILE: apps/giantt/android/app/src/main/java/io/flutter/plugins/GeneratedPluginRegistrant.java
// =====================================================================

package io.flutter.plugins;

import androidx.annotation.Keep;
import androidx.annotation.NonNull;
import io.flutter.Log;

import io.flutter.embedding.engine.FlutterEngine;

/**
 * Generated file. Do not edit.
 * This file is generated by the Flutter tool based on the
 * plugins that support the Android platform.
 */
@Keep
public final class GeneratedPluginRegistrant {
  private static final String TAG = "GeneratedPluginRegistrant";
  public static void registerWith(@NonNull FlutterEngine flutterEngine) {
    try {
      flutterEngine.getPlugins().add(new io.flutter.plugins.pathprovider.PathProviderPlugin());
    } catch (Exception e) {
      Log.e(TAG, "Error registering plugin path_provider_android, io.flutter.plugins.pathprovider.PathProviderPlugin", e);
    }
    try {
      flutterEngine.getPlugins().add(new com.baseflow.permissionhandler.PermissionHandlerPlugin());
    } catch (Exception e) {
      Log.e(TAG, "Error registering plugin permission_handler_android, com.baseflow.permissionhandler.PermissionHandlerPlugin", e);
    }
  }
}


// =====================================================================
// FILE: apps/giantt/lib/main.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:giantt_core/giantt_core.dart';
import 'screens/home_screen.dart';
import 'screens/chart_view_screen.dart';
import 'screens/item_detail_screen.dart';
import 'screens/add_item_screen.dart';
import 'services/giantt_service.dart';
import 'theme/app_theme.dart';

void main() {
  runApp(const GianttApp());
}

class GianttApp extends StatelessWidget {
  const GianttApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Giantt',
      theme: AppTheme.lightTheme,
      darkTheme: AppTheme.darkTheme,
      themeMode: ThemeMode.system,
      home: const GianttHomePage(),
      routes: {
        '/chart': (context) => const ChartViewScreen(),
        '/add-item': (context) => const AddItemScreen(),
      },
      onGenerateRoute: (settings) {
        if (settings.name?.startsWith('/item/') == true) {
          final itemId = settings.name!.substring(6);
          return MaterialPageRoute(
            builder: (context) => ItemDetailScreen(itemId: itemId),
          );
        }
        return null;
      },
    );
  }
}

class GianttHomePage extends StatefulWidget {
  const GianttHomePage({super.key});

  @override
  State<GianttHomePage> createState() => _GianttHomePageState();
}

class _GianttHomePageState extends State<GianttHomePage> {
  final GianttService _gianttService = GianttService();
  int _selectedIndex = 0;
  
  static const List<Widget> _pages = <Widget>[
    HomeScreen(),
    ChartViewScreen(),
    AddItemScreen(),
  ];

  void _onItemTapped(int index) {
    setState(() {
      _selectedIndex = index;
    });
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: IndexedStack(
        index: _selectedIndex,
        children: _pages,
      ),
      bottomNavigationBar: BottomNavigationBar(
        items: const <BottomNavigationBarItem>[
          BottomNavigationBarItem(
            icon: Icon(Icons.home),
            label: 'Home',
          ),
          BottomNavigationBarItem(
            icon: Icon(Icons.timeline),
            label: 'Charts',
          ),
          BottomNavigationBarItem(
            icon: Icon(Icons.add),
            label: 'Add Item',
          ),
        ],
        currentIndex: _selectedIndex,
        onTap: _onItemTapped,
      ),
    );
  }
}


// =====================================================================
// FILE: apps/giantt/lib/screens/add_item_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:giantt_core/giantt_core.dart';
import '../services/giantt_service.dart';

class AddItemScreen extends StatefulWidget {
  const AddItemScreen({super.key});

  @override
  State<AddItemScreen> createState() => _AddItemScreenState();
}

class _AddItemScreenState extends State<AddItemScreen> {
  final GianttService _gianttService = GianttService();
  final _formKey = GlobalKey<FormState>();
  
  final _idController = TextEditingController();
  final _titleController = TextEditingController();
  final _durationController = TextEditingController();
  final _chartsController = TextEditingController();
  final _tagsController = TextEditingController();
  
  GianttStatus _selectedStatus = GianttStatus.notStarted;
  GianttPriority _selectedPriority = GianttPriority.neutral;
  bool _isSubmitting = false;

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Add Item'),
        actions: [
          TextButton(
            onPressed: _isSubmitting ? null : _submitForm,
            child: _isSubmitting
                ? const SizedBox(
                    width: 20,
                    height: 20,
                    child: CircularProgressIndicator(strokeWidth: 2),
                  )
                : const Text('Save', style: TextStyle(color: Colors.white)),
          ),
        ],
      ),
      body: Form(
        key: _formKey,
        child: ListView(
          padding: const EdgeInsets.all(16.0),
          children: [
            // ID field
            TextFormField(
              controller: _idController,
              decoration: const InputDecoration(
                labelText: 'ID *',
                hintText: 'e.g., learn_python',
                border: OutlineInputBorder(),
              ),
              validator: (value) {
                if (value == null || value.isEmpty) {
                  return 'ID is required';
                }
                if (!RegExp(r'^[a-z0-9_]+$').hasMatch(value)) {
                  return 'ID must be lowercase letters, numbers, and underscores only';
                }
                return null;
              },
            ),
            
            const SizedBox(height: 16),
            
            // Title field
            TextFormField(
              controller: _titleController,
              decoration: const InputDecoration(
                labelText: 'Title *',
                hintText: 'e.g., Learn Python basics',
                border: OutlineInputBorder(),
              ),
              validator: (value) {
                if (value == null || value.isEmpty) {
                  return 'Title is required';
                }
                return null;
              },
            ),
            
            const SizedBox(height: 16),
            
            // Status dropdown
            DropdownButtonFormField<GianttStatus>(
              value: _selectedStatus,
              decoration: const InputDecoration(
                labelText: 'Status',
                border: OutlineInputBorder(),
              ),
              items: GianttStatus.values.map((status) {
                return DropdownMenuItem(
                  value: status,
                  child: Row(
                    children: [
                      Text(status.symbol),
                      const SizedBox(width: 8),
                      Text(status.name),
                    ],
                  ),
                );
              }).toList(),
              onChanged: (value) {
                if (value != null) {
                  setState(() {
                    _selectedStatus = value;
                  });
                }
              },
            ),
            
            const SizedBox(height: 16),
            
            // Priority dropdown
            DropdownButtonFormField<GianttPriority>(
              value: _selectedPriority,
              decoration: const InputDecoration(
                labelText: 'Priority',
                border: OutlineInputBorder(),
              ),
              items: GianttPriority.values.map((priority) {
                return DropdownMenuItem(
                  value: priority,
                  child: Row(
                    children: [
                      Text(priority.symbol.isEmpty ? '(none)' : priority.symbol),
                      const SizedBox(width: 8),
                      Text(priority.name),
                    ],
                  ),
                );
              }).toList(),
              onChanged: (value) {
                if (value != null) {
                  setState(() {
                    _selectedPriority = value;
                  });
                }
              },
            ),
            
            const SizedBox(height: 16),
            
            // Duration field
            TextFormField(
              controller: _durationController,
              decoration: const InputDecoration(
                labelText: 'Duration',
                hintText: 'e.g., 3mo, 2w, 5d',
                border: OutlineInputBorder(),
              ),
              validator: (value) {
                if (value != null && value.isNotEmpty) {
                  try {
                    GianttDuration.parse(value);
                  } catch (e) {
                    return 'Invalid duration format';
                  }
                }
                return null;
              },
            ),
            
            const SizedBox(height: 16),
            
            // Charts field
            TextFormField(
              controller: _chartsController,
              decoration: const InputDecoration(
                labelText: 'Charts',
                hintText: 'e.g., Programming, Education (comma-separated)',
                border: OutlineInputBorder(),
              ),
            ),
            
            const SizedBox(height: 16),
            
            // Tags field
            TextFormField(
              controller: _tagsController,
              decoration: const InputDecoration(
                labelText: 'Tags',
                hintText: 'e.g., beginner, coding (comma-separated)',
                border: OutlineInputBorder(),
              ),
            ),
            
            const SizedBox(height: 32),
            
            // Submit button
            ElevatedButton(
              onPressed: _isSubmitting ? null : _submitForm,
              child: _isSubmitting
                  ? const Row(
                      mainAxisAlignment: MainAxisAlignment.center,
                      children: [
                        SizedBox(
                          width: 20,
                          height: 20,
                          child: CircularProgressIndicator(strokeWidth: 2),
                        ),
                        SizedBox(width: 8),
                        Text('Adding...'),
                      ],
                    )
                  : const Text('Add Item'),
            ),
          ],
        ),
      ),
    );
  }

  Future<void> _submitForm() async {
    if (!_formKey.currentState!.validate()) {
      return;
    }

    setState(() {
      _isSubmitting = true;
    });

    try {
      // Parse duration
      GianttDuration? duration;
      if (_durationController.text.isNotEmpty) {
        duration = GianttDuration.parse(_durationController.text);
      }

      // Parse charts
      final charts = _chartsController.text
          .split(',')
          .map((c) => c.trim())
          .where((c) => c.isNotEmpty)
          .toList();

      // Parse tags
      final tags = _tagsController.text
          .split(',')
          .map((t) => t.trim())
          .where((t) => t.isNotEmpty)
          .toList();

      final result = await _gianttService.addItem(
        id: _idController.text,
        title: _titleController.text,
        status: _selectedStatus,
        priority: _selectedPriority,
        duration: duration,
        charts: charts,
        tags: tags,
      );

      if (result.success) {
        if (mounted) {
          ScaffoldMessenger.of(context).showSnackBar(
            SnackBar(content: Text(result.message ?? 'Item added successfully')),
          );
          Navigator.pop(context, true);
        }
      } else {
        if (mounted) {
          ScaffoldMessenger.of(context).showSnackBar(
            SnackBar(
              content: Text(result.error ?? 'Failed to add item'),
              backgroundColor: Colors.red,
            ),
          );
        }
      }
    } catch (e) {
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(
            content: Text('Error: $e'),
            backgroundColor: Colors.red,
          ),
        );
      }
    } finally {
      setState(() {
        _isSubmitting = false;
      });
    }
  }

  @override
  void dispose() {
    _idController.dispose();
    _titleController.dispose();
    _durationController.dispose();
    _chartsController.dispose();
    _tagsController.dispose();
    super.dispose();
  }
}


// =====================================================================
// FILE: apps/giantt/lib/screens/item_detail_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:giantt_core/giantt_core.dart';
import '../services/giantt_service.dart';

class ItemDetailScreen extends StatefulWidget {
  final String itemId;

  const ItemDetailScreen({super.key, required this.itemId});

  @override
  State<ItemDetailScreen> createState() => _ItemDetailScreenState();
}

class _ItemDetailScreenState extends State<ItemDetailScreen> {
  final GianttService _gianttService = GianttService();
  
  GianttItem? _item;
  bool _isLoading = true;

  @override
  void initState() {
    super.initState();
    _loadItem();
  }

  Future<void> _loadItem() async {
    setState(() {
      _isLoading = true;
    });

    try {
      final graph = await _gianttService.getGraph();
      final item = graph.items[widget.itemId];
      
      setState(() {
        _item = item;
        _isLoading = false;
      });
    } catch (e) {
      setState(() {
        _isLoading = false;
      });
      
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(content: Text('Error loading item: $e')),
        );
      }
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text(_item?.title ?? 'Item Details'),
        actions: [
          if (_item != null) ...[
            IconButton(
              icon: Icon(_item!.occlude ? Icons.visibility : Icons.visibility_off),
              onPressed: () async {
                final result = _item!.occlude
                    ? await _gianttService.includeItem(_item!.id)
                    : await _gianttService.occludeItem(_item!.id);
                
                if (result.success) {
                  await _loadItem();
                  if (mounted) {
                    ScaffoldMessenger.of(context).showSnackBar(
                      SnackBar(content: Text(result.message ?? 'Item updated')),
                    );
                  }
                } else {
                  if (mounted) {
                    ScaffoldMessenger.of(context).showSnackBar(
                      SnackBar(
                        content: Text(result.error ?? 'Failed to update item'),
                        backgroundColor: Colors.red,
                      ),
                    );
                  }
                }
              },
            ),
          ],
        ],
      ),
      body: _isLoading
          ? const Center(child: CircularProgressIndicator())
          : _item == null
              ? Center(
                  child: Column(
                    mainAxisAlignment: MainAxisAlignment.center,
                    children: [
                      Icon(
                        Icons.error_outline,
                        size: 64,
                        color: Colors.grey[400],
                      ),
                      const SizedBox(height: 16),
                      Text(
                        'Item not found',
                        style: Theme.of(context).textTheme.titleMedium?.copyWith(
                          color: Colors.grey[600],
                        ),
                      ),
                      const SizedBox(height: 8),
                      Text(
                        'The item "${widget.itemId}" could not be found',
                        style: Theme.of(context).textTheme.bodyMedium?.copyWith(
                          color: Colors.grey[500],
                        ),
                      ),
                    ],
                  ),
                )
              : SingleChildScrollView(
                  padding: const EdgeInsets.all(16.0),
                  child: Column(
                    crossAxisAlignment: CrossAxisAlignment.start,
                    children: [
                      // Basic info card
                      Card(
                        child: Padding(
                          padding: const EdgeInsets.all(16.0),
                          child: Column(
                            crossAxisAlignment: CrossAxisAlignment.start,
                            children: [
                              Row(
                                children: [
                                  Text(
                                    _item!.status.symbol,
                                    style: const TextStyle(fontSize: 24),
                                  ),
                                  const SizedBox(width: 8),
                                  Expanded(
                                    child: Text(
                                      _item!.title,
                                      style: Theme.of(context).textTheme.headlineSmall,
                                    ),
                                  ),
                                  if (_item!.priority.symbol.isNotEmpty)
                                    Text(
                                      _item!.priority.symbol,
                                      style: const TextStyle(
                                        fontSize: 18,
                                        fontWeight: FontWeight.bold,
                                      ),
                                    ),
                                ],
                              ),
                              const SizedBox(height: 8),
                              Text(
                                'ID: ${_item!.id}',
                                style: Theme.of(context).textTheme.bodyMedium?.copyWith(
                                  color: Colors.grey[600],
                                ),
                              ),
                              const SizedBox(height: 16),
                              _buildInfoRow('Status', _item!.status.name),
                              _buildInfoRow('Priority', _item!.priority.name),
                              _buildInfoRow('Duration', _item!.duration.toString()),
                              if (_item!.occlude)
                                _buildInfoRow('Visibility', 'Occluded', isWarning: true),
                            ],
                          ),
                        ),
                      ),
                      
                      const SizedBox(height: 16),
                      
                      // Charts
                      if (_item!.charts.isNotEmpty) ...[
                        Card(
                          child: Padding(
                            padding: const EdgeInsets.all(16.0),
                            child: Column(
                              crossAxisAlignment: CrossAxisAlignment.start,
                              children: [
                                Text(
                                  'Charts',
                                  style: Theme.of(context).textTheme.titleMedium,
                                ),
                                const SizedBox(height: 8),
                                Wrap(
                                  spacing: 8,
                                  children: _item!.charts.map((chart) {
                                    return Chip(
                                      label: Text(chart),
                                      backgroundColor: Theme.of(context).primaryColor.withOpacity(0.1),
                                    );
                                  }).toList(),
                                ),
                              ],
                            ),
                          ),
                        ),
                        const SizedBox(height: 16),
                      ],
                      
                      // Tags
                      if (_item!.tags.isNotEmpty) ...[
                        Card(
                          child: Padding(
                            padding: const EdgeInsets.all(16.0),
                            child: Column(
                              crossAxisAlignment: CrossAxisAlignment.start,
                              children: [
                                Text(
                                  'Tags',
                                  style: Theme.of(context).textTheme.titleMedium,
                                ),
                                const SizedBox(height: 8),
                                Wrap(
                                  spacing: 8,
                                  children: _item!.tags.map((tag) {
                                    return Chip(
                                      label: Text(tag),
                                      backgroundColor: Colors.grey[200],
                                    );
                                  }).toList(),
                                ),
                              ],
                            ),
                          ),
                        ),
                        const SizedBox(height: 16),
                      ],
                      
                      // Relations
                      if (_item!.relations.isNotEmpty) ...[
                        Card(
                          child: Padding(
                            padding: const EdgeInsets.all(16.0),
                            child: Column(
                              crossAxisAlignment: CrossAxisAlignment.start,
                              children: [
                                Text(
                                  'Relations',
                                  style: Theme.of(context).textTheme.titleMedium,
                                ),
                                const SizedBox(height: 8),
                                ..._item!.relations.entries.map((entry) {
                                  return Padding(
                                    padding: const EdgeInsets.symmetric(vertical: 4.0),
                                    child: Row(
                                      crossAxisAlignment: CrossAxisAlignment.start,
                                      children: [
                                        SizedBox(
                                          width: 100,
                                          child: Text(
                                            entry.key,
                                            style: const TextStyle(fontWeight: FontWeight.bold),
                                          ),
                                        ),
                                        Expanded(
                                          child: Text(entry.value.join(', ')),
                                        ),
                                      ],
                                    ),
                                  );
                                }).toList(),
                              ],
                            ),
                          ),
                        ),
                        const SizedBox(height: 16),
                      ],
                      
                      // Time constraints
                      if (_item!.timeConstraints.isNotEmpty) ...[
                        Card(
                          child: Padding(
                            padding: const EdgeInsets.all(16.0),
                            child: Column(
                              crossAxisAlignment: CrossAxisAlignment.start,
                              children: [
                                Text(
                                  'Time Constraints',
                                  style: Theme.of(context).textTheme.titleMedium,
                                ),
                                const SizedBox(height: 8),
                                ..._item!.timeConstraints.map((constraint) {
                                  return Padding(
                                    padding: const EdgeInsets.symmetric(vertical: 4.0),
                                    child: Text(constraint.toString()),
                                  );
                                }).toList(),
                              ],
                            ),
                          ),
                        ),
                        const SizedBox(height: 16),
                      ],
                      
                      // Comments
                      if (_item!.userComment != null || _item!.autoComment != null) ...[
                        Card(
                          child: Padding(
                            padding: const EdgeInsets.all(16.0),
                            child: Column(
                              crossAxisAlignment: CrossAxisAlignment.start,
                              children: [
                                Text(
                                  'Comments',
                                  style: Theme.of(context).textTheme.titleMedium,
                                ),
                                const SizedBox(height: 8),
                                if (_item!.userComment != null) ...[
                                  Text(
                                    'User: ${_item!.userComment}',
                                    style: Theme.of(context).textTheme.bodyMedium,
                                  ),
                                  const SizedBox(height: 4),
                                ],
                                if (_item!.autoComment != null) ...[
                                  Text(
                                    'Auto: ${_item!.autoComment}',
                                    style: Theme.of(context).textTheme.bodyMedium?.copyWith(
                                      color: Colors.grey[600],
                                    ),
                                  ),
                                ],
                              ],
                            ),
                          ),
                        ),
                      ],
                    ],
                  ),
                ),
    );
  }

  Widget _buildInfoRow(String label, String value, {bool isWarning = false}) {
    return Padding(
      padding: const EdgeInsets.symmetric(vertical: 2.0),
      child: Row(
        children: [
          SizedBox(
            width: 80,
            child: Text(
              '$label:',
              style: const TextStyle(fontWeight: FontWeight.bold),
            ),
          ),
          Expanded(
            child: Text(
              value,
              style: TextStyle(
                color: isWarning ? Colors.orange : null,
              ),
            ),
          ),
        ],
      ),
    );
  }
}


// =====================================================================
// FILE: apps/giantt/lib/screens/chart_view_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:giantt_core/giantt_core.dart';
import '../services/giantt_service.dart';
import '../widgets/item_card.dart';

class ChartViewScreen extends StatefulWidget {
  const ChartViewScreen({super.key});

  @override
  State<ChartViewScreen> createState() => _ChartViewScreenState();
}

class _ChartViewScreenState extends State<ChartViewScreen> {
  final GianttService _gianttService = GianttService();
  
  List<String> _charts = [];
  String? _selectedChart;
  List<GianttItem> _chartItems = [];
  bool _isLoading = true;

  @override
  void initState() {
    super.initState();
    _loadCharts();
  }

  Future<void> _loadCharts() async {
    setState(() {
      _isLoading = true;
    });

    try {
      final charts = await _gianttService.getAllCharts();
      setState(() {
        _charts = charts;
        _isLoading = false;
        if (charts.isNotEmpty && _selectedChart == null) {
          _selectedChart = charts.first;
          _loadChartItems();
        }
      });
    } catch (e) {
      setState(() {
        _isLoading = false;
      });
      
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(content: Text('Error loading charts: $e')),
        );
      }
    }
  }

  Future<void> _loadChartItems() async {
    if (_selectedChart == null) return;

    try {
      final items = await _gianttService.getItemsByChart(_selectedChart!);
      setState(() {
        _chartItems = items;
      });
    } catch (e) {
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(content: Text('Error loading chart items: $e')),
        );
      }
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Charts'),
        actions: [
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: () async {
              await _gianttService.refresh();
              await _loadCharts();
            },
          ),
        ],
      ),
      body: _isLoading
          ? const Center(child: CircularProgressIndicator())
          : _charts.isEmpty
              ? Center(
                  child: Column(
                    mainAxisAlignment: MainAxisAlignment.center,
                    children: [
                      Icon(
                        Icons.timeline,
                        size: 64,
                        color: Colors.grey[400],
                      ),
                      const SizedBox(height: 16),
                      Text(
                        'No charts yet',
                        style: Theme.of(context).textTheme.titleMedium?.copyWith(
                          color: Colors.grey[600],
                        ),
                      ),
                      const SizedBox(height: 8),
                      Text(
                        'Add items with charts to see them here',
                        style: Theme.of(context).textTheme.bodyMedium?.copyWith(
                          color: Colors.grey[500],
                        ),
                      ),
                    ],
                  ),
                )
              : Column(
                  children: [
                    // Chart selector
                    Container(
                      height: 60,
                      padding: const EdgeInsets.symmetric(horizontal: 16.0),
                      child: ListView.builder(
                        scrollDirection: Axis.horizontal,
                        itemCount: _charts.length,
                        itemBuilder: (context, index) {
                          final chart = _charts[index];
                          final isSelected = chart == _selectedChart;
                          
                          return Padding(
                            padding: const EdgeInsets.only(right: 8.0),
                            child: FilterChip(
                              label: Text(chart),
                              selected: isSelected,
                              onSelected: (selected) {
                                if (selected) {
                                  setState(() {
                                    _selectedChart = chart;
                                  });
                                  _loadChartItems();
                                }
                              },
                            ),
                          );
                        },
                      ),
                    ),
                    
                    const Divider(),
                    
                    // Chart items
                    Expanded(
                      child: _chartItems.isEmpty
                          ? Center(
                              child: Column(
                                mainAxisAlignment: MainAxisAlignment.center,
                                children: [
                                  Icon(
                                    Icons.inbox,
                                    size: 64,
                                    color: Colors.grey[400],
                                  ),
                                  const SizedBox(height: 16),
                                  Text(
                                    'No items in "$_selectedChart"',
                                    style: Theme.of(context).textTheme.titleMedium?.copyWith(
                                      color: Colors.grey[600],
                                    ),
                                  ),
                                ],
                              ),
                            )
                          : ListView.builder(
                              padding: const EdgeInsets.symmetric(horizontal: 16.0),
                              itemCount: _chartItems.length,
                              itemBuilder: (context, index) {
                                final item = _chartItems[index];
                                return ItemCard(
                                  item: item,
                                  onTap: () {
                                    Navigator.pushNamed(context, '/item/${item.id}');
                                  },
                                );
                              },
                            ),
                    ),
                  ],
                ),
    );
  }
}


// =====================================================================
// FILE: apps/giantt/lib/screens/home_screen.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:giantt_core/giantt_core.dart';
import '../services/giantt_service.dart';
import '../widgets/item_card.dart';
import '../widgets/stats_card.dart';

class HomeScreen extends StatefulWidget {
  const HomeScreen({super.key});

  @override
  State<HomeScreen> createState() => _HomeScreenState();
}

class _HomeScreenState extends State<HomeScreen> {
  final GianttService _gianttService = GianttService();
  final TextEditingController _searchController = TextEditingController();
  
  List<GianttItem> _items = [];
  Map<String, dynamic> _stats = {};
  bool _isLoading = true;
  String _searchTerm = '';

  @override
  void initState() {
    super.initState();
    _loadData();
  }

  Future<void> _loadData() async {
    setState(() {
      _isLoading = true;
    });

    try {
      final items = await _gianttService.searchItems(_searchTerm);
      final stats = await _gianttService.getWorkspaceStats();
      
      setState(() {
        _items = items;
        _stats = stats;
        _isLoading = false;
      });
    } catch (e) {
      setState(() {
        _isLoading = false;
      });
      
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(content: Text('Error loading data: $e')),
        );
      }
    }
  }

  Future<void> _onSearch(String searchTerm) async {
    setState(() {
      _searchTerm = searchTerm;
    });
    await _loadData();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Giantt'),
        actions: [
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: () async {
              await _gianttService.refresh();
              await _loadData();
            },
          ),
        ],
      ),
      body: Column(
        children: [
          // Search bar
          Padding(
            padding: const EdgeInsets.all(16.0),
            child: TextField(
              controller: _searchController,
              decoration: const InputDecoration(
                hintText: 'Search items...',
                prefixIcon: Icon(Icons.search),
                border: OutlineInputBorder(),
              ),
              onChanged: _onSearch,
            ),
          ),
          
          // Stats cards
          if (_stats.isNotEmpty) ...[
            SizedBox(
              height: 120,
              child: ListView(
                scrollDirection: Axis.horizontal,
                padding: const EdgeInsets.symmetric(horizontal: 16.0),
                children: [
                  StatsCard(
                    title: 'Total Items',
                    value: _stats['total_items']?.toString() ?? '0',
                    icon: Icons.list,
                  ),
                  StatsCard(
                    title: 'Active Items',
                    value: _stats['included_items']?.toString() ?? '0',
                    icon: Icons.check_circle_outline,
                  ),
                  StatsCard(
                    title: 'Charts',
                    value: (_stats['charts'] as List?)?.length.toString() ?? '0',
                    icon: Icons.timeline,
                  ),
                  StatsCard(
                    title: 'Tags',
                    value: (_stats['tags'] as List?)?.length.toString() ?? '0',
                    icon: Icons.label,
                  ),
                ],
              ),
            ),
            const SizedBox(height: 16),
          ],
          
          // Items list
          Expanded(
            child: _isLoading
                ? const Center(child: CircularProgressIndicator())
                : _items.isEmpty
                    ? Center(
                        child: Column(
                          mainAxisAlignment: MainAxisAlignment.center,
                          children: [
                            Icon(
                              Icons.inbox,
                              size: 64,
                              color: Colors.grey[400],
                            ),
                            const SizedBox(height: 16),
                            Text(
                              _searchTerm.isEmpty 
                                  ? 'No items yet'
                                  : 'No items found for "$_searchTerm"',
                              style: Theme.of(context).textTheme.titleMedium?.copyWith(
                                color: Colors.grey[600],
                              ),
                            ),
                            const SizedBox(height: 8),
                            Text(
                              _searchTerm.isEmpty
                                  ? 'Add your first item to get started'
                                  : 'Try a different search term',
                              style: Theme.of(context).textTheme.bodyMedium?.copyWith(
                                color: Colors.grey[500],
                              ),
                            ),
                          ],
                        ),
                      )
                    : ListView.builder(
                        padding: const EdgeInsets.symmetric(horizontal: 16.0),
                        itemCount: _items.length,
                        itemBuilder: (context, index) {
                          final item = _items[index];
                          return ItemCard(
                            item: item,
                            onTap: () {
                              Navigator.pushNamed(context, '/item/${item.id}');
                            },
                          );
                        },
                      ),
          ),
        ],
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: () async {
          final result = await Navigator.pushNamed(context, '/add-item');
          if (result == true) {
            await _loadData();
          }
        },
        child: const Icon(Icons.add),
      ),
    );
  }

  @override
  void dispose() {
    _searchController.dispose();
    super.dispose();
  }
}


// =====================================================================
// FILE: apps/giantt/lib/theme/app_theme.dart
// =====================================================================

import 'package:flutter/material.dart';

class AppTheme {
  static const Color primaryColor = Color(0xFF667EEA);
  static const Color secondaryColor = Color(0xFF764BA2);
  static const Color accentColor = Color(0xFF48BB78);
  static const Color errorColor = Color(0xFFF56565);
  
  static ThemeData lightTheme = ThemeData(
    useMaterial3: true,
    colorScheme: ColorScheme.fromSeed(
      seedColor: primaryColor,
      brightness: Brightness.light,
    ),
    appBarTheme: const AppBarTheme(
      backgroundColor: primaryColor,
      foregroundColor: Colors.white,
      elevation: 2,
    ),
    cardTheme: CardTheme(
      elevation: 4,
      shape: RoundedRectangleBorder(
        borderRadius: BorderRadius.circular(12),
      ),
    ),
    elevatedButtonTheme: ElevatedButtonThemeData(
      style: ElevatedButton.styleFrom(
        backgroundColor: primaryColor,
        foregroundColor: Colors.white,
        shape: RoundedRectangleBorder(
          borderRadius: BorderRadius.circular(8),
        ),
      ),
    ),
    floatingActionButtonTheme: const FloatingActionButtonThemeData(
      backgroundColor: accentColor,
      foregroundColor: Colors.white,
    ),
  );

  static ThemeData darkTheme = ThemeData(
    useMaterial3: true,
    colorScheme: ColorScheme.fromSeed(
      seedColor: primaryColor,
      brightness: Brightness.dark,
    ),
    appBarTheme: const AppBarTheme(
      backgroundColor: Color(0xFF1A1A1A),
      foregroundColor: Colors.white,
      elevation: 2,
    ),
    cardTheme: CardTheme(
      elevation: 4,
      shape: RoundedRectangleBorder(
        borderRadius: BorderRadius.circular(12),
      ),
    ),
    elevatedButtonTheme: ElevatedButtonThemeData(
      style: ElevatedButton.styleFrom(
        backgroundColor: primaryColor,
        foregroundColor: Colors.white,
        shape: RoundedRectangleBorder(
          borderRadius: BorderRadius.circular(8),
        ),
      ),
    ),
    floatingActionButtonTheme: const FloatingActionButtonThemeData(
      backgroundColor: accentColor,
      foregroundColor: Colors.white,
    ),
  );
}


// =====================================================================
// FILE: apps/giantt/lib/services/giantt_service.dart
// =====================================================================

import 'dart:io';
import 'package:path_provider/path_provider.dart';
import 'package:giantt_core/giantt_core.dart';

/// Service class for managing Giantt operations in Flutter
class GianttService {
  static final GianttService _instance = GianttService._internal();
  factory GianttService() => _instance;
  GianttService._internal();

  String? _workspacePath;
  GianttGraph? _graph;
  LogCollection? _logs;

  /// Initialize the service with workspace path
  Future<void> initialize() async {
    if (_workspacePath == null) {
      final documentsDir = await getApplicationDocumentsDirectory();
      _workspacePath = '${documentsDir.path}/giantt';
      
      // Ensure workspace exists
      if (!FileRepository.isWorkspaceInitialized(_workspacePath!)) {
        FileRepository.initializeWorkspace(_workspacePath!);
      }
    }
  }

  /// Get the current workspace path
  String get workspacePath {
    if (_workspacePath == null) {
      throw StateError('GianttService not initialized. Call initialize() first.');
    }
    return _workspacePath!;
  }

  /// Get the current graph, loading if necessary
  Future<GianttGraph> getGraph() async {
    await initialize();
    
    if (_graph == null) {
      await _loadGraph();
    }
    return _graph!;
  }

  /// Get the current logs, loading if necessary
  Future<LogCollection> getLogs() async {
    await initialize();
    
    if (_logs == null) {
      await _loadLogs();
    }
    return _logs!;
  }

  /// Load the graph from files
  Future<void> _loadGraph() async {
    final paths = FileRepository.getDefaultFilePaths(_workspacePath);
    _graph = DualFileManager.loadGraph(
      paths['items']!,
      paths['occlude_items']!,
    );
  }

  /// Load the logs from files
  Future<void> _loadLogs() async {
    final paths = FileRepository.getDefaultFilePaths(_workspacePath);
    _logs = DualFileManager.loadLogs(
      paths['logs']!,
      paths['occlude_logs']!,
    );
  }

  /// Save the current graph to files
  Future<void> saveGraph() async {
    if (_graph == null) return;
    
    final paths = FileRepository.getDefaultFilePaths(_workspacePath);
    DualFileManager.saveGraph(
      paths['items']!,
      paths['occlude_items']!,
      _graph!,
    );
  }

  /// Save the current logs to files
  Future<void> saveLogs() async {
    if (_logs == null) return;
    
    final paths = FileRepository.getDefaultFilePaths(_workspacePath);
    DualFileManager.saveLogs(
      paths['logs']!,
      paths['occlude_logs']!,
      _logs!,
    );
  }

  /// Add a new item to the graph
  Future<CommandResult<GianttItem>> addItem({
    required String id,
    required String title,
    GianttStatus status = GianttStatus.notStarted,
    GianttPriority priority = GianttPriority.neutral,
    GianttDuration? duration,
    List<String> charts = const [],
    List<String> tags = const [],
    Map<String, List<String>> relations = const {},
    List<TimeConstraint> timeConstraints = const [],
  }) async {
    final graph = await getGraph();
    
    // Check if item already exists
    if (graph.items.containsKey(id)) {
      return CommandResult.failure('Item with ID "$id" already exists');
    }

    // Create new item
    final newItem = GianttItem(
      id: id,
      title: title,
      status: status,
      priority: priority,
      duration: duration ?? GianttDuration.zero(),
      charts: charts,
      tags: tags,
      relations: relations,
      timeConstraints: timeConstraints,
    );

    // Add to graph
    graph.addItem(newItem);
    _graph = graph;

    // Save to files
    await saveGraph();

    return CommandResult.success(newItem, 'Added item "$id" successfully');
  }

  /// Get items matching a search term
  Future<List<GianttItem>> searchItems(String searchTerm, {bool includeOccluded = false}) async {
    final graph = await getGraph();
    
    final items = includeOccluded 
        ? graph.items.values.toList()
        : graph.includedItems.values.toList();
    
    if (searchTerm.isEmpty) {
      return items;
    }
    
    return items.where((item) =>
        item.id.toLowerCase().contains(searchTerm.toLowerCase()) ||
        item.title.toLowerCase().contains(searchTerm.toLowerCase()) ||
        item.tags.any((tag) => tag.toLowerCase().contains(searchTerm.toLowerCase()))
    ).toList();
  }

  /// Get items by chart
  Future<List<GianttItem>> getItemsByChart(String chartName, {bool includeOccluded = false}) async {
    final graph = await getGraph();
    
    final items = includeOccluded 
        ? graph.items.values.toList()
        : graph.includedItems.values.toList();
    
    return items.where((item) => item.charts.contains(chartName)).toList();
  }

  /// Get all unique chart names
  Future<List<String>> getAllCharts({bool includeOccluded = false}) async {
    final graph = await getGraph();
    
    final items = includeOccluded 
        ? graph.items.values.toList()
        : graph.includedItems.values.toList();
    
    final charts = <String>{};
    for (final item in items) {
      charts.addAll(item.charts);
    }
    
    return charts.toList()..sort();
  }

  /// Get all unique tags
  Future<List<String>> getAllTags({bool includeOccluded = false}) async {
    final graph = await getGraph();
    
    final items = includeOccluded 
        ? graph.items.values.toList()
        : graph.includedItems.values.toList();
    
    final tags = <String>{};
    for (final item in items) {
      tags.addAll(item.tags);
    }
    
    return tags.toList()..sort();
  }

  /// Update an existing item
  Future<CommandResult<GianttItem>> updateItem(String itemId, GianttItem updatedItem) async {
    final graph = await getGraph();
    
    if (!graph.items.containsKey(itemId)) {
      return CommandResult.failure('Item with ID "$itemId" not found');
    }

    // Update the item
    graph.addItem(updatedItem);
    _graph = graph;

    // Save to files
    await saveGraph();

    return CommandResult.success(updatedItem, 'Updated item "$itemId" successfully');
  }

  /// Occlude an item
  Future<CommandResult<String>> occludeItem(String itemId) async {
    final graph = await getGraph();
    
    final item = graph.items[itemId];
    if (item == null) {
      return CommandResult.failure('Item with ID "$itemId" not found');
    }

    if (item.occlude) {
      return CommandResult.failure('Item "$itemId" is already occluded');
    }

    // Occlude the item
    graph.occludeItem(itemId);
    _graph = graph;

    // Save to files
    await saveGraph();

    return CommandResult.success(itemId, 'Occluded item "$itemId" successfully');
  }

  /// Include (un-occlude) an item
  Future<CommandResult<String>> includeItem(String itemId) async {
    final graph = await getGraph();
    
    final item = graph.items[itemId];
    if (item == null) {
      return CommandResult.failure('Item with ID "$itemId" not found');
    }

    if (!item.occlude) {
      return CommandResult.failure('Item "$itemId" is not occluded');
    }

    // Include the item
    graph.includeItem(itemId);
    _graph = graph;

    // Save to files
    await saveGraph();

    return CommandResult.success(itemId, 'Included item "$itemId" successfully');
  }

  /// Refresh data from files (reload)
  Future<void> refresh() async {
    _graph = null;
    _logs = null;
    await _loadGraph();
    await _loadLogs();
  }

  /// Get workspace statistics
  Future<Map<String, dynamic>> getWorkspaceStats() async {
    final graph = await getGraph();
    final logs = await getLogs();
    
    final includedItems = graph.includedItems.values.toList();
    final occludedItems = graph.occludedItems.values.toList();
    
    return {
      'total_items': graph.items.length,
      'included_items': includedItems.length,
      'occluded_items': occludedItems.length,
      'total_logs': logs.length,
      'included_logs': logs.includedEntries.length,
      'occluded_logs': logs.occludedEntries.length,
      'charts': await getAllCharts(),
      'tags': await getAllTags(),
      'status_breakdown': _getStatusBreakdown(includedItems),
      'priority_breakdown': _getPriorityBreakdown(includedItems),
    };
  }

  Map<String, int> _getStatusBreakdown(List<GianttItem> items) {
    final breakdown = <String, int>{};
    for (final item in items) {
      final status = item.status.name;
      breakdown[status] = (breakdown[status] ?? 0) + 1;
    }
    return breakdown;
  }

  Map<String, int> _getPriorityBreakdown(List<GianttItem> items) {
    final breakdown = <String, int>{};
    for (final item in items) {
      final priority = item.priority.name;
      breakdown[priority] = (breakdown[priority] ?? 0) + 1;
    }
    return breakdown;
  }
}


// =====================================================================
// FILE: apps/giantt/lib/widgets/stats_card.dart
// =====================================================================

import 'package:flutter/material.dart';

class StatsCard extends StatelessWidget {
  final String title;
  final String value;
  final IconData icon;
  final Color? color;

  const StatsCard({
    super.key,
    required this.title,
    required this.value,
    required this.icon,
    this.color,
  });

  @override
  Widget build(BuildContext context) {
    return Container(
      width: 120,
      margin: const EdgeInsets.only(right: 16.0),
      child: Card(
        child: Padding(
          padding: const EdgeInsets.all(12.0),
          child: Column(
            mainAxisAlignment: MainAxisAlignment.center,
            children: [
              Icon(
                icon,
                size: 24,
                color: color ?? Theme.of(context).primaryColor,
              ),
              const SizedBox(height: 8),
              Text(
                value,
                style: Theme.of(context).textTheme.titleLarge?.copyWith(
                  fontWeight: FontWeight.bold,
                ),
              ),
              const SizedBox(height: 4),
              Text(
                title,
                style: Theme.of(context).textTheme.bodySmall,
                textAlign: TextAlign.center,
              ),
            ],
          ),
        ),
      ),
    );
  }
}


// =====================================================================
// FILE: apps/giantt/lib/widgets/item_card.dart
// =====================================================================

import 'package:flutter/material.dart';
import 'package:giantt_core/giantt_core.dart';

class ItemCard extends StatelessWidget {
  final GianttItem item;
  final VoidCallback? onTap;

  const ItemCard({
    super.key,
    required this.item,
    this.onTap,
  });

  @override
  Widget build(BuildContext context) {
    return Card(
      margin: const EdgeInsets.symmetric(vertical: 4.0),
      child: ListTile(
        leading: CircleAvatar(
          backgroundColor: _getStatusColor(),
          child: Text(
            item.status.symbol,
            style: const TextStyle(color: Colors.white),
          ),
        ),
        title: Row(
          children: [
            Expanded(
              child: Text(
                item.title,
                style: TextStyle(
                  decoration: item.occlude ? TextDecoration.lineThrough : null,
                  color: item.occlude ? Colors.grey : null,
                ),
              ),
            ),
            if (item.priority.symbol.isNotEmpty)
              Text(
                item.priority.symbol,
                style: TextStyle(
                  fontWeight: FontWeight.bold,
                  color: _getPriorityColor(),
                ),
              ),
          ],
        ),
        subtitle: Column(
          crossAxisAlignment: CrossAxisAlignment.start,
          children: [
            Text('ID: ${item.id}'),
            if (item.duration.totalSeconds > 0)
              Text('Duration: ${item.duration}'),
            if (item.charts.isNotEmpty)
              Text('Charts: ${item.charts.join(', ')}'),
            if (item.tags.isNotEmpty)
              Wrap(
                spacing: 4,
                children: item.tags.take(3).map((tag) {
                  return Chip(
                    label: Text(
                      tag,
                      style: const TextStyle(fontSize: 10),
                    ),
                    materialTapTargetSize: MaterialTapTargetSize.shrinkWrap,
                    visualDensity: VisualDensity.compact,
                  );
                }).toList(),
              ),
          ],
        ),
        trailing: item.occlude
            ? Icon(Icons.visibility_off, color: Colors.grey[400])
            : const Icon(Icons.chevron_right),
        onTap: onTap,
      ),
    );
  }

  Color _getStatusColor() {
    switch (item.status) {
      case GianttStatus.notStarted:
        return Colors.grey;
      case GianttStatus.inProgress:
        return Colors.blue;
      case GianttStatus.blocked:
        return Colors.red;
      case GianttStatus.completed:
        return Colors.green;
    }
  }

  Color _getPriorityColor() {
    switch (item.priority) {
      case GianttPriority.lowest:
      case GianttPriority.low:
        return Colors.green;
      case GianttPriority.neutral:
        return Colors.grey;
      case GianttPriority.unsure:
        return Colors.orange;
      case GianttPriority.medium:
        return Colors.amber;
      case GianttPriority.high:
        return Colors.deepOrange;
      case GianttPriority.critical:
        return Colors.red;
    }
  }
}


// =====================================================================
// FILE: apps/giantt/windows/runner/flutter_window.cpp
// =====================================================================

#include "flutter_window.h"

#include <optional>

#include "flutter/generated_plugin_registrant.h"

FlutterWindow::FlutterWindow(const flutter::DartProject& project)
    : project_(project) {}

FlutterWindow::~FlutterWindow() {}

bool FlutterWindow::OnCreate() {
  if (!Win32Window::OnCreate()) {
    return false;
  }

  RECT frame = GetClientArea();

  // The size here must match the window dimensions to avoid unnecessary surface
  // creation / destruction in the startup path.
  flutter_controller_ = std::make_unique<flutter::FlutterViewController>(
      frame.right - frame.left, frame.bottom - frame.top, project_);
  // Ensure that basic setup of the controller was successful.
  if (!flutter_controller_->engine() || !flutter_controller_->view()) {
    return false;
  }
  RegisterPlugins(flutter_controller_->engine());
  SetChildContent(flutter_controller_->view()->GetNativeWindow());

  flutter_controller_->engine()->SetNextFrameCallback([&]() {
    this->Show();
  });

  // Flutter can complete the first frame before the "show window" callback is
  // registered. The following call ensures a frame is pending to ensure the
  // window is shown. It is a no-op if the first frame hasn't completed yet.
  flutter_controller_->ForceRedraw();

  return true;
}

void FlutterWindow::OnDestroy() {
  if (flutter_controller_) {
    flutter_controller_ = nullptr;
  }

  Win32Window::OnDestroy();
}

LRESULT
FlutterWindow::MessageHandler(HWND hwnd, UINT const message,
                              WPARAM const wparam,
                              LPARAM const lparam) noexcept {
  // Give Flutter, including plugins, an opportunity to handle window messages.
  if (flutter_controller_) {
    std::optional<LRESULT> result =
        flutter_controller_->HandleTopLevelWindowProc(hwnd, message, wparam,
                                                      lparam);
    if (result) {
      return *result;
    }
  }

  switch (message) {
    case WM_FONTCHANGE:
      flutter_controller_->engine()->ReloadSystemFonts();
      break;
  }

  return Win32Window::MessageHandler(hwnd, message, wparam, lparam);
}


// =====================================================================
// FILE: apps/giantt/windows/runner/utils.h
// =====================================================================

#ifndef RUNNER_UTILS_H_
#define RUNNER_UTILS_H_

#include <string>
#include <vector>

// Creates a console for the process, and redirects stdout and stderr to
// it for both the runner and the Flutter library.
void CreateAndAttachConsole();

// Takes a null-terminated wchar_t* encoded in UTF-16 and returns a std::string
// encoded in UTF-8. Returns an empty std::string on failure.
std::string Utf8FromUtf16(const wchar_t* utf16_string);

// Gets the command line arguments passed in as a std::vector<std::string>,
// encoded in UTF-8. Returns an empty std::vector<std::string> on failure.
std::vector<std::string> GetCommandLineArguments();

#endif  // RUNNER_UTILS_H_


// =====================================================================
// FILE: apps/giantt/windows/runner/utils.cpp
// =====================================================================

#include "utils.h"

#include <flutter_windows.h>
#include <io.h>
#include <stdio.h>
#include <windows.h>

#include <iostream>

void CreateAndAttachConsole() {
  if (::AllocConsole()) {
    FILE *unused;
    if (freopen_s(&unused, "CONOUT$", "w", stdout)) {
      _dup2(_fileno(stdout), 1);
    }
    if (freopen_s(&unused, "CONOUT$", "w", stderr)) {
      _dup2(_fileno(stdout), 2);
    }
    std::ios::sync_with_stdio();
    FlutterDesktopResyncOutputStreams();
  }
}

std::vector<std::string> GetCommandLineArguments() {
  // Convert the UTF-16 command line arguments to UTF-8 for the Engine to use.
  int argc;
  wchar_t** argv = ::CommandLineToArgvW(::GetCommandLineW(), &argc);
  if (argv == nullptr) {
    return std::vector<std::string>();
  }

  std::vector<std::string> command_line_arguments;

  // Skip the first argument as it's the binary name.
  for (int i = 1; i < argc; i++) {
    command_line_arguments.push_back(Utf8FromUtf16(argv[i]));
  }

  ::LocalFree(argv);

  return command_line_arguments;
}

std::string Utf8FromUtf16(const wchar_t* utf16_string) {
  if (utf16_string == nullptr) {
    return std::string();
  }
  unsigned int target_length = ::WideCharToMultiByte(
      CP_UTF8, WC_ERR_INVALID_CHARS, utf16_string,
      -1, nullptr, 0, nullptr, nullptr)
    -1; // remove the trailing null character
  int input_length = (int)wcslen(utf16_string);
  std::string utf8_string;
  if (target_length == 0 || target_length > utf8_string.max_size()) {
    return utf8_string;
  }
  utf8_string.resize(target_length);
  int converted_length = ::WideCharToMultiByte(
      CP_UTF8, WC_ERR_INVALID_CHARS, utf16_string,
      input_length, utf8_string.data(), target_length, nullptr, nullptr);
  if (converted_length == 0) {
    return std::string();
  }
  return utf8_string;
}


// =====================================================================
// FILE: apps/giantt/windows/runner/win32_window.h
// =====================================================================

#ifndef RUNNER_WIN32_WINDOW_H_
#define RUNNER_WIN32_WINDOW_H_

#include <windows.h>

#include <functional>
#include <memory>
#include <string>

// A class abstraction for a high DPI-aware Win32 Window. Intended to be
// inherited from by classes that wish to specialize with custom
// rendering and input handling
class Win32Window {
 public:
  struct Point {
    unsigned int x;
    unsigned int y;
    Point(unsigned int x, unsigned int y) : x(x), y(y) {}
  };

  struct Size {
    unsigned int width;
    unsigned int height;
    Size(unsigned int width, unsigned int height)
        : width(width), height(height) {}
  };

  Win32Window();
  virtual ~Win32Window();

  // Creates a win32 window with |title| that is positioned and sized using
  // |origin| and |size|. New windows are created on the default monitor. Window
  // sizes are specified to the OS in physical pixels, hence to ensure a
  // consistent size this function will scale the inputted width and height as
  // as appropriate for the default monitor. The window is invisible until
  // |Show| is called. Returns true if the window was created successfully.
  bool Create(const std::wstring& title, const Point& origin, const Size& size);

  // Show the current window. Returns true if the window was successfully shown.
  bool Show();

  // Release OS resources associated with window.
  void Destroy();

  // Inserts |content| into the window tree.
  void SetChildContent(HWND content);

  // Returns the backing Window handle to enable clients to set icon and other
  // window properties. Returns nullptr if the window has been destroyed.
  HWND GetHandle();

  // If true, closing this window will quit the application.
  void SetQuitOnClose(bool quit_on_close);

  // Return a RECT representing the bounds of the current client area.
  RECT GetClientArea();

 protected:
  // Processes and route salient window messages for mouse handling,
  // size change and DPI. Delegates handling of these to member overloads that
  // inheriting classes can handle.
  virtual LRESULT MessageHandler(HWND window,
                                 UINT const message,
                                 WPARAM const wparam,
                                 LPARAM const lparam) noexcept;

  // Called when CreateAndShow is called, allowing subclass window-related
  // setup. Subclasses should return false if setup fails.
  virtual bool OnCreate();

  // Called when Destroy is called.
  virtual void OnDestroy();

 private:
  friend class WindowClassRegistrar;

  // OS callback called by message pump. Handles the WM_NCCREATE message which
  // is passed when the non-client area is being created and enables automatic
  // non-client DPI scaling so that the non-client area automatically
  // responds to changes in DPI. All other messages are handled by
  // MessageHandler.
  static LRESULT CALLBACK WndProc(HWND const window,
                                  UINT const message,
                                  WPARAM const wparam,
                                  LPARAM const lparam) noexcept;

  // Retrieves a class instance pointer for |window|
  static Win32Window* GetThisFromHandle(HWND const window) noexcept;

  // Update the window frame's theme to match the system theme.
  static void UpdateTheme(HWND const window);

  bool quit_on_close_ = false;

  // window handle for top level window.
  HWND window_handle_ = nullptr;

  // window handle for hosted content.
  HWND child_content_ = nullptr;
};

#endif  // RUNNER_WIN32_WINDOW_H_


// =====================================================================
// FILE: apps/giantt/windows/runner/win32_window.cpp
// =====================================================================

#include "win32_window.h"

#include <dwmapi.h>
#include <flutter_windows.h>

#include "resource.h"

namespace {

/// Window attribute that enables dark mode window decorations.
///
/// Redefined in case the developer's machine has a Windows SDK older than
/// version 10.0.22000.0.
/// See: https://docs.microsoft.com/windows/win32/api/dwmapi/ne-dwmapi-dwmwindowattribute
#ifndef DWMWA_USE_IMMERSIVE_DARK_MODE
#define DWMWA_USE_IMMERSIVE_DARK_MODE 20
#endif

constexpr const wchar_t kWindowClassName[] = L"FLUTTER_RUNNER_WIN32_WINDOW";

/// Registry key for app theme preference.
///
/// A value of 0 indicates apps should use dark mode. A non-zero or missing
/// value indicates apps should use light mode.
constexpr const wchar_t kGetPreferredBrightnessRegKey[] =
  L"Software\\Microsoft\\Windows\\CurrentVersion\\Themes\\Personalize";
constexpr const wchar_t kGetPreferredBrightnessRegValue[] = L"AppsUseLightTheme";

// The number of Win32Window objects that currently exist.
static int g_active_window_count = 0;

using EnableNonClientDpiScaling = BOOL __stdcall(HWND hwnd);

// Scale helper to convert logical scaler values to physical using passed in
// scale factor
int Scale(int source, double scale_factor) {
  return static_cast<int>(source * scale_factor);
}

// Dynamically loads the |EnableNonClientDpiScaling| from the User32 module.
// This API is only needed for PerMonitor V1 awareness mode.
void EnableFullDpiSupportIfAvailable(HWND hwnd) {
  HMODULE user32_module = LoadLibraryA("User32.dll");
  if (!user32_module) {
    return;
  }
  auto enable_non_client_dpi_scaling =
      reinterpret_cast<EnableNonClientDpiScaling*>(
          GetProcAddress(user32_module, "EnableNonClientDpiScaling"));
  if (enable_non_client_dpi_scaling != nullptr) {
    enable_non_client_dpi_scaling(hwnd);
  }
  FreeLibrary(user32_module);
}

}  // namespace

// Manages the Win32Window's window class registration.
class WindowClassRegistrar {
 public:
  ~WindowClassRegistrar() = default;

  // Returns the singleton registrar instance.
  static WindowClassRegistrar* GetInstance() {
    if (!instance_) {
      instance_ = new WindowClassRegistrar();
    }
    return instance_;
  }

  // Returns the name of the window class, registering the class if it hasn't
  // previously been registered.
  const wchar_t* GetWindowClass();

  // Unregisters the window class. Should only be called if there are no
  // instances of the window.
  void UnregisterWindowClass();

 private:
  WindowClassRegistrar() = default;

  static WindowClassRegistrar* instance_;

  bool class_registered_ = false;
};

WindowClassRegistrar* WindowClassRegistrar::instance_ = nullptr;

const wchar_t* WindowClassRegistrar::GetWindowClass() {
  if (!class_registered_) {
    WNDCLASS window_class{};
    window_class.hCursor = LoadCursor(nullptr, IDC_ARROW);
    window_class.lpszClassName = kWindowClassName;
    window_class.style = CS_HREDRAW | CS_VREDRAW;
    window_class.cbClsExtra = 0;
    window_class.cbWndExtra = 0;
    window_class.hInstance = GetModuleHandle(nullptr);
    window_class.hIcon =
        LoadIcon(window_class.hInstance, MAKEINTRESOURCE(IDI_APP_ICON));
    window_class.hbrBackground = 0;
    window_class.lpszMenuName = nullptr;
    window_class.lpfnWndProc = Win32Window::WndProc;
    RegisterClass(&window_class);
    class_registered_ = true;
  }
  return kWindowClassName;
}

void WindowClassRegistrar::UnregisterWindowClass() {
  UnregisterClass(kWindowClassName, nullptr);
  class_registered_ = false;
}

Win32Window::Win32Window() {
  ++g_active_window_count;
}

Win32Window::~Win32Window() {
  --g_active_window_count;
  Destroy();
}

bool Win32Window::Create(const std::wstring& title,
                         const Point& origin,
                         const Size& size) {
  Destroy();

  const wchar_t* window_class =
      WindowClassRegistrar::GetInstance()->GetWindowClass();

  const POINT target_point = {static_cast<LONG>(origin.x),
                              static_cast<LONG>(origin.y)};
  HMONITOR monitor = MonitorFromPoint(target_point, MONITOR_DEFAULTTONEAREST);
  UINT dpi = FlutterDesktopGetDpiForMonitor(monitor);
  double scale_factor = dpi / 96.0;

  HWND window = CreateWindow(
      window_class, title.c_str(), WS_OVERLAPPEDWINDOW,
      Scale(origin.x, scale_factor), Scale(origin.y, scale_factor),
      Scale(size.width, scale_factor), Scale(size.height, scale_factor),
      nullptr, nullptr, GetModuleHandle(nullptr), this);

  if (!window) {
    return false;
  }

  UpdateTheme(window);

  return OnCreate();
}

bool Win32Window::Show() {
  return ShowWindow(window_handle_, SW_SHOWNORMAL);
}

// static
LRESULT CALLBACK Win32Window::WndProc(HWND const window,
                                      UINT const message,
                                      WPARAM const wparam,
                                      LPARAM const lparam) noexcept {
  if (message == WM_NCCREATE) {
    auto window_struct = reinterpret_cast<CREATESTRUCT*>(lparam);
    SetWindowLongPtr(window, GWLP_USERDATA,
                     reinterpret_cast<LONG_PTR>(window_struct->lpCreateParams));

    auto that = static_cast<Win32Window*>(window_struct->lpCreateParams);
    EnableFullDpiSupportIfAvailable(window);
    that->window_handle_ = window;
  } else if (Win32Window* that = GetThisFromHandle(window)) {
    return that->MessageHandler(window, message, wparam, lparam);
  }

  return DefWindowProc(window, message, wparam, lparam);
}

LRESULT
Win32Window::MessageHandler(HWND hwnd,
                            UINT const message,
                            WPARAM const wparam,
                            LPARAM const lparam) noexcept {
  switch (message) {
    case WM_DESTROY:
      window_handle_ = nullptr;
      Destroy();
      if (quit_on_close_) {
        PostQuitMessage(0);
      }
      return 0;

    case WM_DPICHANGED: {
      auto newRectSize = reinterpret_cast<RECT*>(lparam);
      LONG newWidth = newRectSize->right - newRectSize->left;
      LONG newHeight = newRectSize->bottom - newRectSize->top;

      SetWindowPos(hwnd, nullptr, newRectSize->left, newRectSize->top, newWidth,
                   newHeight, SWP_NOZORDER | SWP_NOACTIVATE);

      return 0;
    }
    case WM_SIZE: {
      RECT rect = GetClientArea();
      if (child_content_ != nullptr) {
        // Size and position the child window.
        MoveWindow(child_content_, rect.left, rect.top, rect.right - rect.left,
                   rect.bottom - rect.top, TRUE);
      }
      return 0;
    }

    case WM_ACTIVATE:
      if (child_content_ != nullptr) {
        SetFocus(child_content_);
      }
      return 0;

    case WM_DWMCOLORIZATIONCOLORCHANGED:
      UpdateTheme(hwnd);
      return 0;
  }

  return DefWindowProc(window_handle_, message, wparam, lparam);
}

void Win32Window::Destroy() {
  OnDestroy();

  if (window_handle_) {
    DestroyWindow(window_handle_);
    window_handle_ = nullptr;
  }
  if (g_active_window_count == 0) {
    WindowClassRegistrar::GetInstance()->UnregisterWindowClass();
  }
}

Win32Window* Win32Window::GetThisFromHandle(HWND const window) noexcept {
  return reinterpret_cast<Win32Window*>(
      GetWindowLongPtr(window, GWLP_USERDATA));
}

void Win32Window::SetChildContent(HWND content) {
  child_content_ = content;
  SetParent(content, window_handle_);
  RECT frame = GetClientArea();

  MoveWindow(content, frame.left, frame.top, frame.right - frame.left,
             frame.bottom - frame.top, true);

  SetFocus(child_content_);
}

RECT Win32Window::GetClientArea() {
  RECT frame;
  GetClientRect(window_handle_, &frame);
  return frame;
}

HWND Win32Window::GetHandle() {
  return window_handle_;
}

void Win32Window::SetQuitOnClose(bool quit_on_close) {
  quit_on_close_ = quit_on_close;
}

bool Win32Window::OnCreate() {
  // No-op; provided for subclasses.
  return true;
}

void Win32Window::OnDestroy() {
  // No-op; provided for subclasses.
}

void Win32Window::UpdateTheme(HWND const window) {
  DWORD light_mode;
  DWORD light_mode_size = sizeof(light_mode);
  LSTATUS result = RegGetValue(HKEY_CURRENT_USER, kGetPreferredBrightnessRegKey,
                               kGetPreferredBrightnessRegValue,
                               RRF_RT_REG_DWORD, nullptr, &light_mode,
                               &light_mode_size);

  if (result == ERROR_SUCCESS) {
    BOOL enable_dark_mode = light_mode == 0;
    DwmSetWindowAttribute(window, DWMWA_USE_IMMERSIVE_DARK_MODE,
                          &enable_dark_mode, sizeof(enable_dark_mode));
  }
}


// =====================================================================
// FILE: apps/giantt/windows/runner/resource.h
// =====================================================================

//{{NO_DEPENDENCIES}}
// Microsoft Visual C++ generated include file.
// Used by Runner.rc
//
#define IDI_APP_ICON                    101

// Next default values for new objects
//
#ifdef APSTUDIO_INVOKED
#ifndef APSTUDIO_READONLY_SYMBOLS
#define _APS_NEXT_RESOURCE_VALUE        102
#define _APS_NEXT_COMMAND_VALUE         40001
#define _APS_NEXT_CONTROL_VALUE         1001
#define _APS_NEXT_SYMED_VALUE           101
#endif
#endif


// =====================================================================
// FILE: apps/giantt/windows/runner/main.cpp
// =====================================================================

#include <flutter/dart_project.h>
#include <flutter/flutter_view_controller.h>
#include <windows.h>

#include "flutter_window.h"
#include "utils.h"

int APIENTRY wWinMain(_In_ HINSTANCE instance, _In_opt_ HINSTANCE prev,
                      _In_ wchar_t *command_line, _In_ int show_command) {
  // Attach to console when present (e.g., 'flutter run') or create a
  // new console when running with a debugger.
  if (!::AttachConsole(ATTACH_PARENT_PROCESS) && ::IsDebuggerPresent()) {
    CreateAndAttachConsole();
  }

  // Initialize COM, so that it is available for use in the library and/or
  // plugins.
  ::CoInitializeEx(nullptr, COINIT_APARTMENTTHREADED);

  flutter::DartProject project(L"data");

  std::vector<std::string> command_line_arguments =
      GetCommandLineArguments();

  project.set_dart_entrypoint_arguments(std::move(command_line_arguments));

  FlutterWindow window(project);
  Win32Window::Point origin(10, 10);
  Win32Window::Size size(1280, 720);
  if (!window.Create(L"giantt", origin, size)) {
    return EXIT_FAILURE;
  }
  window.SetQuitOnClose(true);

  ::MSG msg;
  while (::GetMessage(&msg, nullptr, 0, 0)) {
    ::TranslateMessage(&msg);
    ::DispatchMessage(&msg);
  }

  ::CoUninitialize();
  return EXIT_SUCCESS;
}


// =====================================================================
// FILE: apps/giantt/windows/runner/flutter_window.h
// =====================================================================

#ifndef RUNNER_FLUTTER_WINDOW_H_
#define RUNNER_FLUTTER_WINDOW_H_

#include <flutter/dart_project.h>
#include <flutter/flutter_view_controller.h>

#include <memory>

#include "win32_window.h"

// A window that does nothing but host a Flutter view.
class FlutterWindow : public Win32Window {
 public:
  // Creates a new FlutterWindow hosting a Flutter view running |project|.
  explicit FlutterWindow(const flutter::DartProject& project);
  virtual ~FlutterWindow();

 protected:
  // Win32Window:
  bool OnCreate() override;
  void OnDestroy() override;
  LRESULT MessageHandler(HWND window, UINT const message, WPARAM const wparam,
                         LPARAM const lparam) noexcept override;

 private:
  // The project to run.
  flutter::DartProject project_;

  // The Flutter instance hosted by this window.
  std::unique_ptr<flutter::FlutterViewController> flutter_controller_;
};

#endif  // RUNNER_FLUTTER_WINDOW_H_


// =====================================================================
// FILE: apps/giantt/windows/flutter/generated_plugin_registrant.h
// =====================================================================

//
//  Generated file. Do not edit.
//

// clang-format off

#ifndef GENERATED_PLUGIN_REGISTRANT_
#define GENERATED_PLUGIN_REGISTRANT_

#include <flutter/plugin_registry.h>

// Registers Flutter plugins.
void RegisterPlugins(flutter::PluginRegistry* registry);

#endif  // GENERATED_PLUGIN_REGISTRANT_


======================================================================
= CONFIGURATION FILES                                                =
======================================================================



# =====================================================================
# FILE: pubspec.yaml
# =====================================================================

name: soradyne_workspace
description: Workspace root for the Soradyne monorepo.
publish_to: "none"

environment:
  sdk: ">=3.0.0 <4.0.0"

dev_dependencies:
  melos: ^4.0.0



# =====================================================================
# FILE: melos.yaml
# =====================================================================

name: soradyne_workspace
repository: https://github.com/yourusername/soradyne

packages:
  - packages/*
  - apps/*

command:
  bootstrap:
    runPubGetInParallel: false
    
scripts:
  build:rust:
    run: cargo build --release
    exec:
      cwd: packages/soradyne_core
      
  test:flutter:
    run: flutter test
    exec:
      packageFilters:
        scope: ['soradyne_flutter', 'ai_chat_flutter']
        
  analyze:
    run: flutter analyze
    exec:
      packageFilters:
        scope: ['*']
        
  build:apps:
    run: flutter build
    exec:
      packageFilters:
        scope: ['soradyne_demo', 'giantt']


# =====================================================================
# FILE: analysis_options.yaml
# =====================================================================

# This file configures the analyzer, which statically analyzes Dart code to
# check for errors, warnings, and lints.
#
# The issues identified by the analyzer are surfaced in the UI of Dart-enabled
# IDEs (https://dart.dev/tools#ides-and-editors). The analyzer can also be
# invoked from the command line by running `flutter analyze`.

# The following line activates a set of recommended lints for Flutter apps,
# packages, and plugins designed to encourage good coding practices.
include: package:flutter_lints/flutter.yaml

linter:
  # The lint rules applied to this project can be customized in the
  # section below to disable rules from the `package:flutter_lints/flutter.yaml`
  # included above or to enable additional rules. A list of all available lints
  # and their documentation is published at https://dart.dev/lints.
  #
  # Instead of disabling a lint rule for the entire project in the
  # section below, it can also be suppressed for a single line of code
  # or a specific dart file by using the `// ignore: name_of_lint` and
  # `// ignore_for_file: name_of_lint` syntax on the line or in the file
  # producing the lint.
  rules:
    # avoid_print: false  # Uncomment to disable the `avoid_print` rule
    # prefer_single_quotes: true  # Uncomment to enable the `prefer_single_quotes` rule

# Additional information about this file can be found at
# https://dart.dev/guides/language/analysis-options


// =====================================================================
// FILE: .source_manager/config.json
// =====================================================================

{
  "project_name": "Soradyne & Giantt Multi-Platform Suite",
  "output_file": "soradyne_giantt_source.txt",
  "file_types": {
    "source": {
      "extensions": [
        ".py",
        ".js",
        ".ts",
        ".rs",
        ".dart",
        ".java",
        ".cpp",
        ".c",
        ".h",
        ".go",
        ".cs"
      ],
      "description": "Source code files"
    },
    "config": {
      "extensions": [
        ".json",
        ".yaml",
        ".yml",
        ".toml",
        ".ini",
        ".cfg"
      ],
      "description": "Configuration files"
    },
    "markup": {
      "extensions": [
        ".html",
        ".xml",
        ".md",
        ".rst"
      ],
      "description": "Markup and documentation files"
    },
    "scripts": {
      "extensions": [
        ".sh",
        ".bat",
        ".ps1"
      ],
      "description": "Shell scripts and batch files"
    }
  },
  "excluded_dirs": [
    ".git",
    "node_modules",
    "__pycache__",
    ".pytest_cache",
    "target",
    "build",
    "dist",
    ".dart_tool",
    "coverage",
    "venv",
    "env",
    ".env",
    "vendor",
    ".aider",
    ".vscode",
    ".idea",
    "bin",
    "obj",
    ".webpack",
    "out",
    "Generated"
  ],
  "auto_include_patterns": [],
  "banner_config": {
    "padding_h": 5,
    "padding_v": 1,
    "char": "="
  }
}


# =====================================================================
# FILE: packages/ai_chat_flutter/pubspec.yaml
# =====================================================================

name: ai_chat_flutter
description: AI chat with action calling capabilities for Flutter apps
version: 0.1.0

environment:
  sdk: '>=3.0.0 <4.0.0'
  flutter: ">=3.0.0"

dependencies:
  flutter:
    sdk: flutter
  http: ^1.1.0
  uuid: ^4.0.0
  collection: ^1.17.0

dev_dependencies:
  flutter_test:
    sdk: flutter
  flutter_lints: ^3.0.0



# =====================================================================
# FILE: packages/soradyne_core/Cargo.toml
# =====================================================================

[package]
name = "soradyne"
version = "0.1.0"
edition = "2021"
license = "MIT"

[lib]
crate-type = ["cdylib", "rlib"]

[[example]]
name = "block_storage_demo"
path = "examples/block_storage_demo.rs"

[dependencies]
# Core functionality
tokio = { version = "1.28", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
uuid = { version = "1.3", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }

# Error handling
thiserror = "1.0"
log = "0.4"
anyhow = "1.0"
hex = "0.4.3"
sha2 = "0.10.9"

# Networking
warp = { version = "0.3", features = ["multipart"] }
futures-util = "0.3"
bytes = "1.0"
env_logger = "0.10"
multer = "2.1"

# HTTP client for downloading test images
reqwest = { version = "0.11", features = ["stream", "json"] }

# Image processing
image = "0.24"

# Video frame extraction
ffmpeg-next = { version = "7.0", features = ["static", "build"], optional = true }

# Base64 encoding for testing
base64 = { version = "0.21", optional = true }

# Additional dependencies for album system
tempfile = "3.8"

# Rendering dependencies
imageproc = "0.23"
ab_glyph = "0.2"
async-trait = "0.1.88"

# Erasure coding
reed-solomon-erasure = "6.0"

# Cryptography
rand = "0.8"
aes-gcm = "0.10"

# Platform-specific dependencies for device fingerprinting
[target.'cfg(unix)'.dependencies]
libc = "0.2"

[target.'cfg(windows)'.dependencies]
winapi = { version = "0.3", features = ["fileapi", "handleapi", "winioctl"] }

[build-dependencies]
ffmpeg-next = { version = "7.0", features = ["build"] }

[dev-dependencies]
# Testing utilities
tempfile = "3.8"
criterion = "0.5"

[features]
default = ["video-thumbnails"]
image-processing = ["base64"]
video-thumbnails = ["ffmpeg-next"]

[profile.release]
lto = true
opt-level = 3
codegen-units = 1
panic = "abort"

[[example]]
name = "block_storage_example"
path = "examples/block_storage_example.rs"

[[example]]
name = "block_storage_real_images"
path = "examples/block_storage_real_images.rs"
required-features = []

[[example]]
name = "visual_block_test"
path = "examples/visual_block_test.rs"
required-features = []

[[example]]
name = "large_video_test"
path = "examples/large_video_test.rs"
required-features = []

[[example]]
name = "album_sync_test"
path = "examples/album_sync_test.rs"
required-features = []

[[example]]
name = "renderer_test"
path = "examples/renderer_test.rs"
required-features = []

[[bin]]
name = "sd_card_test"
path = "src/bin/sd_card_test.rs"
required-features = []




// =====================================================================
// FILE: packages/soradyne_core/large_video_test/metadata.json
// =====================================================================

{
  "blocks": {
    "e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66": {
      "id": [
        225,
        103,
        108,
        195,
        201,
        192,
        100,
        102,
        165,
        179,
        47,
        152,
        122,
        96,
        17,
        133,
        173,
        4,
        124,
        92,
        98,
        122,
        171,
        148,
        115,
        170,
        66,
        150,
        149,
        250,
        188,
        102
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:07:38.224841Z",
      "modified_at": "2025-07-17T04:07:38.224842Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "0ec07a22-d9dc-4d05-ae00-467ece05fa4c",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.0.shard",
          "key_share_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "3fb05b48-1f57-4590-82ab-9846cc619c40",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.1.shard",
          "key_share_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "e8046da6-2eb2-4829-9468-0960f031e49b",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.2.shard",
          "key_share_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "ea6aef3f-dccf-48c3-8677-6d5cec037493",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.3.shard",
          "key_share_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "febbcaef-b969-4289-ac69-9309324273a3",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.4.shard",
          "key_share_path": "e1/67/e1676cc3c9c06466a5b32f987a601185ad047c5c627aab9473aa429695fabc66.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        82,
        132,
        11,
        185,
        216,
        26,
        9,
        111,
        227,
        11,
        225,
        85
      ]
    },
    "f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b": {
      "id": [
        240,
        213,
        36,
        237,
        131,
        235,
        143,
        44,
        230,
        121,
        226,
        25,
        25,
        202,
        170,
        64,
        208,
        227,
        240,
        114,
        231,
        102,
        112,
        233,
        106,
        185,
        76,
        28,
        151,
        195,
        107,
        123
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:06:58.351513Z",
      "modified_at": "2025-07-17T04:06:58.351513Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "f556920f-c8ab-42eb-8647-caf006fbb81a",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.0.shard",
          "key_share_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "b4ef2781-3832-4deb-86d1-3cfc2bdd08bb",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.1.shard",
          "key_share_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "c5e77258-f52a-43f9-a3a3-2f527d1eadda",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.2.shard",
          "key_share_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "18503f7c-b19c-4efe-a234-ca4a8c457d7b",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.3.shard",
          "key_share_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "d95b5ccf-9859-4405-b345-80953fbf423f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.4.shard",
          "key_share_path": "f0/d5/f0d524ed83eb8f2ce679e21919caaa40d0e3f072e76670e96ab94c1c97c36b7b.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        206,
        163,
        213,
        90,
        32,
        211,
        1,
        192,
        158,
        164,
        76,
        186
      ]
    },
    "c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957": {
      "id": [
        193,
        57,
        8,
        42,
        11,
        45,
        194,
        232,
        53,
        61,
        97,
        98,
        65,
        254,
        134,
        159,
        39,
        171,
        111,
        242,
        147,
        194,
        116,
        54,
        21,
        172,
        160,
        30,
        250,
        10,
        233,
        87
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:04:57.134496Z",
      "modified_at": "2025-07-17T04:04:57.134496Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "e4022835-33c1-46d2-8784-c3c2ff68104b",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.0.shard",
          "key_share_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "f31fe362-f564-4ee2-8a5e-61c9c0ffa9d8",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.1.shard",
          "key_share_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "4bbed1e5-9fd0-4610-8d21-dea5366c67f5",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.2.shard",
          "key_share_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "9ef47b40-2c80-4da5-b206-39d8e7124b68",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.3.shard",
          "key_share_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "3ffdf3bc-f2e8-4a5c-8c8c-686ea14711d8",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.4.shard",
          "key_share_path": "c1/39/c139082a0b2dc2e8353d616241fe869f27ab6ff293c2743615aca01efa0ae957.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        24,
        98,
        0,
        241,
        164,
        147,
        87,
        226,
        29,
        183,
        216,
        176
      ]
    },
    "89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469": {
      "id": [
        137,
        215,
        75,
        127,
        182,
        168,
        245,
        3,
        249,
        131,
        189,
        137,
        39,
        4,
        194,
        170,
        144,
        110,
        232,
        144,
        128,
        233,
        24,
        145,
        172,
        204,
        64,
        3,
        82,
        245,
        52,
        105
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:05:19.966203Z",
      "modified_at": "2025-07-17T04:05:19.966203Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "db52f069-3508-47d3-b3ca-98c4385da303",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.0.shard",
          "key_share_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "cc0023fd-b28e-4bed-9d92-f58c29e8d0d3",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.1.shard",
          "key_share_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "b38cb901-9948-4150-acf7-cf6c12ace0d4",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.2.shard",
          "key_share_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "b8f2dbeb-72dc-4250-bb98-def63630e5e0",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.3.shard",
          "key_share_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "e536592d-b4c2-4b72-972a-1327b95ed0c1",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.4.shard",
          "key_share_path": "89/d7/89d74b7fb6a8f503f983bd892704c2aa906ee89080e91891accc400352f53469.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        2,
        183,
        100,
        76,
        3,
        125,
        47,
        226,
        112,
        148,
        0,
        97
      ]
    },
    "6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed": {
      "id": [
        106,
        199,
        140,
        94,
        8,
        198,
        140,
        214,
        102,
        30,
        92,
        158,
        246,
        192,
        149,
        125,
        26,
        136,
        185,
        137,
        163,
        167,
        41,
        42,
        197,
        243,
        255,
        139,
        179,
        22,
        200,
        237
      ],
      "directness": 0,
      "size": 64,
      "created_at": "2025-07-17T04:05:12.922528Z",
      "modified_at": "2025-07-17T04:05:12.922528Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "550adc61-efe2-4d49-afda-47c5321b9c1c",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.0.shard",
          "key_share_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "433a9904-1019-48fc-9424-dd65b76f4cb6",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.1.shard",
          "key_share_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "7058e208-34fc-45cb-abb1-468afcf4ed67",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.2.shard",
          "key_share_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "86d67d0d-ff55-40c3-9c44-2e0f6f1db5bb",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.3.shard",
          "key_share_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "ddba585a-7630-4dc7-9e1b-ab8886d4596d",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.4.shard",
          "key_share_path": "6a/c7/6ac78c5e08c68cd6661e5c9ef6c0957d1a88b989a3a7292ac5f3ff8bb316c8ed.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        221,
        79,
        129,
        177,
        151,
        50,
        32,
        145,
        81,
        55,
        34,
        52
      ]
    },
    "e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545": {
      "id": [
        232,
        100,
        172,
        238,
        162,
        64,
        179,
        178,
        83,
        195,
        95,
        76,
        23,
        215,
        138,
        69,
        136,
        116,
        11,
        45,
        39,
        235,
        174,
        184,
        36,
        51,
        213,
        235,
        105,
        96,
        245,
        69
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:04:41.103206Z",
      "modified_at": "2025-07-17T04:04:41.103206Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "7b5c0bb3-47d0-4e70-bc8e-033b84735003",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.0.shard",
          "key_share_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "56a99200-a49a-4825-bdaa-c3f26474c3ef",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.1.shard",
          "key_share_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "32dfb0e7-3d0a-4b54-a85f-b39ac8e05f54",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.2.shard",
          "key_share_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "b5f6d347-017f-41c6-a70c-cd7496bbae42",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.3.shard",
          "key_share_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "cb92ec58-3be9-4eee-8813-b56381796d97",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.4.shard",
          "key_share_path": "e8/64/e864aceea240b3b253c35f4c17d78a4588740b2d27ebaeb82433d5eb6960f545.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        57,
        138,
        6,
        95,
        219,
        32,
        190,
        156,
        212,
        169,
        252,
        62
      ]
    },
    "603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33": {
      "id": [
        96,
        63,
        200,
        185,
        212,
        29,
        144,
        21,
        244,
        20,
        58,
        177,
        108,
        146,
        8,
        58,
        31,
        250,
        186,
        141,
        138,
        152,
        129,
        211,
        124,
        9,
        237,
        213,
        162,
        47,
        238,
        51
      ],
      "directness": 0,
      "size": 16777216,
      "created_at": "2025-07-17T04:04:49.307091Z",
      "modified_at": "2025-07-17T04:04:49.307091Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "e303730c-41d7-4867-895c-56d481a853be",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.0.shard",
          "key_share_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "7ed682bd-f92a-44bc-99d9-0bc94b731cf5",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.1.shard",
          "key_share_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "787cc0ac-780a-4e77-8ed3-44e62d66f9c3",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.2.shard",
          "key_share_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "14a099ab-d1f3-48be-9b9a-ba4cf8961956",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.3.shard",
          "key_share_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "21630e41-e457-4a19-a1cb-440ae15180f5",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.4.shard",
          "key_share_path": "60/3f/603fc8b9d41d9015f4143ab16c92083a1ffaba8d8a9881d37c09edd5a22fee33.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        28,
        72,
        98,
        167,
        152,
        100,
        106,
        209,
        193,
        126,
        236,
        10
      ]
    },
    "b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1": {
      "id": [
        185,
        250,
        140,
        68,
        92,
        37,
        161,
        240,
        41,
        59,
        107,
        240,
        250,
        45,
        204,
        192,
        222,
        155,
        205,
        24,
        192,
        130,
        127,
        158,
        132,
        163,
        7,
        233,
        17,
        204,
        52,
        209
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:07:30.291972Z",
      "modified_at": "2025-07-17T04:07:30.291972Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "4b10a04d-80ce-428e-9d89-925b465111ea",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.0.shard",
          "key_share_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "59256925-7c52-45d7-8c88-4bfdbc005588",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.1.shard",
          "key_share_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "a14aff25-64f2-4c77-83df-fb3389bea027",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.2.shard",
          "key_share_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "9f5e304b-f2f5-419e-97a7-4b0f6d167928",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.3.shard",
          "key_share_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "01c0cdac-b140-4964-9530-147bc795588e",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.4.shard",
          "key_share_path": "b9/fa/b9fa8c445c25a1f0293b6bf0fa2dccc0de9bcd18c0827f9e84a307e911cc34d1.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        54,
        201,
        139,
        100,
        80,
        240,
        109,
        52,
        111,
        205,
        7,
        74
      ]
    },
    "14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c": {
      "id": [
        20,
        226,
        140,
        213,
        197,
        104,
        248,
        38,
        187,
        209,
        151,
        146,
        97,
        53,
        197,
        228,
        122,
        112,
        89,
        181,
        46,
        231,
        41,
        39,
        94,
        17,
        211,
        151,
        202,
        84,
        33,
        60
      ],
      "directness": 0,
      "size": 16777216,
      "created_at": "2025-07-17T04:04:28.238838Z",
      "modified_at": "2025-07-17T04:04:28.238838Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "abff3800-018c-4c11-917f-00a0fd03fb87",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.0.shard",
          "key_share_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "e5d3010c-fa02-4155-9b2d-a852cf9dcdc6",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.1.shard",
          "key_share_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "a4feec12-4439-4e17-8fa0-8a518816ee6f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.2.shard",
          "key_share_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "cf1bb9ca-4359-4fe3-bbfb-b14fefbcd716",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.3.shard",
          "key_share_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "a3d5ee06-5d83-49ef-a6c5-c920bdeacefb",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.4.shard",
          "key_share_path": "14/e2/14e28cd5c568f826bbd197926135c5e47a7059b52ee729275e11d397ca54213c.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        101,
        13,
        251,
        136,
        226,
        223,
        50,
        201,
        99,
        73,
        166,
        253
      ]
    },
    "d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642": {
      "id": [
        209,
        215,
        112,
        139,
        63,
        54,
        130,
        235,
        227,
        168,
        198,
        129,
        195,
        83,
        30,
        242,
        239,
        233,
        178,
        238,
        149,
        144,
        32,
        175,
        239,
        214,
        124,
        191,
        186,
        147,
        102,
        66
      ],
      "directness": 0,
      "size": 64,
      "created_at": "2025-07-17T04:04:53.577263Z",
      "modified_at": "2025-07-17T04:04:53.577263Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "ede1456e-c5c6-4c2b-9ef3-9f1b932fce88",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.0.shard",
          "key_share_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "fe8fba86-790c-40f4-a88c-b5776a1e470a",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.1.shard",
          "key_share_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "922604db-8e08-4372-8ce5-4e2f47a9eb1f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.2.shard",
          "key_share_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "21a1a69d-14fa-4fbe-9983-f9463de01535",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.3.shard",
          "key_share_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "9fac9ea4-402a-4a24-aea4-b211135e1390",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.4.shard",
          "key_share_path": "d1/d7/d1d7708b3f3682ebe3a8c681c3531ef2efe9b2ee959020afefd67cbfba936642.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        149,
        56,
        242,
        50,
        170,
        108,
        184,
        180,
        145,
        193,
        97,
        23
      ]
    },
    "a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c": {
      "id": [
        164,
        31,
        153,
        95,
        95,
        172,
        65,
        248,
        199,
        99,
        237,
        87,
        85,
        165,
        9,
        52,
        115,
        249,
        145,
        95,
        246,
        20,
        4,
        66,
        90,
        239,
        219,
        108,
        37,
        57,
        168,
        60
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:07:06.282381Z",
      "modified_at": "2025-07-17T04:07:06.282381Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "a2989bda-0570-4451-94d3-9c1953d5d5f9",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.0.shard",
          "key_share_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "6e1461bb-6e23-4e27-9f3c-f64996b50f26",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.1.shard",
          "key_share_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "bf312035-7480-4086-9d1d-6d82ab9aec00",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.2.shard",
          "key_share_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "a268fbe3-905b-4886-b8e1-1bbeb00d28c5",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.3.shard",
          "key_share_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "d5320b3d-6de1-47ca-b8d2-670ee173a493",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.4.shard",
          "key_share_path": "a4/1f/a41f995f5fac41f8c763ed5755a5093473f9915ff61404425aefdb6c2539a83c.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        219,
        31,
        109,
        121,
        182,
        88,
        96,
        176,
        151,
        39,
        130,
        134
      ]
    },
    "4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6": {
      "id": [
        78,
        89,
        26,
        185,
        152,
        69,
        248,
        75,
        126,
        170,
        62,
        4,
        209,
        224,
        191,
        201,
        230,
        213,
        93,
        6,
        167,
        87,
        249,
        200,
        67,
        75,
        70,
        205,
        254,
        167,
        8,
        214
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:07:22.359591Z",
      "modified_at": "2025-07-17T04:07:22.359591Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "3b85549a-ca65-4925-879a-e419cd0c7180",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.0.shard",
          "key_share_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "c7835328-8e1e-4e47-94df-626cacafd546",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.1.shard",
          "key_share_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "c349f639-5826-4f6a-8a31-bebf3c6e7c54",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.2.shard",
          "key_share_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "176b3eac-27e7-432a-9d65-b65006d27953",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.3.shard",
          "key_share_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "83fbb6bb-c192-49cf-934c-4aac23dd904a",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.4.shard",
          "key_share_path": "4e/59/4e591ab99845f84b7eaa3e04d1e0bfc9e6d55d06a757f9c8434b46cdfea708d6.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        182,
        145,
        83,
        128,
        226,
        37,
        79,
        83,
        19,
        181,
        181,
        10
      ]
    },
    "4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6": {
      "id": [
        76,
        48,
        227,
        29,
        97,
        232,
        67,
        58,
        18,
        206,
        222,
        99,
        153,
        53,
        68,
        11,
        88,
        5,
        145,
        126,
        106,
        23,
        185,
        25,
        216,
        109,
        82,
        203,
        108,
        198,
        71,
        182
      ],
      "directness": 0,
      "size": 256,
      "created_at": "2025-07-17T04:07:46.107020Z",
      "modified_at": "2025-07-17T04:07:46.107021Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "dc95c10e-e078-4730-aa63-c37756d493d3",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.0.shard",
          "key_share_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "d4d3cca3-4aec-4885-a736-9b9471a95fd2",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.1.shard",
          "key_share_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "8f777c8c-fbd4-458f-a8be-a12d01bf0bfa",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.2.shard",
          "key_share_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "d7a4fa1e-49f6-408b-a1fa-46963d73ed5f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.3.shard",
          "key_share_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "d408022e-365e-43e8-99fb-2bcf3d8e2122",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.4.shard",
          "key_share_path": "4c/30/4c30e31d61e8433a12cede639935440b5805917e6a17b919d86d52cb6cc647b6.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        117,
        241,
        236,
        103,
        123,
        81,
        204,
        51,
        50,
        243,
        20,
        29
      ]
    },
    "2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e": {
      "id": [
        37,
        65,
        218,
        160,
        139,
        159,
        17,
        110,
        163,
        17,
        80,
        27,
        160,
        0,
        101,
        145,
        174,
        109,
        40,
        244,
        69,
        121,
        246,
        231,
        54,
        67,
        223,
        60,
        97,
        236,
        135,
        62
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:05:35.675183Z",
      "modified_at": "2025-07-17T04:05:35.675183Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "a40fc2a3-43e4-4505-a533-ec7b5673aa54",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.0.shard",
          "key_share_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "b4c7e5cd-5b08-4d82-9b58-b75519e240b4",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.1.shard",
          "key_share_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "1f82024b-fb70-4b68-82ad-980b7f088bf6",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.2.shard",
          "key_share_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "3e8b67de-c48e-46f5-bd64-94cfa28f7e9c",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.3.shard",
          "key_share_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "e259ffb5-5703-4dc2-8402-dbac1633dc28",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.4.shard",
          "key_share_path": "25/41/2541daa08b9f116ea311501ba0006591ae6d28f44579f6e73643df3c61ec873e.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        146,
        166,
        95,
        190,
        234,
        245,
        184,
        153,
        71,
        240,
        163,
        173
      ]
    },
    "c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41": {
      "id": [
        201,
        103,
        68,
        239,
        109,
        82,
        172,
        245,
        161,
        7,
        221,
        206,
        17,
        28,
        227,
        11,
        181,
        78,
        130,
        187,
        93,
        153,
        44,
        232,
        208,
        9,
        202,
        125,
        231,
        172,
        42,
        65
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:07:14.374728Z",
      "modified_at": "2025-07-17T04:07:14.374728Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "fc273d9f-8c3e-4410-9128-b22a1604985d",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.0.shard",
          "key_share_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "2c19ab78-2d0b-4bdc-9538-68ae92b62f2e",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.1.shard",
          "key_share_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "a9d03ef8-9c21-429e-99f8-eb56eb20bd54",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.2.shard",
          "key_share_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "01b30f88-e764-472f-8c2e-2e9a8d58ccfd",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.3.shard",
          "key_share_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "a0c564ab-c3d1-43cf-b532-aa9baa29ca00",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.4.shard",
          "key_share_path": "c9/67/c96744ef6d52acf5a107ddce111ce30bb54e82bb5d992ce8d009ca7de7ac2a41.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        188,
        110,
        97,
        234,
        33,
        69,
        78,
        18,
        178,
        238,
        15,
        163
      ]
    },
    "8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700": {
      "id": [
        135,
        70,
        251,
        127,
        59,
        126,
        231,
        46,
        136,
        1,
        63,
        66,
        249,
        95,
        248,
        29,
        159,
        27,
        32,
        101,
        170,
        35,
        7,
        139,
        44,
        110,
        166,
        223,
        168,
        142,
        119,
        0
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:05:43.505664Z",
      "modified_at": "2025-07-17T04:05:43.505664Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "21b03c00-f430-4f69-81b4-70d77a0a3a2f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.0.shard",
          "key_share_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "3b53ac41-a857-4e4d-b5cd-0387775067f8",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.1.shard",
          "key_share_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "9248115e-0db3-4b0c-923c-d2fa01d4561d",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.2.shard",
          "key_share_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "14985295-28ec-4eda-9ed8-005a203f6675",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.3.shard",
          "key_share_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "2deb06a6-53a6-4428-9d51-4e0eade8547b",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.4.shard",
          "key_share_path": "87/46/8746fb7f3b7ee72e88013f42f95ff81d9f1b2065aa23078b2c6ea6dfa88e7700.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        214,
        2,
        176,
        154,
        99,
        60,
        235,
        164,
        204,
        163,
        205,
        11
      ]
    },
    "fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025": {
      "id": [
        251,
        189,
        195,
        157,
        55,
        97,
        33,
        191,
        197,
        52,
        122,
        68,
        163,
        96,
        64,
        99,
        180,
        19,
        80,
        199,
        7,
        30,
        211,
        126,
        38,
        91,
        240,
        188,
        193,
        202,
        208,
        37
      ],
      "directness": 0,
      "size": 128,
      "created_at": "2025-07-17T04:05:51.966573Z",
      "modified_at": "2025-07-17T04:05:51.966573Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "118aa2a8-9e1c-4407-910a-1889d90eeaf9",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.0.shard",
          "key_share_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "db524409-df9a-4bb0-8c55-9297c23deae3",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.1.shard",
          "key_share_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "d466294c-76d3-4bea-90c2-06049899ccdb",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.2.shard",
          "key_share_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "2fdf772c-2680-467f-83d9-597116a5f179",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.3.shard",
          "key_share_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "89f5f55e-a515-4f46-a707-d23a5b5a1f62",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.4.shard",
          "key_share_path": "fb/bd/fbbdc39d376121bfc5347a44a3604063b41350c7071ed37e265bf0bcc1cad025.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        233,
        7,
        20,
        125,
        81,
        218,
        20,
        3,
        93,
        22,
        62,
        158
      ]
    },
    "06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c": {
      "id": [
        6,
        1,
        143,
        10,
        171,
        26,
        142,
        181,
        82,
        124,
        61,
        226,
        225,
        184,
        35,
        241,
        217,
        222,
        253,
        102,
        157,
        146,
        26,
        172,
        47,
        241,
        99,
        96,
        54,
        174,
        95,
        44
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:06:42.588534Z",
      "modified_at": "2025-07-17T04:06:42.588534Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "0a47d866-5111-40f0-8d06-50230375cbe3",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.0.shard",
          "key_share_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "d15e48c1-6280-49a2-a041-b6338e702676",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.1.shard",
          "key_share_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "e7ea1788-3d83-49be-827d-f46c8cd152d2",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.2.shard",
          "key_share_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "01d970e0-3f85-4bc1-bc98-1c05f1f4c427",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.3.shard",
          "key_share_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "d064a5be-66a7-460f-b409-ef01b557bb7d",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.4.shard",
          "key_share_path": "06/01/06018f0aab1a8eb5527c3de2e1b823f1d9defd669d921aac2ff1636036ae5f2c.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        69,
        3,
        161,
        165,
        51,
        24,
        21,
        71,
        243,
        165,
        37,
        214
      ]
    },
    "fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4": {
      "id": [
        251,
        185,
        144,
        242,
        119,
        191,
        216,
        216,
        16,
        210,
        173,
        210,
        70,
        142,
        219,
        8,
        200,
        109,
        174,
        118,
        39,
        172,
        159,
        73,
        54,
        217,
        51,
        143,
        131,
        143,
        174,
        180
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:06:50.422626Z",
      "modified_at": "2025-07-17T04:06:50.422626Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "5efd0d74-ad40-4c5c-bb82-7979720ecc34",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.0.shard",
          "key_share_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "6122691d-0081-4a80-b1f2-c49f145273df",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.1.shard",
          "key_share_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "ad39a418-90f8-4533-88a7-e860debe9803",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.2.shard",
          "key_share_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "f182792b-bd0f-43ae-9cae-8d77a6f44e9f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.3.shard",
          "key_share_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "ecd1d8dd-1354-4d67-84d5-81b14051e3f9",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.4.shard",
          "key_share_path": "fb/b9/fbb990f277bfd8d810d2add2468edb08c86dae7627ac9f4936d9338f838faeb4.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        6,
        39,
        208,
        116,
        203,
        75,
        214,
        44,
        17,
        118,
        62,
        19
      ]
    },
    "b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41": {
      "id": [
        182,
        58,
        181,
        0,
        26,
        220,
        25,
        91,
        60,
        91,
        161,
        29,
        154,
        133,
        215,
        138,
        98,
        139,
        164,
        239,
        237,
        254,
        217,
        237,
        206,
        79,
        10,
        86,
        130,
        213,
        238,
        65
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:05:04.998454Z",
      "modified_at": "2025-07-17T04:05:04.998454Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "b0d44df9-1eff-4731-b09f-a49a5b85ec60",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.0.shard",
          "key_share_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "bd4c7234-af0c-4d5d-a59c-25a665466cfa",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.1.shard",
          "key_share_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "872d025e-2e21-445a-ac91-ca8b065bb11e",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.2.shard",
          "key_share_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "e5f55fc0-bb59-4c81-96e6-c09e9e3374ef",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.3.shard",
          "key_share_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "e81e9d0e-a971-4472-9465-ad46f2e96b40",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.4.shard",
          "key_share_path": "b6/3a/b63ab5001adc195b3c5ba11d9a85d78a628ba4efedfed9edce4f0a5682d5ee41.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        109,
        187,
        139,
        249,
        165,
        250,
        32,
        180,
        98,
        183,
        159,
        125
      ]
    },
    "29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64": {
      "id": [
        41,
        7,
        27,
        183,
        111,
        230,
        68,
        255,
        44,
        204,
        56,
        166,
        150,
        26,
        32,
        119,
        11,
        181,
        255,
        252,
        227,
        104,
        206,
        155,
        245,
        116,
        90,
        127,
        125,
        131,
        207,
        100
      ],
      "directness": 0,
      "size": 33554432,
      "created_at": "2025-07-17T04:05:27.867879Z",
      "modified_at": "2025-07-17T04:05:27.867879Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "b169771a-99f2-4d11-aeb8-fb79110ae96d",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_0/.rimsd",
          "relative_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.0.shard",
          "key_share_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "1dcaea58-4beb-4cb1-9d2c-976e5d109192",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_1/.rimsd",
          "relative_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.1.shard",
          "key_share_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "2eaca651-7c75-40a0-bcf4-6f4b82798348",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_2/.rimsd",
          "relative_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.2.shard",
          "key_share_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "5d39274c-4d7e-4bf5-8acc-1b82ed929206",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_3/.rimsd",
          "relative_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.3.shard",
          "key_share_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.3.keyshare"
        },
        {
          "shard_index": 4,
          "device_id": "99368403-ec79-444c-812b-9e4580778d55",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/large_video_test/rimsd_4/.rimsd",
          "relative_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.4.shard",
          "key_share_path": "29/07/29071bb76fe644ff2ccc38a6961a20770bb5fffce368ce9bf5745a7f7d83cf64.4.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        139,
        219,
        81,
        250,
        114,
        59,
        229,
        205,
        88,
        206,
        89,
        145
      ]
    }
  }
}

// =====================================================================
// FILE: packages/soradyne_core/examples/data/6112f9f8-aa50-4852-a105-f7a42914739b.json
// =====================================================================

{"bpm":76.4,"source_device_id":"8e255c95-f410-4fc5-ab6b-488bfb283097","timestamp":"2025-08-16T01:41:23.443800Z"}

// =====================================================================
// FILE: packages/soradyne_core/examples/data/34e6e366-562d-4189-9a03-3dad0434cdd6.json
// =====================================================================

{"bpm":76.4,"source_device_id":"a2962031-930e-4257-b5ad-e654375b37a0","timestamp":"2025-08-16T01:36:47.655588Z"}

// =====================================================================
// FILE: packages/soradyne_core/visual_block_test/metadata.json
// =====================================================================

{
  "blocks": {
    "175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79": {
      "id": [
        23,
        85,
        132,
        175,
        14,
        17,
        49,
        102,
        143,
        82,
        147,
        27,
        53,
        241,
        40,
        45,
        161,
        79,
        153,
        84,
        148,
        177,
        193,
        44,
        97,
        34,
        98,
        184,
        197,
        193,
        235,
        121
      ],
      "directness": 0,
      "size": 52507,
      "created_at": "2025-08-15T17:51:33.192279Z",
      "modified_at": "2025-08-15T17:51:33.192279Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "23b078fe-980d-4248-a199-61230127cf36",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.0.shard",
          "key_share_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "ca8634f0-8e8d-4f3b-8d94-c4b62ad0c07e",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.1.shard",
          "key_share_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "2322d4be-7020-468a-a08b-c660c27ad010",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.2.shard",
          "key_share_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "041b0800-5ab0-4385-b724-791e46d7c375",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.3.shard",
          "key_share_path": "17/55/175584af0e1131668f52931b35f1282da14f995494b1c12c612262b8c5c1eb79.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        240,
        188,
        212,
        54,
        63,
        125,
        212,
        192,
        255,
        77,
        135,
        249
      ]
    },
    "4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785": {
      "id": [
        71,
        37,
        137,
        43,
        20,
        184,
        94,
        17,
        160,
        2,
        78,
        239,
        88,
        157,
        110,
        165,
        29,
        27,
        63,
        201,
        203,
        172,
        2,
        238,
        108,
        52,
        221,
        152,
        248,
        30,
        167,
        133
      ],
      "directness": 0,
      "size": 35588,
      "created_at": "2025-08-15T17:51:28.059226Z",
      "modified_at": "2025-08-15T17:51:28.059227Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "60bf5d08-4ff5-48ea-abee-a9e738c45454",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.0.shard",
          "key_share_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "3da99118-02ba-4df7-a082-f89c88461de0",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.1.shard",
          "key_share_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "04340848-ba33-449b-b48b-e09d009f0b65",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.2.shard",
          "key_share_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "eb9a88c5-1b05-45a6-add8-42975ebd170c",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.3.shard",
          "key_share_path": "47/25/4725892b14b85e11a0024eef589d6ea51d1b3fc9cbac02ee6c34dd98f81ea785.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        206,
        140,
        177,
        141,
        151,
        179,
        135,
        70,
        62,
        83,
        3,
        218
      ]
    },
    "79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12": {
      "id": [
        121,
        254,
        236,
        234,
        17,
        7,
        198,
        249,
        122,
        0,
        226,
        126,
        139,
        168,
        0,
        138,
        45,
        153,
        207,
        189,
        41,
        204,
        25,
        39,
        93,
        155,
        175,
        41,
        32,
        200,
        61,
        18
      ],
      "directness": 0,
      "size": 24378,
      "created_at": "2025-08-15T17:51:33.020287Z",
      "modified_at": "2025-08-15T17:51:33.020290Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "c6b70977-6767-437f-8f7d-444211fde52e",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.0.shard",
          "key_share_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "2d3a6e0f-3ead-42d0-a76c-dfad0caf4770",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.1.shard",
          "key_share_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "c71a2283-c4cc-4fd5-b8b3-e6abd07d75c8",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.2.shard",
          "key_share_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "f00921c7-964e-423a-8c19-937e81f5caf2",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.3.shard",
          "key_share_path": "79/fe/79feecea1107c6f97a00e27e8ba8008a2d99cfbd29cc19275d9baf2920c83d12.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        79,
        174,
        66,
        77,
        252,
        177,
        80,
        33,
        156,
        251,
        217,
        82
      ]
    },
    "8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21": {
      "id": [
        138,
        146,
        167,
        63,
        90,
        0,
        18,
        1,
        4,
        220,
        188,
        138,
        174,
        7,
        169,
        194,
        71,
        29,
        183,
        245,
        207,
        118,
        101,
        57,
        114,
        193,
        142,
        224,
        143,
        50,
        138,
        33
      ],
      "directness": 0,
      "size": 8090,
      "created_at": "2025-08-15T17:51:28.203666Z",
      "modified_at": "2025-08-15T17:51:28.203666Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "2f8a7402-afbb-4dc9-b2d1-31c469eeb48f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.0.shard",
          "key_share_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "039dc416-2994-49de-bc6f-2d1654fa6aee",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.1.shard",
          "key_share_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "0dfbb90e-c659-4a03-857e-1bceb04af225",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.2.shard",
          "key_share_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "536883b7-a81b-4a5d-b5c6-a3c3a0bd1d37",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.3.shard",
          "key_share_path": "8a/92/8a92a73f5a00120104dcbc8aae07a9c2471db7f5cf76653972c18ee08f328a21.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        74,
        65,
        125,
        176,
        208,
        126,
        123,
        162,
        74,
        178,
        70,
        250
      ]
    },
    "945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f": {
      "id": [
        148,
        92,
        174,
        122,
        169,
        199,
        160,
        158,
        56,
        133,
        155,
        99,
        41,
        118,
        126,
        10,
        188,
        233,
        122,
        1,
        226,
        35,
        2,
        156,
        234,
        231,
        182,
        252,
        160,
        122,
        89,
        79
      ],
      "directness": 0,
      "size": 10962,
      "created_at": "2025-08-15T17:51:28.541265Z",
      "modified_at": "2025-08-15T17:51:28.541265Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "6306ac9b-137e-425a-b53b-c7172d2ab859",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.0.shard",
          "key_share_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "dc4df508-2b0a-4801-9797-1a0e7df0c1e8",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.1.shard",
          "key_share_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "31dba21d-7238-417a-862f-f5e1963b17cb",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.2.shard",
          "key_share_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "c3b6a317-a46c-4dca-bee6-8c550d58e26a",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.3.shard",
          "key_share_path": "94/5c/945cae7aa9c7a09e38859b6329767e0abce97a01e223029ceae7b6fca07a594f.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        16,
        103,
        245,
        20,
        114,
        135,
        164,
        71,
        107,
        131,
        21,
        88
      ]
    },
    "4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf": {
      "id": [
        76,
        38,
        37,
        81,
        176,
        96,
        5,
        101,
        211,
        15,
        150,
        172,
        59,
        240,
        228,
        39,
        235,
        228,
        225,
        214,
        137,
        64,
        227,
        116,
        64,
        224,
        170,
        86,
        147,
        63,
        221,
        175
      ],
      "directness": 0,
      "size": 508,
      "created_at": "2025-08-15T17:51:31.942864Z",
      "modified_at": "2025-08-15T17:51:31.942864Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "54fd0c25-929c-40ca-80d7-ce8657527a68",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.0.shard",
          "key_share_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "40c415c9-615f-4a5a-9550-8ba6dd8d2800",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.1.shard",
          "key_share_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "a030270b-0744-4fe4-8578-7b4e0c87d1fa",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.2.shard",
          "key_share_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "e5ea26de-d578-479c-a57b-9da5c5422bb7",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.3.shard",
          "key_share_path": "4c/26/4c262551b0600565d30f96ac3bf0e427ebe4e1d68940e37440e0aa56933fddaf.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        67,
        201,
        104,
        230,
        248,
        25,
        152,
        35,
        180,
        90,
        184,
        64
      ]
    },
    "285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14": {
      "id": [
        40,
        86,
        148,
        226,
        107,
        53,
        23,
        96,
        206,
        130,
        212,
        123,
        94,
        172,
        248,
        59,
        119,
        208,
        89,
        87,
        175,
        120,
        176,
        175,
        106,
        49,
        5,
        48,
        83,
        57,
        92,
        20
      ],
      "directness": 0,
      "size": 17631,
      "created_at": "2025-08-15T17:51:28.692840Z",
      "modified_at": "2025-08-15T17:51:28.692840Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "071980ad-5268-4380-b55f-bce4d8209d3f",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.0.shard",
          "key_share_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "2cd97c42-e7d0-4e36-8376-94e763bc4053",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.1.shard",
          "key_share_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "ec817c9d-8293-48cf-bb4e-4ec4f4363ea1",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.2.shard",
          "key_share_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "ac91e6c6-39e7-4d2a-b59f-87c9f0c5bbb6",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.3.shard",
          "key_share_path": "28/56/285694e26b351760ce82d47b5eacf83b77d05957af78b0af6a31053053395c14.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        124,
        207,
        98,
        189,
        33,
        244,
        126,
        97,
        234,
        192,
        24,
        133
      ]
    },
    "d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0": {
      "id": [
        211,
        84,
        225,
        40,
        250,
        209,
        3,
        52,
        109,
        14,
        36,
        191,
        11,
        67,
        80,
        59,
        9,
        152,
        53,
        40,
        20,
        90,
        205,
        116,
        167,
        201,
        132,
        79,
        184,
        57,
        70,
        192
      ],
      "directness": 0,
      "size": 112149,
      "created_at": "2025-08-15T17:51:36.590804Z",
      "modified_at": "2025-08-15T17:51:36.590805Z",
      "shard_locations": [
        {
          "shard_index": 0,
          "device_id": "14e58438-2336-441c-89f6-3c10616d43e9",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_0/.rimsd",
          "relative_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.0.shard",
          "key_share_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.0.keyshare"
        },
        {
          "shard_index": 1,
          "device_id": "ada6a0e8-758f-45c0-9c76-b73577096ff5",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_1/.rimsd",
          "relative_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.1.shard",
          "key_share_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.1.keyshare"
        },
        {
          "shard_index": 2,
          "device_id": "37cc786c-2238-4296-8334-c69f071b60b0",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_2/.rimsd",
          "relative_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.2.shard",
          "key_share_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.2.keyshare"
        },
        {
          "shard_index": 3,
          "device_id": "6ffa08fe-0deb-4deb-aae6-3452e5a32851",
          "rimsd_path": "/Users/danagretton/Dropbox (Personal)/Personal Projects/Rim/soradyne/packages/soradyne_core/visual_block_test/rimsd_3/.rimsd",
          "relative_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.3.shard",
          "key_share_path": "d3/54/d354e128fad103346d0e24bf0b43503b09983528145acd74a7c9844fb83946c0.3.keyshare"
        }
      ],
      "encryption_version": 1,
      "nonce": [
        107,
        228,
        83,
        25,
        111,
        103,
        221,
        117,
        10,
        213,
        159,
        219
      ]
    }
  }
}

// =====================================================================
// FILE: packages/soradyne_core/data/552489f5-0065-4ab7-a0cb-c56363ff1690.json
// =====================================================================

{"bpm":76.4,"source_device_id":"2062a5cc-a4c0-4dcf-aad5-4c96937f3d07","timestamp":"2025-08-16T01:31:47.240885Z"}

// =====================================================================
// FILE: packages/soradyne_core/data/be36205d-7489-4f27-95d0-7dddfe35bab7.json
// =====================================================================

{"bpm":76.4,"source_device_id":"727b4132-5c3b-4c24-b4f4-853dabbda32d","timestamp":"2025-08-16T01:32:28.320421Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/83db1212-3443-43d9-b8d1-f3dbc4fb4736.json
// =====================================================================

{"bpm":75.14244,"source_device_id":"a2a79cd6-a33b-41c3-8485-e149fe7b60f3","timestamp":"2025-08-17T17:54:08.164437Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/cd685576-4829-4b5f-81c3-f7f26ec40704.json
// =====================================================================

{"bpm":70.0,"source_device_id":"cb526d9a-f952-4041-bb3a-328ab1a20228","timestamp":"2025-08-16T01:59:40.089226Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/df263cc7-f3f1-4ca1-b42a-23e900e14752.json
// =====================================================================

{"bpm":85.0,"source_device_id":"1436aebb-ac02-472c-a310-04b6f9b8e29b","timestamp":"2025-08-17T18:17:05.241383Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/e3b0e6bf-301c-42f9-9bfa-2e38fe5c72b5.json
// =====================================================================

{"bpm":73.5,"source_device_id":"f6ebfbd9-69b3-4eee-834d-20128dde6755","timestamp":"2025-08-17T17:57:57.669781Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/ce8ea0c1-4e1b-43f9-8e30-23a9df3d3e58.json
// =====================================================================

{"bpm":71.0,"source_device_id":"2e2bc215-2734-4f2d-a893-657a5146d82e","timestamp":"2025-08-17T17:48:08.975447Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/004fbc20-d8db-4314-a63e-d71715452474.json
// =====================================================================

{"bpm":75.14244,"source_device_id":"459d1b14-3cf1-478d-9ef1-88d32c59f7ea","timestamp":"2025-08-17T17:45:26.758420Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/9c5240f8-221f-47f3-847e-966c0db13006.json
// =====================================================================

{"bpm":76.91204,"source_device_id":"da3bb37b-eeb6-49c1-9015-e8b67f45856e","timestamp":"2025-08-16T02:14:45.939315Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/b76ddcdc-e2e2-4343-9ed2-da8aaf77a078.json
// =====================================================================

{"bpm":67.0,"source_device_id":"fdca661f-680c-4d8b-96f2-191290df26af","timestamp":"2025-08-17T17:52:17.545602Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/e98b83da-fc33-4f3a-bf22-f6fc3dade5c8.json
// =====================================================================

{"bpm":74.0,"source_device_id":"be3fb3f5-c31a-4b5a-bda8-53faf514afc4","timestamp":"2025-08-17T17:39:41.719248Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/04aa44fd-d642-4b01-bb77-19f36578c48e.json
// =====================================================================

{"bpm":73.0,"source_device_id":"88e7b46e-7316-4263-8fd2-a61476a709d8","timestamp":"2025-08-16T01:59:11.412643Z"}

// =====================================================================
// FILE: packages/soradyne_core/heartrate_data/950080ad-a6ca-4d3b-8baf-9efb09ba1063.json
// =====================================================================

{"bpm":70.0,"source_device_id":"89cd918e-89d2-4d55-ae5f-f9a63dd5f870","timestamp":"2025-08-16T01:43:51.829343Z"}

# =====================================================================
# FILE: packages/soradyne_flutter/pubspec.yaml
# =====================================================================

name: soradyne_flutter
description: Flutter package for Soradyne file syncing, real-time messaging, and CRDTs
version: 0.1.0

environment:
  sdk: '>=3.0.0 <4.0.0'
  flutter: ">=3.0.0"

dependencies:
  flutter:
    sdk: flutter
  ffi: ^2.1.0
  path_provider: ^2.1.5
  uuid: ^4.0.0
  collection: ^1.17.0

dev_dependencies:
  flutter_test:
    sdk: flutter
  flutter_lints: ^3.0.0

flutter:
  plugin:
    platforms:
      android:
        package: com.soradyne.flutter
        pluginClass: SoradyneFlutterPlugin
      ios:
        pluginClass: SoradyneFlutterPlugin
      linux:
        pluginClass: SoradyneFlutterPlugin
      macos:
        pluginClass: SoradyneFlutterPlugin
      windows:
        pluginClass: SoradyneFlutterPlugin


# =====================================================================
# FILE: packages/giantt_core/pubspec.yaml
# =====================================================================

name: giantt_core
description: Core giantt logic for task dependency management
version: 1.0.0

environment:
  sdk: '^3.8.0'

dependencies:
  meta: ^1.9.1
  collection: ^1.17.2
  args: ^2.4.2

dev_dependencies:
  test: ^1.24.0
  lints: ^3.0.0

executables:
  giantt: giantt


# =====================================================================
# FILE: apps/soradyne_demo/pubspec.yaml
# =====================================================================

name: soradyne_demo
description: Demo app showcasing Soradyne capabilities

environment:
  sdk: '^3.8.0'

dependencies:
  flutter:
    sdk: flutter
  soradyne_flutter:
    path: ../../packages/soradyne_flutter
  ai_chat_flutter:
    path: ../../packages/ai_chat_flutter



# =====================================================================
# FILE: apps/soradyne_demo/pubspec_overrides.yaml
# =====================================================================

# melos_managed_dependency_overrides: ai_chat_flutter,soradyne_flutter
dependency_overrides:
  ai_chat_flutter:
    path: ../../packages/ai_chat_flutter
  soradyne_flutter:
    path: ../../packages/soradyne_flutter


# =====================================================================
# FILE: apps/soradyne_demo/flutter_app/devtools_options.yaml
# =====================================================================

description: This file stores settings for Dart & Flutter DevTools.
documentation: https://docs.flutter.dev/tools/devtools/extensions#configure-extension-enablement-states
extensions:


# =====================================================================
# FILE: apps/soradyne_demo/flutter_app/pubspec.yaml
# =====================================================================

name: soradyne_app
description: A beautiful media sharing and collaboration app built on Soradyne

publish_to: 'none'

version: 1.0.0+1

environment:
  sdk: '>=3.0.0 <4.0.0'

dependencies:
  flutter:
    sdk: flutter
  cupertino_icons: ^1.0.2
  http: ^1.1.0
  cached_network_image: ^3.3.0
  photo_view: ^0.14.0
  flutter_staggered_grid_view: ^0.7.0
  provider: ^6.1.5
  shared_preferences: ^2.2.2
  image_picker: ^1.1.2
  ffi: ^2.1.0
  path_provider: ^2.1.5
  path: ^1.8.3
  desktop_drop: ^0.4.4

dev_dependencies:
  flutter_test:
    sdk: flutter
  flutter_lints: ^3.0.0

flutter:
  uses-material-design: true


# =====================================================================
# FILE: apps/soradyne_demo/flutter_app/analysis_options.yaml
# =====================================================================

# This file configures the analyzer, which statically analyzes Dart code to
# check for errors, warnings, and lints.
#
# The issues identified by the analyzer are surfaced in the UI of Dart-enabled
# IDEs (https://dart.dev/tools#ides-and-editors). The analyzer can also be
# invoked from the command line by running `flutter analyze`.

# The following line activates a set of recommended lints for Flutter apps,
# packages, and plugins designed to encourage good coding practices.
include: package:flutter_lints/flutter.yaml

linter:
  # The lint rules applied to this project can be customized in the
  # section below to disable rules from the `package:flutter_lints/flutter.yaml`
  # included above or to enable additional rules. A list of all available lints
  # and their documentation is published at https://dart.dev/lints.
  #
  # Instead of disabling a lint rule for the entire project in the
  # section below, it can also be suppressed for a single line of code
  # or a specific dart file by using the `// ignore: name_of_lint` and
  # `// ignore_for_file: name_of_lint` syntax on the line or in the file
  # producing the lint.
  rules:
    # avoid_print: false  # Uncomment to disable the `avoid_print` rule
    # prefer_single_quotes: true  # Uncomment to enable the `prefer_single_quotes` rule

# Additional information about this file can be found at
# https://dart.dev/guides/language/analysis-options


# =====================================================================
# FILE: apps/giantt/pubspec.yaml
# =====================================================================

name: giantt
description: Gantt chart CLI and Flutter app powered by Soradyne
version: 1.0.0+1

environment:
  sdk: '^3.8.0'
  flutter: ">=3.0.0"

dependencies:
  flutter:
    sdk: flutter
  
  # Core giantt logic
  giantt_core:
    path: ../../packages/giantt_core
  
  
  # CLI dependencies
  args: ^2.4.2
  path: ^1.8.3
  io: ^1.0.4
  
  # Gantt-specific dependencies
  gantt_chart: ^0.2.0
  timeline_tile: ^2.0.0
  
  # Mobile-specific dependencies
  path_provider: ^2.1.5
  permission_handler: ^11.0.1

dev_dependencies:
  flutter_test:
    sdk: flutter
  test: ^1.24.0
  lints: ^3.0.0

flutter:
  uses-material-design: true

executables:
  giantt: giantt


# =====================================================================
# FILE: apps/giantt/pubspec_overrides.yaml
# =====================================================================

# melos_managed_dependency_overrides: ai_chat_flutter,soradyne_flutter
dependency_overrides:


# =====================================================================
# FILE: apps/giantt/analysis_options.yaml
# =====================================================================

# This file configures the analyzer, which statically analyzes Dart code to
# check for errors, warnings, and lints.
#
# The issues identified by the analyzer are surfaced in the UI of Dart-enabled
# IDEs (https://dart.dev/tools#ides-and-editors). The analyzer can also be
# invoked from the command line by running `flutter analyze`.

# The following line activates a set of recommended lints for Flutter apps,
# packages, and plugins designed to encourage good coding practices.
include: package:flutter_lints/flutter.yaml

linter:
  # The lint rules applied to this project can be customized in the
  # section below to disable rules from the `package:flutter_lints/flutter.yaml`
  # included above or to enable additional rules. A list of all available lints
  # and their documentation is published at https://dart.dev/lints.
  #
  # Instead of disabling a lint rule for the entire project in the
  # section below, it can also be suppressed for a single line of code
  # or a specific dart file by using the `// ignore: name_of_lint` and
  # `// ignore_for_file: name_of_lint` syntax on the line or in the file
  # producing the lint.
  rules:
    # avoid_print: false  # Uncomment to disable the `avoid_print` rule
    # prefer_single_quotes: true  # Uncomment to enable the `prefer_single_quotes` rule

# Additional information about this file can be found at
# https://dart.dev/guides/language/analysis-options


// =====================================================================
// FILE: apps/giantt/macos/Runner/Assets.xcassets/AppIcon.appiconset/Contents.json
// =====================================================================

{
  "images" : [
    {
      "size" : "16x16",
      "idiom" : "mac",
      "filename" : "app_icon_16.png",
      "scale" : "1x"
    },
    {
      "size" : "16x16",
      "idiom" : "mac",
      "filename" : "app_icon_32.png",
      "scale" : "2x"
    },
    {
      "size" : "32x32",
      "idiom" : "mac",
      "filename" : "app_icon_32.png",
      "scale" : "1x"
    },
    {
      "size" : "32x32",
      "idiom" : "mac",
      "filename" : "app_icon_64.png",
      "scale" : "2x"
    },
    {
      "size" : "128x128",
      "idiom" : "mac",
      "filename" : "app_icon_128.png",
      "scale" : "1x"
    },
    {
      "size" : "128x128",
      "idiom" : "mac",
      "filename" : "app_icon_256.png",
      "scale" : "2x"
    },
    {
      "size" : "256x256",
      "idiom" : "mac",
      "filename" : "app_icon_256.png",
      "scale" : "1x"
    },
    {
      "size" : "256x256",
      "idiom" : "mac",
      "filename" : "app_icon_512.png",
      "scale" : "2x"
    },
    {
      "size" : "512x512",
      "idiom" : "mac",
      "filename" : "app_icon_512.png",
      "scale" : "1x"
    },
    {
      "size" : "512x512",
      "idiom" : "mac",
      "filename" : "app_icon_1024.png",
      "scale" : "2x"
    }
  ],
  "info" : {
    "version" : 1,
    "author" : "xcode"
  }
}


// =====================================================================
// FILE: apps/giantt/web/manifest.json
// =====================================================================

{
    "name": "giantt",
    "short_name": "giantt",
    "start_url": ".",
    "display": "standalone",
    "background_color": "#0175C2",
    "theme_color": "#0175C2",
    "description": "A new Flutter project.",
    "orientation": "portrait-primary",
    "prefer_related_applications": false,
    "icons": [
        {
            "src": "icons/Icon-192.png",
            "sizes": "192x192",
            "type": "image/png"
        },
        {
            "src": "icons/Icon-512.png",
            "sizes": "512x512",
            "type": "image/png"
        },
        {
            "src": "icons/Icon-maskable-192.png",
            "sizes": "192x192",
            "type": "image/png",
            "purpose": "maskable"
        },
        {
            "src": "icons/Icon-maskable-512.png",
            "sizes": "512x512",
            "type": "image/png",
            "purpose": "maskable"
        }
    ]
}


// =====================================================================
// FILE: apps/giantt/ios/Runner/Assets.xcassets/LaunchImage.imageset/Contents.json
// =====================================================================

{
  "images" : [
    {
      "idiom" : "universal",
      "filename" : "LaunchImage.png",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "filename" : "LaunchImage@2x.png",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "filename" : "LaunchImage@3x.png",
      "scale" : "3x"
    }
  ],
  "info" : {
    "version" : 1,
    "author" : "xcode"
  }
}


// =====================================================================
// FILE: apps/giantt/ios/Runner/Assets.xcassets/AppIcon.appiconset/Contents.json
// =====================================================================

{
  "images" : [
    {
      "size" : "20x20",
      "idiom" : "iphone",
      "filename" : "Icon-App-20x20@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "20x20",
      "idiom" : "iphone",
      "filename" : "Icon-App-20x20@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "29x29",
      "idiom" : "iphone",
      "filename" : "Icon-App-29x29@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "29x29",
      "idiom" : "iphone",
      "filename" : "Icon-App-29x29@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "29x29",
      "idiom" : "iphone",
      "filename" : "Icon-App-29x29@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "40x40",
      "idiom" : "iphone",
      "filename" : "Icon-App-40x40@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "40x40",
      "idiom" : "iphone",
      "filename" : "Icon-App-40x40@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "60x60",
      "idiom" : "iphone",
      "filename" : "Icon-App-60x60@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "60x60",
      "idiom" : "iphone",
      "filename" : "Icon-App-60x60@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "20x20",
      "idiom" : "ipad",
      "filename" : "Icon-App-20x20@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "20x20",
      "idiom" : "ipad",
      "filename" : "Icon-App-20x20@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "29x29",
      "idiom" : "ipad",
      "filename" : "Icon-App-29x29@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "29x29",
      "idiom" : "ipad",
      "filename" : "Icon-App-29x29@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "40x40",
      "idiom" : "ipad",
      "filename" : "Icon-App-40x40@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "40x40",
      "idiom" : "ipad",
      "filename" : "Icon-App-40x40@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "76x76",
      "idiom" : "ipad",
      "filename" : "Icon-App-76x76@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "76x76",
      "idiom" : "ipad",
      "filename" : "Icon-App-76x76@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "83.5x83.5",
      "idiom" : "ipad",
      "filename" : "Icon-App-83.5x83.5@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "1024x1024",
      "idiom" : "ios-marketing",
      "filename" : "Icon-App-1024x1024@1x.png",
      "scale" : "1x"
    }
  ],
  "info" : {
    "version" : 1,
    "author" : "xcode"
  }
}


======================================================================
= MARKUP AND DOCUMENTATION FILES                                     =
======================================================================



<!-- ===================================================================== -->
<!-- FILE: README.md -->
<!-- ===================================================================== -->

<div align="center">
  <img src="docs/img/soradynelogo.png" alt="Soradyne Logo" width="400">
</div>

# Soradyne

⚠️ **PROOF OF CONCEPT - NOT FOR PRODUCTION USE** ⚠️

A protocol and app demonstrator for secure, peer-to-peer shared self-data objects with a focus on privacy and user control.

**Important Notice**: This is experimental proof-of-concept code designed to demonstrate what's possible with decentralized self-data objects. The codebase is not well organized for others to easily view and understand - it's really meant to get the ball rolling and show concrete possibilities. Much improvement is needed before this would be ready for prime time or production use.

Made with Aider Chat Bot backed by Claude Sonnet 4 hosted on OpenRouter.

## What is Soradyne?

Soradyne is a protocol that enables secure, peer-to-peer sharing of self-data objects (SDOs). It's designed to give users full control over their personal data, allowing them to share it with others on their terms.

Key features:

- **Data Sovereignty**: You own and control your data, not a third-party service or company.
- **Peer-to-Peer**: Direct sharing between devices without requiring centralized servers.
- **Data Dissolution**: Split your data across multiple devices for enhanced security and resilience.
- **Data Crystallization**: Recombine dissolved data when needed.
- **Real-Time & Eventually Consistent SDOs**: Support for different synchronization models based on use case.
- **Encrypted & Private**: All data is encrypted by default, and privacy is built into the core design.

## Use Cases

Soradyne is designed to support various use cases, including:

- **Heart Rate Monitoring**: Share real-time heart rate data with healthcare providers or family members.
- **Chat Conversations**: Private, end-to-end encrypted messaging between individuals or groups.
- **Photo Albums**: Share photos with friends and family without uploading them to a cloud service.
- **File Sharing**: Securely share files with others directly, with no size limits or usage tracking.
- **Collaborative Robotics**: Share real-time robot joint positions and forces for remote collaboration.

## Quick Start

### Prerequisites

- Rust (latest stable)
- Node.js (v14 or higher)
- npm or yarn

### Building

```bash
# Clone the repository
git clone https://github.com/your-username/soradyne.git
cd soradyne

# Build the library
./build.sh
```

### Running Examples

```bash
# Heart rate example
cd ts && npm run example:heartrate

# Chat example
cd ts && npm run example:chat
```

## Architecture

Soradyne is built with a layered architecture:

1. **Core Layer**: Identity management, cryptography, and transport protocols.
2. **SDO Layer**: Self-Data Object definitions and implementations.
3. **Storage Layer**: Data dissolution and crystallization mechanisms.
4. **Application Layer**: Examples and applications built on top of Soradyne.

## Project Status

⚠️ **This is proof-of-concept code only** ⚠️

Soradyne is currently in the very early experimental phase. This codebase serves as a demonstration of concepts and possibilities rather than a production-ready system. Key limitations include:

- **Not production ready**: This code should never be used in production environments
- **Poor organization**: The codebase structure is not well organized for external contributors or users
- **Experimental nature**: Many components are rough prototypes to explore feasibility
- **Security concerns**: Cryptographic and security implementations need thorough review
- **Documentation gaps**: Much of the code lacks proper documentation and examples

The goal is to show what's possible concretely and get the conversation started about decentralized self-data protocols. Significant work is needed to make this suitable for real-world use.

We welcome feedback and discussions about the concepts, but please do not use this code for anything beyond experimentation and learning!

## License

This project is licensed under the MIT License - see the LICENSE file for details.


<!-- ===================================================================== -->
<!-- FILE: android/app/src/profile/AndroidManifest.xml -->
<!-- ===================================================================== -->

<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- The INTERNET permission is required for development. Specifically,
         the Flutter tool needs it to communicate with the running application
         to allow setting breakpoints, to provide hot reload, etc.
    -->
    <uses-permission android:name="android.permission.INTERNET"/>
</manifest>


<!-- ===================================================================== -->
<!-- FILE: android/app/src/main/AndroidManifest.xml -->
<!-- ===================================================================== -->

<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <application
        android:label="soradyne_workspace"
        android:name="${applicationName}"
        android:icon="@mipmap/ic_launcher">
        <activity
            android:name=".MainActivity"
            android:exported="true"
            android:launchMode="singleTop"
            android:taskAffinity=""
            android:theme="@style/LaunchTheme"
            android:configChanges="orientation|keyboardHidden|keyboard|screenSize|smallestScreenSize|locale|layoutDirection|fontScale|screenLayout|density|uiMode"
            android:hardwareAccelerated="true"
            android:windowSoftInputMode="adjustResize">
            <!-- Specifies an Android theme to apply to this Activity as soon as
                 the Android process has started. This theme is visible to the user
                 while the Flutter UI initializes. After that, this theme continues
                 to determine the Window background behind the Flutter UI. -->
            <meta-data
              android:name="io.flutter.embedding.android.NormalTheme"
              android:resource="@style/NormalTheme"
              />
            <intent-filter>
                <action android:name="android.intent.action.MAIN"/>
                <category android:name="android.intent.category.LAUNCHER"/>
            </intent-filter>
        </activity>
        <!-- Don't delete the meta-data below.
             This is used by the Flutter tool to generate GeneratedPluginRegistrant.java -->
        <meta-data
            android:name="flutterEmbedding"
            android:value="2" />
    </application>
    <!-- Required to query activities that can process text, see:
         https://developer.android.com/training/package-visibility and
         https://developer.android.com/reference/android/content/Intent#ACTION_PROCESS_TEXT.

         In particular, this is used by the Flutter engine in io.flutter.plugin.text.ProcessTextPlugin. -->
    <queries>
        <intent>
            <action android:name="android.intent.action.PROCESS_TEXT"/>
            <data android:mimeType="text/plain"/>
        </intent>
    </queries>
</manifest>


<!-- ===================================================================== -->
<!-- FILE: android/app/src/main/res/drawable/launch_background.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<!-- Modify this file to customize your launch splash screen -->
<layer-list xmlns:android="http://schemas.android.com/apk/res/android">
    <item android:drawable="@android:color/white" />

    <!-- You can insert your own image assets here -->
    <!-- <item>
        <bitmap
            android:gravity="center"
            android:src="@mipmap/launch_image" />
    </item> -->
</layer-list>


<!-- ===================================================================== -->
<!-- FILE: android/app/src/main/res/values-night/styles.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<resources>
    <!-- Theme applied to the Android Window while the process is starting when the OS's Dark Mode setting is on -->
    <style name="LaunchTheme" parent="@android:style/Theme.Black.NoTitleBar">
        <!-- Show a splash screen on the activity. Automatically removed when
             the Flutter engine draws its first frame -->
        <item name="android:windowBackground">@drawable/launch_background</item>
    </style>
    <!-- Theme applied to the Android Window as soon as the process has started.
         This theme determines the color of the Android Window while your
         Flutter UI initializes, as well as behind your Flutter UI while its
         running.

         This Theme is only used starting with V2 of Flutter's Android embedding. -->
    <style name="NormalTheme" parent="@android:style/Theme.Black.NoTitleBar">
        <item name="android:windowBackground">?android:colorBackground</item>
    </style>
</resources>


<!-- ===================================================================== -->
<!-- FILE: android/app/src/main/res/values/styles.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<resources>
    <!-- Theme applied to the Android Window while the process is starting when the OS's Dark Mode setting is off -->
    <style name="LaunchTheme" parent="@android:style/Theme.Light.NoTitleBar">
        <!-- Show a splash screen on the activity. Automatically removed when
             the Flutter engine draws its first frame -->
        <item name="android:windowBackground">@drawable/launch_background</item>
    </style>
    <!-- Theme applied to the Android Window as soon as the process has started.
         This theme determines the color of the Android Window while your
         Flutter UI initializes, as well as behind your Flutter UI while its
         running.

         This Theme is only used starting with V2 of Flutter's Android embedding. -->
    <style name="NormalTheme" parent="@android:style/Theme.Light.NoTitleBar">
        <item name="android:windowBackground">?android:colorBackground</item>
    </style>
</resources>


<!-- ===================================================================== -->
<!-- FILE: android/app/src/main/res/drawable-v21/launch_background.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<!-- Modify this file to customize your launch splash screen -->
<layer-list xmlns:android="http://schemas.android.com/apk/res/android">
    <item android:drawable="?android:colorBackground" />

    <!-- You can insert your own image assets here -->
    <!-- <item>
        <bitmap
            android:gravity="center"
            android:src="@mipmap/launch_image" />
    </item> -->
</layer-list>


<!-- ===================================================================== -->
<!-- FILE: android/app/src/debug/AndroidManifest.xml -->
<!-- ===================================================================== -->

<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- The INTERNET permission is required for development. Specifically,
         the Flutter tool needs it to communicate with the running application
         to allow setting breakpoints, to provide hot reload, etc.
    -->
    <uses-permission android:name="android.permission.INTERNET"/>
</manifest>


<!-- ===================================================================== -->
<!-- FILE: packages/soradyne_core/visual_block_test/results/comparison.html -->
<!-- ===================================================================== -->

<!DOCTYPE html>
<html>
<head>
<title>Soradyne Block Storage Visual Test Results</title>
<style>
body { font-family: Arial, sans-serif; margin: 20px; }
.test-result { border: 1px solid #ccc; margin: 20px 0; padding: 15px; }
.success { border-color: #4CAF50; background-color: #f9fff9; }
.failure { border-color: #f44336; background-color: #fff9f9; }
.image-comparison { display: flex; gap: 20px; margin: 10px 0; }
.image-container { text-align: center; }
img { max-width: 300px; max-height: 300px; border: 1px solid #ddd; }
.stats { background-color: #f5f5f5; padding: 10px; margin: 10px 0; }
</style>
</head>
<body>
<h1>🖼️ Soradyne Block Storage Visual Test Results</h1>
<p>Test completed with 2/8 successful results</p>
<div class="test-result success">
<h2>✅ SUCCESS httpbin_test.jpg</h2>
<p><strong>Description:</strong> A simple JPEG test image</p>
<div class="image-comparison">
<div class="image-container">
<h3>Original</h3>
<img src="original_httpbin_test.jpg" alt="Original httpbin_test.jpg">
<p>35588 bytes</p>
</div>
<div class="image-container">
<h3>Retrieved</h3>
<img src="retrieved_httpbin_test.jpg" alt="Retrieved httpbin_test.jpg">
<p>35588 bytes</p>
</div>
</div>
<div class="stats">
<p><strong>Hash Match:</strong> true</p>
<p><strong>Storage Time:</strong> 22.73ms</p>
<p><strong>Retrieval Time:</strong> 26.22ms</p>
</div>
</div>
<div class="test-result success">
<h2>✅ SUCCESS httpbin_test.png</h2>
<p><strong>Description:</strong> A simple PNG test image</p>
<div class="image-comparison">
<div class="image-container">
<h3>Original</h3>
<img src="original_httpbin_test.png" alt="Original httpbin_test.png">
<p>8090 bytes</p>
</div>
<div class="image-container">
<h3>Retrieved</h3>
<img src="retrieved_httpbin_test.png" alt="Retrieved httpbin_test.png">
<p>8090 bytes</p>
</div>
</div>
<div class="stats">
<p><strong>Hash Match:</strong> true</p>
<p><strong>Storage Time:</strong> 6.43ms</p>
<p><strong>Retrieval Time:</strong> 18.19ms</p>
</div>
</div>
<div class="test-result failure">
<h2>❌ FAILED picsum_300x200.jpg</h2>
<p><strong>Description:</strong> Random photo 300x200</p>
</div>
<div class="test-result failure">
<h2>❌ FAILED picsum_400x300.jpg</h2>
<p><strong>Description:</strong> Random photo 400x300</p>
</div>
<div class="test-result failure">
<h2>❌ FAILED picsum_500x300.jpg</h2>
<p><strong>Description:</strong> Random photo 500x300</p>
</div>
<div class="test-result failure">
<h2>❌ FAILED picsum_600x400.jpg</h2>
<p><strong>Description:</strong> Random photo 600x400</p>
</div>
<div class="test-result failure">
<h2>❌ FAILED picsum_800x600.jpg</h2>
<p><strong>Description:</strong> Random photo 800x600</p>
</div>
<div class="test-result failure">
<h2>❌ FAILED picsum_1024x768.jpg</h2>
<p><strong>Description:</strong> Random photo 1024x768</p>
</div>
</body>
</html>


<!-- ===================================================================== -->
<!-- FILE: apps/soradyne_demo/flutter_app/README.md -->
<!-- ===================================================================== -->

# soradyne_app

A new Flutter project.

## Getting Started

This project is a starting point for a Flutter application.

A few resources to get you started if this is your first Flutter project:

- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)
- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)

For help getting started with Flutter development, view the
[online documentation](https://docs.flutter.dev/), which offers tutorials,
samples, guidance on mobile development, and a full API reference.


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/README.md -->
<!-- ===================================================================== -->

# giantt

A new Flutter project.

## Getting Started

This project is a starting point for a Flutter application.

A few resources to get you started if this is your first Flutter project:

- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)
- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)

For help getting started with Flutter development, view the
[online documentation](https://docs.flutter.dev/), which offers tutorials,
samples, guidance on mobile development, and a full API reference.


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/web/index.html -->
<!-- ===================================================================== -->

<!DOCTYPE html>
<html>
<head>
  <!--
    If you are serving your web app in a path other than the root, change the
    href value below to reflect the base path you are serving from.

    The path provided below has to start and end with a slash "/" in order for
    it to work correctly.

    For more details:
    * https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base

    This is a placeholder for base href that will be replaced by the value of
    the `--base-href` argument provided to `flutter build`.
  -->
  <base href="$FLUTTER_BASE_HREF">

  <meta charset="UTF-8">
  <meta content="IE=Edge" http-equiv="X-UA-Compatible">
  <meta name="description" content="A new Flutter project.">

  <!-- iOS meta tags & icons -->
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="giantt">
  <link rel="apple-touch-icon" href="icons/Icon-192.png">

  <!-- Favicon -->
  <link rel="icon" type="image/png" href="favicon.png"/>

  <title>giantt</title>
  <link rel="manifest" href="manifest.json">
</head>
<body>
  <script src="flutter_bootstrap.js" async></script>
</body>
</html>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/ios/Runner/Assets.xcassets/LaunchImage.imageset/README.md -->
<!-- ===================================================================== -->

# Launch Screen Assets

You can customize the launch screen with your own desired assets by replacing the image files in this directory.

You can also do it by opening your Flutter project's Xcode project with `open ios/Runner.xcworkspace`, selecting `Runner/Assets.xcassets` in the Project Navigator and dropping in the desired images.

<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/profile/AndroidManifest.xml -->
<!-- ===================================================================== -->

<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- The INTERNET permission is required for development. Specifically,
         the Flutter tool needs it to communicate with the running application
         to allow setting breakpoints, to provide hot reload, etc.
    -->
    <uses-permission android:name="android.permission.INTERNET"/>
</manifest>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/main/AndroidManifest.xml -->
<!-- ===================================================================== -->

<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <application
        android:label="giantt"
        android:name="${applicationName}"
        android:icon="@mipmap/ic_launcher">
        <activity
            android:name=".MainActivity"
            android:exported="true"
            android:launchMode="singleTop"
            android:taskAffinity=""
            android:theme="@style/LaunchTheme"
            android:configChanges="orientation|keyboardHidden|keyboard|screenSize|smallestScreenSize|locale|layoutDirection|fontScale|screenLayout|density|uiMode"
            android:hardwareAccelerated="true"
            android:windowSoftInputMode="adjustResize">
            <!-- Specifies an Android theme to apply to this Activity as soon as
                 the Android process has started. This theme is visible to the user
                 while the Flutter UI initializes. After that, this theme continues
                 to determine the Window background behind the Flutter UI. -->
            <meta-data
              android:name="io.flutter.embedding.android.NormalTheme"
              android:resource="@style/NormalTheme"
              />
            <intent-filter>
                <action android:name="android.intent.action.MAIN"/>
                <category android:name="android.intent.category.LAUNCHER"/>
            </intent-filter>
        </activity>
        <!-- Don't delete the meta-data below.
             This is used by the Flutter tool to generate GeneratedPluginRegistrant.java -->
        <meta-data
            android:name="flutterEmbedding"
            android:value="2" />
    </application>
    <!-- Required to query activities that can process text, see:
         https://developer.android.com/training/package-visibility and
         https://developer.android.com/reference/android/content/Intent#ACTION_PROCESS_TEXT.

         In particular, this is used by the Flutter engine in io.flutter.plugin.text.ProcessTextPlugin. -->
    <queries>
        <intent>
            <action android:name="android.intent.action.PROCESS_TEXT"/>
            <data android:mimeType="text/plain"/>
        </intent>
    </queries>
</manifest>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/main/res/drawable/launch_background.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<!-- Modify this file to customize your launch splash screen -->
<layer-list xmlns:android="http://schemas.android.com/apk/res/android">
    <item android:drawable="@android:color/white" />

    <!-- You can insert your own image assets here -->
    <!-- <item>
        <bitmap
            android:gravity="center"
            android:src="@mipmap/launch_image" />
    </item> -->
</layer-list>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/main/res/values-night/styles.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<resources>
    <!-- Theme applied to the Android Window while the process is starting when the OS's Dark Mode setting is on -->
    <style name="LaunchTheme" parent="@android:style/Theme.Black.NoTitleBar">
        <!-- Show a splash screen on the activity. Automatically removed when
             the Flutter engine draws its first frame -->
        <item name="android:windowBackground">@drawable/launch_background</item>
    </style>
    <!-- Theme applied to the Android Window as soon as the process has started.
         This theme determines the color of the Android Window while your
         Flutter UI initializes, as well as behind your Flutter UI while its
         running.

         This Theme is only used starting with V2 of Flutter's Android embedding. -->
    <style name="NormalTheme" parent="@android:style/Theme.Black.NoTitleBar">
        <item name="android:windowBackground">?android:colorBackground</item>
    </style>
</resources>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/main/res/values/styles.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<resources>
    <!-- Theme applied to the Android Window while the process is starting when the OS's Dark Mode setting is off -->
    <style name="LaunchTheme" parent="@android:style/Theme.Light.NoTitleBar">
        <!-- Show a splash screen on the activity. Automatically removed when
             the Flutter engine draws its first frame -->
        <item name="android:windowBackground">@drawable/launch_background</item>
    </style>
    <!-- Theme applied to the Android Window as soon as the process has started.
         This theme determines the color of the Android Window while your
         Flutter UI initializes, as well as behind your Flutter UI while its
         running.

         This Theme is only used starting with V2 of Flutter's Android embedding. -->
    <style name="NormalTheme" parent="@android:style/Theme.Light.NoTitleBar">
        <item name="android:windowBackground">?android:colorBackground</item>
    </style>
</resources>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/main/res/drawable-v21/launch_background.xml -->
<!-- ===================================================================== -->

<?xml version="1.0" encoding="utf-8"?>
<!-- Modify this file to customize your launch splash screen -->
<layer-list xmlns:android="http://schemas.android.com/apk/res/android">
    <item android:drawable="?android:colorBackground" />

    <!-- You can insert your own image assets here -->
    <!-- <item>
        <bitmap
            android:gravity="center"
            android:src="@mipmap/launch_image" />
    </item> -->
</layer-list>


<!-- ===================================================================== -->
<!-- FILE: apps/giantt/android/app/src/debug/AndroidManifest.xml -->
<!-- ===================================================================== -->

<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- The INTERNET permission is required for development. Specifically,
         the Flutter tool needs it to communicate with the running application
         to allow setting breakpoints, to provide hot reload, etc.
    -->
    <uses-permission android:name="android.permission.INTERNET"/>
</manifest>


======================================================================
= SHELL SCRIPTS AND BATCH FILES                                      =
======================================================================



// =====================================================================
// FILE: android/gradlew.bat
// =====================================================================

@if "%DEBUG%" == "" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS=

set DIRNAME=%~dp0
if "%DIRNAME%" == "" set DIRNAME=.
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if "%ERRORLEVEL%" == "0" goto init

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto init

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:init
@rem Get command-line arguments, handling Windowz variants

if not "%OS%" == "Windows_NT" goto win9xME_args
if "%@eval[2+2]" == "4" goto 4NT_args

:win9xME_args
@rem Slurp the command line arguments.
set CMD_LINE_ARGS=
set _SKIP=2

:win9xME_args_slurp
if "x%~1" == "x" goto execute

set CMD_LINE_ARGS=%*
goto execute

:4NT_args
@rem Get arguments from the 4NT Shell from JP Software
set CMD_LINE_ARGS=%$

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar

@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%

:end
@rem End local scope for the variables with windows NT shell
if "%ERRORLEVEL%"=="0" goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
if  not "" == "%GRADLE_EXIT_CONSOLE%" exit 1
exit /b 1

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega


// =====================================================================
// FILE: apps/giantt/android/gradlew.bat
// =====================================================================

@if "%DEBUG%" == "" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS=

set DIRNAME=%~dp0
if "%DIRNAME%" == "" set DIRNAME=.
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if "%ERRORLEVEL%" == "0" goto init

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto init

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:init
@rem Get command-line arguments, handling Windowz variants

if not "%OS%" == "Windows_NT" goto win9xME_args
if "%@eval[2+2]" == "4" goto 4NT_args

:win9xME_args
@rem Slurp the command line arguments.
set CMD_LINE_ARGS=
set _SKIP=2

:win9xME_args_slurp
if "x%~1" == "x" goto execute

set CMD_LINE_ARGS=%*
goto execute

:4NT_args
@rem Get arguments from the 4NT Shell from JP Software
set CMD_LINE_ARGS=%$

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar

@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%

:end
@rem End local scope for the variables with windows NT shell
if "%ERRORLEVEL%"=="0" goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
if  not "" == "%GRADLE_EXIT_CONSOLE%" exit 1
exit /b 1

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega


======================================================================
= END OF FILES                                                       =
======================================================================

